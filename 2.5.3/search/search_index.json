{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Jenkins Templating Engine \u00b6 Welcome! The Jenkins Templating Engine is a pipeline development framework for Jenkins that allows teams to create tool-agnostic Pipeline Templates that can be reused across applications. Concepts Concept pages are understanding oriented articles describing how JTE works. Reference Reference pages are information oriented descriptions of JTE mechanics. Tutorials Tutorials are learning oriented lessons to teach users about JTE. How-To Guides How-To Guides are goal oriented step-by-step instructions for specific problems.","title":"Home"},{"location":"#jenkins-templating-engine","text":"Welcome! The Jenkins Templating Engine is a pipeline development framework for Jenkins that allows teams to create tool-agnostic Pipeline Templates that can be reused across applications. Concepts Concept pages are understanding oriented articles describing how JTE works. Reference Reference pages are information oriented descriptions of JTE mechanics. Tutorials Tutorials are learning oriented lessons to teach users about JTE. How-To Guides How-To Guides are goal oriented step-by-step instructions for specific problems.","title":"Jenkins Templating Engine"},{"location":"glossary/","text":"","title":"Glossary"},{"location":"concepts/advanced/overloading-steps/","text":"Overloading Steps \u00b6 Function Overloading 1 is when there are multiple methods with the same name and different implementation. Conflicting Library Steps \u00b6 To allow multiple libraries to contribute the same step, the Pipeline Configuration must have jte.permissive_initialization set to true. If multiple libraries do contribute the same step, the step will no longer be able to be invoked by its short_name . Instead, overloaded steps must be accessed using the Pipeline Primitive Namespace . Overloaded Library Steps The following example assumes a gradle and npm library are available that both contribute a build() step. Pipeline Configuration Pipeline Template pipeline_config.groovy jte { permissive_initialization = true // pipeline will fail if not set } libraries { npm // contributes build() gradle // contributes build() } Jenkinsfile // build() <-- would fail because step is overloaded jte . libraries . npm . build () jte . libraries . gradle . build () Default Mode is to Fail By default, if two libraries are loaded that contribute the same step then the Pipeline Run will fail during Pipeline Initialization . This behavior is modified by setting jte.permissive_initialization to True . Library Steps Overriding Jenkins Pipeline Steps \u00b6 Jenkins Pipeline DSL steps can be overridden by Library Steps . If a Library Step has the same name as a Jenkins Pipeline DSL step, such as node or build , the Library Step will take precedence. To invoke the original Jenkins Pipeline DSL Step, use the steps Global Variable . Declarative Syntax This isn't true for Declarative Syntax. Check out the Declarative Syntax page to learn more. Use Case: Overriding node If a library were to contribute a Library Step called node , then it would override the node step used in Jenkins Scripted Pipelines. The following example shows how to override the default node step to augment its functionality. node.groovy Pipeline Template node.groovy // support the original node interface void call ( String label = null , Closure body ){ if ( label ){ steps . node ( label , body ) // steps var used to access original \"node\" implementation } else { steps . node ( body ) } } // support new functionality void call ( Map args = [:], Closure body ){ if ( args . containsKey ( \"container\" )){ docker . image ( args . container ). inside ( body ) } } Jenkinsfile // assume the Pipeline Configuration loaded the library contributing node.groovy node { println \"hi\" } node ( \"my-label\" ){ println \"hi\" } node ( container: \"maven\" ){ sh \"mvn -v\" } // custom functionality Function Overloading \u21a9","title":"Overloading Steps"},{"location":"concepts/advanced/overloading-steps/#overloading-steps","text":"Function Overloading 1 is when there are multiple methods with the same name and different implementation.","title":"Overloading Steps"},{"location":"concepts/advanced/overloading-steps/#conflicting-library-steps","text":"To allow multiple libraries to contribute the same step, the Pipeline Configuration must have jte.permissive_initialization set to true. If multiple libraries do contribute the same step, the step will no longer be able to be invoked by its short_name . Instead, overloaded steps must be accessed using the Pipeline Primitive Namespace . Overloaded Library Steps The following example assumes a gradle and npm library are available that both contribute a build() step. Pipeline Configuration Pipeline Template pipeline_config.groovy jte { permissive_initialization = true // pipeline will fail if not set } libraries { npm // contributes build() gradle // contributes build() } Jenkinsfile // build() <-- would fail because step is overloaded jte . libraries . npm . build () jte . libraries . gradle . build () Default Mode is to Fail By default, if two libraries are loaded that contribute the same step then the Pipeline Run will fail during Pipeline Initialization . This behavior is modified by setting jte.permissive_initialization to True .","title":"Conflicting Library Steps"},{"location":"concepts/advanced/overloading-steps/#library-steps-overriding-jenkins-pipeline-steps","text":"Jenkins Pipeline DSL steps can be overridden by Library Steps . If a Library Step has the same name as a Jenkins Pipeline DSL step, such as node or build , the Library Step will take precedence. To invoke the original Jenkins Pipeline DSL Step, use the steps Global Variable . Declarative Syntax This isn't true for Declarative Syntax. Check out the Declarative Syntax page to learn more. Use Case: Overriding node If a library were to contribute a Library Step called node , then it would override the node step used in Jenkins Scripted Pipelines. The following example shows how to override the default node step to augment its functionality. node.groovy Pipeline Template node.groovy // support the original node interface void call ( String label = null , Closure body ){ if ( label ){ steps . node ( label , body ) // steps var used to access original \"node\" implementation } else { steps . node ( body ) } } // support new functionality void call ( Map args = [:], Closure body ){ if ( args . containsKey ( \"container\" )){ docker . image ( args . container ). inside ( body ) } } Jenkinsfile // assume the Pipeline Configuration loaded the library contributing node.groovy node { println \"hi\" } node ( \"my-label\" ){ println \"hi\" } node ( container: \"maven\" ){ sh \"mvn -v\" } // custom functionality Function Overloading \u21a9","title":"Library Steps Overriding Jenkins Pipeline Steps"},{"location":"concepts/advanced/pipeline-initialization/","text":"Pipeline Initialization \u00b6 Pipeline Initialization is the term JTE uses to describe everything that happens from when a Pipeline Run is started to when the Pipeline starts. Overview \u00b6 This stages of Pipeline Initialization are described in the table and picture below. Stage Description Pipeline Configuration Aggregation Pipeline Configurations fetched from Governance Tiers in the Configuration Hierarchy and merged, creating an Aggregated Pipeline Configuration Pipeline Primitives Injected JTE parses the aggregated Pipeline Configuration and creates the corresponding Pipeline Primitives Pipeline Template Determined JTE determines the Pipeline Template to use according to the process outlined in Pipeline Template Selection Pipeline Template Compiled The groovy Pipeline Template is compiled into the final form that will be executed the same way as any other Jenkinsfile Pipeline Configuration Aggregation \u00b6 During the Pipeline Configuration Aggregation phase, JTE fetches the Pipeline Configuration from each Governance Tier (if present) in the Configuration Hierarchy . These Pipeline Configurations are then sequentially merged, according to the procedure outlined in Merging Pipeline Configurations . The result of this process is an aggregated Pipeline Configuration for the Pipeline Run. The Map representation of the aggregated Pipeline Configuration is made available via the pipelineConfig Autowired Variable . Pipeline Primitives Injected \u00b6 This next phase of Pipeline Initialization takes the aggregated Pipeline Configuration and parses it to create Pipeline Primitives . This happens by passing the aggregated Pipeline Configuration to a series of Template Primitive Injectors. Each Pipeline Primitive has a corresponding Template Primitive Injector that reads the aggregated Pipeline Configuration and creates the corresponding Pipeline Primitives. The Pipeline Primitives created during this phase are stored on the Pipeline Primitive Namespace . Important You may remember from reading the Pipeline Configuration Syntax that JTE's Pipeline Configuration DSL is just a dynamic builder language, meaning that it will accept any configurations it is given. The only reason application_environments does anything is because there is a Template Primitive Injector that specifically looks for that block. Pipeline Template Determined \u00b6 Next, JTE determines the Pipeline Template based on the process outlined in Pipeline Template Selection . Pipeline Template Compiled \u00b6 Finally, the determined Pipeline Template is compiled. JTE performs some Compile-Time Metaprogramming to wrap the Pipeline Template in a Try-Catch-Finally block so that the Lifecycle Hooks can be executed before and after Pipeline Template Execution. This compiled Pipeline is then executed just like any other Jenkinsfile . Pipeline Template Compilation JTE performs compile-time metaprogramming to slightly manipulate the Pipeline Template. What follows is an example to understand the transformation that takes place. Provided Pipeline Template Compiled Pipeline Template Jenkinsfile build () Jenkinsfile try { // code that invokes @Validation annotated methods in steps // code that invokes @Init annotated methods in steps build () // <-- original Pipeline Template } finally { // code that invokes @CleanUp annotated methods in steps // code that invokes @Notify annotated methods in steps }","title":"Pipeline Initialization"},{"location":"concepts/advanced/pipeline-initialization/#pipeline-initialization","text":"Pipeline Initialization is the term JTE uses to describe everything that happens from when a Pipeline Run is started to when the Pipeline starts.","title":"Pipeline Initialization"},{"location":"concepts/advanced/pipeline-initialization/#overview","text":"This stages of Pipeline Initialization are described in the table and picture below. Stage Description Pipeline Configuration Aggregation Pipeline Configurations fetched from Governance Tiers in the Configuration Hierarchy and merged, creating an Aggregated Pipeline Configuration Pipeline Primitives Injected JTE parses the aggregated Pipeline Configuration and creates the corresponding Pipeline Primitives Pipeline Template Determined JTE determines the Pipeline Template to use according to the process outlined in Pipeline Template Selection Pipeline Template Compiled The groovy Pipeline Template is compiled into the final form that will be executed the same way as any other Jenkinsfile","title":"Overview"},{"location":"concepts/advanced/pipeline-initialization/#pipeline-configuration-aggregation","text":"During the Pipeline Configuration Aggregation phase, JTE fetches the Pipeline Configuration from each Governance Tier (if present) in the Configuration Hierarchy . These Pipeline Configurations are then sequentially merged, according to the procedure outlined in Merging Pipeline Configurations . The result of this process is an aggregated Pipeline Configuration for the Pipeline Run. The Map representation of the aggregated Pipeline Configuration is made available via the pipelineConfig Autowired Variable .","title":"Pipeline Configuration Aggregation"},{"location":"concepts/advanced/pipeline-initialization/#pipeline-primitives-injected","text":"This next phase of Pipeline Initialization takes the aggregated Pipeline Configuration and parses it to create Pipeline Primitives . This happens by passing the aggregated Pipeline Configuration to a series of Template Primitive Injectors. Each Pipeline Primitive has a corresponding Template Primitive Injector that reads the aggregated Pipeline Configuration and creates the corresponding Pipeline Primitives. The Pipeline Primitives created during this phase are stored on the Pipeline Primitive Namespace . Important You may remember from reading the Pipeline Configuration Syntax that JTE's Pipeline Configuration DSL is just a dynamic builder language, meaning that it will accept any configurations it is given. The only reason application_environments does anything is because there is a Template Primitive Injector that specifically looks for that block.","title":"Pipeline Primitives Injected"},{"location":"concepts/advanced/pipeline-initialization/#pipeline-template-determined","text":"Next, JTE determines the Pipeline Template based on the process outlined in Pipeline Template Selection .","title":"Pipeline Template Determined"},{"location":"concepts/advanced/pipeline-initialization/#pipeline-template-compiled","text":"Finally, the determined Pipeline Template is compiled. JTE performs some Compile-Time Metaprogramming to wrap the Pipeline Template in a Try-Catch-Finally block so that the Lifecycle Hooks can be executed before and after Pipeline Template Execution. This compiled Pipeline is then executed just like any other Jenkinsfile . Pipeline Template Compilation JTE performs compile-time metaprogramming to slightly manipulate the Pipeline Template. What follows is an example to understand the transformation that takes place. Provided Pipeline Template Compiled Pipeline Template Jenkinsfile build () Jenkinsfile try { // code that invokes @Validation annotated methods in steps // code that invokes @Init annotated methods in steps build () // <-- original Pipeline Template } finally { // code that invokes @CleanUp annotated methods in steps // code that invokes @Notify annotated methods in steps }","title":"Pipeline Template Compiled"},{"location":"concepts/framework-overview/","text":"Overview \u00b6 Concept pages are understanding oriented articles describing how JTE works. The Framework Overview section is the best place to get started building your mental model of how JTE works. Getting Started \u00b6 Everyone learns differently. The following pages explain JTE from different perspectives. Page Description Bottom-Up Explanation Learn JTE by transforming a set of existing Jenkins Pipelines into a reusable Pipeline Template Top-Down Learn JTE by diving right in from the perspective of how JTE works Key Benefits Learn how to articulate the value of Pipeline Templating Introductory Webinar \u00b6 If you learn best by watching videos, this Continuous Delivery Foundation webinar is a great place to start. Learn More \u00b6 After reviewing the framework overviews, check out the following sections to start learning more. Section Description Pipeline Templates Learn more about Pipeline Templates and how to define workflows Pipeline Configuration Learn more about Pipeline Configurations Pipeline Primitives Learn more about Pipeline Primitives and how to make Pipeline Templates reusable Library Development Learn more about Library Development and how to write Library Steps Pipeline Governance Learn more about how to govern template selection, library resolution, and Pipeline Configuration settings Advanced Topics Learn about advanced features that don't cleanly fit anywhere else","title":"Overview"},{"location":"concepts/framework-overview/#overview","text":"Concept pages are understanding oriented articles describing how JTE works. The Framework Overview section is the best place to get started building your mental model of how JTE works.","title":"Overview"},{"location":"concepts/framework-overview/#getting-started","text":"Everyone learns differently. The following pages explain JTE from different perspectives. Page Description Bottom-Up Explanation Learn JTE by transforming a set of existing Jenkins Pipelines into a reusable Pipeline Template Top-Down Learn JTE by diving right in from the perspective of how JTE works Key Benefits Learn how to articulate the value of Pipeline Templating","title":"Getting Started"},{"location":"concepts/framework-overview/#introductory-webinar","text":"If you learn best by watching videos, this Continuous Delivery Foundation webinar is a great place to start.","title":"Introductory Webinar"},{"location":"concepts/framework-overview/#learn-more","text":"After reviewing the framework overviews, check out the following sections to start learning more. Section Description Pipeline Templates Learn more about Pipeline Templates and how to define workflows Pipeline Configuration Learn more about Pipeline Configurations Pipeline Primitives Learn more about Pipeline Primitives and how to make Pipeline Templates reusable Library Development Learn more about Library Development and how to write Library Steps Pipeline Governance Learn more about how to govern template selection, library resolution, and Pipeline Configuration settings Advanced Topics Learn about advanced features that don't cleanly fit anywhere else","title":"Learn More"},{"location":"concepts/framework-overview/bottom-up/","text":"Bottom-Up Explanation \u00b6 Welcome ! This page is going to take you on a journey from how pipelines are typically built today (on a per-application basis), pausing to look at the challenges that causes at scale, and then step through together how the Jenkins Templating Engine (JTE) works to remediate those pain points. This will be one of the lengthier pages in the documentation. If you can stick to it, you'll come out the other side seeing pipeline development differently. Writing Pipelines Without the Jenkins Templating Engine \u00b6 Imagine that there are three applications that each need a pipeline to automate the execution of unit tests and building an artifact. Click through the tabs below to see a pipeline for a Gradle application, a Maven application, and an NPM application. Gradle Maven NPM Jenkinsfile // a basic Gradle pipeline stage ( \"Unit Test\" ){ node { sh \"gradle verify\" } } stage ( \"Build\" ){ node { sh \"gradle build\" } } Jenkinsfile // a basic Maven pipeline stage ( \"Unit Test\" ){ node { sh \"mvn verify\" } } stage ( \"Build\" ){ node { sh \"mvn build\" } } Jenkinsfile // a basic NPM pipeline stage ( \"Unit Test\" ){ node { sh \"npm ci && npm run test\" } } stage ( \"Build\" ){ node { sh \"npm ci && npm build\" } } Traditionally, these pipelines would be stored alongside the source code for the application. Why This Approach Doesn't Scale \u00b6 Defining pipelines on a per-application basis works when you have a small number of teams you're supporting . Over time, though, \"DevOps Teams\" at organizations find themselves responsible for administering a growing number of internal tools and scaling the adoption of those tools across application teams. Why is creating bespoke pipelines such a big deal? It doesn't scale. Individual pipelines means work needs to be done to integrate each application with a pipeline Duplicated pipelines introduce uncertainty that common processes are followed Complexity becomes difficult to manage Onboarding Each Team Individually \u00b6 When pipelines are built on a per-application basis, it leaves organizations with two choices. Either you need a developer on every team who knows how to write pipelines aligned with organizational standards, or you need a dedicated pipeline team onboarding application teams to a pipeline. The first choice often isn't super viable - developers should focus on the problem that they're best suited to solve: developing applications. Even if all the software developers could write their own automated pipelines, it becomes very challenging to ensure these pipelines follow required compliance checks. The second choice requires that you scale the dedicated pipeline team to meet the number of application teams that need support. This is often prohibitively expensive. Standardization & Compliance \u00b6 When pipelines are built on a per-application basis, it becomes extremely challenging to be confident that teams throughout the organization are following approved software delivery processes aligned with cyber compliance requirements. Furthermore, for more tightly governed environments, the idea of a developer being able to modify the Jenkinsfile on their branch to perform a deployment to production is a threat vector that introduces unacceptable, unnecessary risks. Mitigating this risk requires very mature monitoring and correlation across systems. Complexity \u00b6 There's an old adage, \"running one container is easy, running many is really, really hard.\" The same applies for pipelines within an organization. Creating a pipeline that runs unit tests, builds artifacts, deploys those artifacts, and runs security scans gets exponentially more complex the more teams and technical stacks need to be supported. A Better Way: Pipeline Templating \u00b6 These challenges are all caused by building pipelines on a per-application basis . Even if you're modularizing your pipeline code for reuse through Jenkins Shared Libraries, you still end up duplicating the Jenkinsfile across every branch of every source code repository. Warning Don't even get me started on how tricky it can be to propagate a change to the pipeline across teams when the representation of the pipeline lives across each branch independently. Taking a step back, you may have noticed that each of the three pipelines above follow the same structure. First they execute unit tests, then they perform a build. In our experience, this simple pattern holds true most of the time ( especially when working with microservices). While the specific tools that get used to perform a step of the pipeline may change, the workflow remains the same. Writing a Pipeline Template \u00b6 The entire philosophy behind the Jenkins Template Engine stems from the concept of common workflows with interchangeable tools. What if it was possible to translate the three separate pipelines above into the following Pipeline Template: Jenkinsfile unit_test () build () Good news, this is exactly what JTE makes possible! Next, you'll need to define your implementations of the unit_test() and build() steps. Writing Pipeline Steps \u00b6 Implement the unit_test() and build() steps by refactoring the original pipelines above. Gradle: unit_test Gradle: build gradle/steps/unit_test.groovy void call (){ stage ( \"Unit Test\" ){ node { sh \"gradle verify\" } } } gradle/steps/build.groovy void call (){ stage ( \"Build\" ){ node { sh \"gradle build\" } } } Maven: unit_test Maven: build maven/steps/unit_test.groovy void call (){ stage ( \"Unit Test\" ){ node { sh \"mvn verify\" } } } maven/steps/build.groovy void call (){ stage ( \"Build\" ){ node { sh \"mvn build\" } } } NPM: unit_test NPM: build npm/steps/unit_test.groovy void call (){ stage ( \"Unit Test\" ){ node { sh \"npm ci && npm run test\" } } } npm/steps/build.groovy void call (){ stage ( \"Build\" ){ node { sh \"npm ci && npm build\" } } } Note These steps are the exact same pipeline code we had written at the start. We just wrapped it in a call method for Reasons you can learn about over on the library steps page. Understanding Libraries \u00b6 If you go back and look at the comments indicating the files those steps are placed in, you'll notice the following file structure: . \u251c\u2500\u2500 gradle \u2502 \u2514\u2500\u2500 steps \u2502 \u251c\u2500\u2500 build.groovy \u2502 \u2514\u2500\u2500 unit_test.groovy \u251c\u2500\u2500 maven \u2502 \u2514\u2500\u2500 steps \u2502 \u251c\u2500\u2500 build.groovy \u2502 \u2514\u2500\u2500 unit_test.groovy \u2514\u2500\u2500 npm \u2514\u2500\u2500 steps \u251c\u2500\u2500 build.groovy \u2514\u2500\u2500 unit_test.groovy A library in JTE is a collection of steps (stored together in a directory) that can be loaded at runtime. Earlier, a common Pipeline Template was defined that executes unit tests and then builds an artifact. This template can be shared across teams. Then, three libraries were created: npm , gradle , and maven . Each of these libraries implemented the steps required by the template. Finally, JTE needs a way to determine which libraries to load for a given team's pipeline. Pipeline Configuration \u00b6 So far, you've defined a Pipeline Template that invokes steps, and libraries that implement those steps. The missing piece is a way to link the two. This is where the Pipeline Configuration comes in. The Pipeline Configuration uses a groovy-based configuration language to ensure the Pipeline Template uses the correct tools and settings for the application. For example, here's a Pipeline Configuration that specifies the npm library should be loaded. pipeline_config.groovy libraries { npm } When a pipeline using JTE runs with this configuration and template, the steps from the npm library are loaded. This means that the unit_test and build steps within the template will use the unit_test and build definitions provided by the npm library! By swapping out Pipeline Configurations , a single Pipeline Template can be used across multiple teams, supporting multiple tech stacks . Tip: Design Patterns in Action There are two general software architecture design patterns that might help in understanding the way JTE works. The first is the Template Method Design Pattern . You could think of the pipeline template as the AbstractClass that defines the scaffold of an algorithm (pipeline) and libraries as various implementations of a ConcreteClass that bring implementations of steps in the algorithm. The second is Dependency Injection . You could think of the JTE framework as the Injector , the pipeline template the Client , and each step a Dependency being injected.","title":"Bottom-Up Explanation"},{"location":"concepts/framework-overview/bottom-up/#bottom-up-explanation","text":"Welcome ! This page is going to take you on a journey from how pipelines are typically built today (on a per-application basis), pausing to look at the challenges that causes at scale, and then step through together how the Jenkins Templating Engine (JTE) works to remediate those pain points. This will be one of the lengthier pages in the documentation. If you can stick to it, you'll come out the other side seeing pipeline development differently.","title":"Bottom-Up Explanation"},{"location":"concepts/framework-overview/bottom-up/#writing-pipelines-without-the-jenkins-templating-engine","text":"Imagine that there are three applications that each need a pipeline to automate the execution of unit tests and building an artifact. Click through the tabs below to see a pipeline for a Gradle application, a Maven application, and an NPM application. Gradle Maven NPM Jenkinsfile // a basic Gradle pipeline stage ( \"Unit Test\" ){ node { sh \"gradle verify\" } } stage ( \"Build\" ){ node { sh \"gradle build\" } } Jenkinsfile // a basic Maven pipeline stage ( \"Unit Test\" ){ node { sh \"mvn verify\" } } stage ( \"Build\" ){ node { sh \"mvn build\" } } Jenkinsfile // a basic NPM pipeline stage ( \"Unit Test\" ){ node { sh \"npm ci && npm run test\" } } stage ( \"Build\" ){ node { sh \"npm ci && npm build\" } } Traditionally, these pipelines would be stored alongside the source code for the application.","title":"Writing Pipelines Without the Jenkins Templating Engine"},{"location":"concepts/framework-overview/bottom-up/#why-this-approach-doesnt-scale","text":"Defining pipelines on a per-application basis works when you have a small number of teams you're supporting . Over time, though, \"DevOps Teams\" at organizations find themselves responsible for administering a growing number of internal tools and scaling the adoption of those tools across application teams. Why is creating bespoke pipelines such a big deal? It doesn't scale. Individual pipelines means work needs to be done to integrate each application with a pipeline Duplicated pipelines introduce uncertainty that common processes are followed Complexity becomes difficult to manage","title":"Why This Approach Doesn't Scale"},{"location":"concepts/framework-overview/bottom-up/#onboarding-each-team-individually","text":"When pipelines are built on a per-application basis, it leaves organizations with two choices. Either you need a developer on every team who knows how to write pipelines aligned with organizational standards, or you need a dedicated pipeline team onboarding application teams to a pipeline. The first choice often isn't super viable - developers should focus on the problem that they're best suited to solve: developing applications. Even if all the software developers could write their own automated pipelines, it becomes very challenging to ensure these pipelines follow required compliance checks. The second choice requires that you scale the dedicated pipeline team to meet the number of application teams that need support. This is often prohibitively expensive.","title":"Onboarding Each Team Individually"},{"location":"concepts/framework-overview/bottom-up/#standardization-compliance","text":"When pipelines are built on a per-application basis, it becomes extremely challenging to be confident that teams throughout the organization are following approved software delivery processes aligned with cyber compliance requirements. Furthermore, for more tightly governed environments, the idea of a developer being able to modify the Jenkinsfile on their branch to perform a deployment to production is a threat vector that introduces unacceptable, unnecessary risks. Mitigating this risk requires very mature monitoring and correlation across systems.","title":"Standardization &amp; Compliance"},{"location":"concepts/framework-overview/bottom-up/#complexity","text":"There's an old adage, \"running one container is easy, running many is really, really hard.\" The same applies for pipelines within an organization. Creating a pipeline that runs unit tests, builds artifacts, deploys those artifacts, and runs security scans gets exponentially more complex the more teams and technical stacks need to be supported.","title":"Complexity"},{"location":"concepts/framework-overview/bottom-up/#a-better-way-pipeline-templating","text":"These challenges are all caused by building pipelines on a per-application basis . Even if you're modularizing your pipeline code for reuse through Jenkins Shared Libraries, you still end up duplicating the Jenkinsfile across every branch of every source code repository. Warning Don't even get me started on how tricky it can be to propagate a change to the pipeline across teams when the representation of the pipeline lives across each branch independently. Taking a step back, you may have noticed that each of the three pipelines above follow the same structure. First they execute unit tests, then they perform a build. In our experience, this simple pattern holds true most of the time ( especially when working with microservices). While the specific tools that get used to perform a step of the pipeline may change, the workflow remains the same.","title":"A Better Way: Pipeline Templating"},{"location":"concepts/framework-overview/bottom-up/#writing-a-pipeline-template","text":"The entire philosophy behind the Jenkins Template Engine stems from the concept of common workflows with interchangeable tools. What if it was possible to translate the three separate pipelines above into the following Pipeline Template: Jenkinsfile unit_test () build () Good news, this is exactly what JTE makes possible! Next, you'll need to define your implementations of the unit_test() and build() steps.","title":"Writing a Pipeline Template"},{"location":"concepts/framework-overview/bottom-up/#writing-pipeline-steps","text":"Implement the unit_test() and build() steps by refactoring the original pipelines above. Gradle: unit_test Gradle: build gradle/steps/unit_test.groovy void call (){ stage ( \"Unit Test\" ){ node { sh \"gradle verify\" } } } gradle/steps/build.groovy void call (){ stage ( \"Build\" ){ node { sh \"gradle build\" } } } Maven: unit_test Maven: build maven/steps/unit_test.groovy void call (){ stage ( \"Unit Test\" ){ node { sh \"mvn verify\" } } } maven/steps/build.groovy void call (){ stage ( \"Build\" ){ node { sh \"mvn build\" } } } NPM: unit_test NPM: build npm/steps/unit_test.groovy void call (){ stage ( \"Unit Test\" ){ node { sh \"npm ci && npm run test\" } } } npm/steps/build.groovy void call (){ stage ( \"Build\" ){ node { sh \"npm ci && npm build\" } } } Note These steps are the exact same pipeline code we had written at the start. We just wrapped it in a call method for Reasons you can learn about over on the library steps page.","title":"Writing Pipeline Steps"},{"location":"concepts/framework-overview/bottom-up/#understanding-libraries","text":"If you go back and look at the comments indicating the files those steps are placed in, you'll notice the following file structure: . \u251c\u2500\u2500 gradle \u2502 \u2514\u2500\u2500 steps \u2502 \u251c\u2500\u2500 build.groovy \u2502 \u2514\u2500\u2500 unit_test.groovy \u251c\u2500\u2500 maven \u2502 \u2514\u2500\u2500 steps \u2502 \u251c\u2500\u2500 build.groovy \u2502 \u2514\u2500\u2500 unit_test.groovy \u2514\u2500\u2500 npm \u2514\u2500\u2500 steps \u251c\u2500\u2500 build.groovy \u2514\u2500\u2500 unit_test.groovy A library in JTE is a collection of steps (stored together in a directory) that can be loaded at runtime. Earlier, a common Pipeline Template was defined that executes unit tests and then builds an artifact. This template can be shared across teams. Then, three libraries were created: npm , gradle , and maven . Each of these libraries implemented the steps required by the template. Finally, JTE needs a way to determine which libraries to load for a given team's pipeline.","title":"Understanding Libraries"},{"location":"concepts/framework-overview/bottom-up/#pipeline-configuration","text":"So far, you've defined a Pipeline Template that invokes steps, and libraries that implement those steps. The missing piece is a way to link the two. This is where the Pipeline Configuration comes in. The Pipeline Configuration uses a groovy-based configuration language to ensure the Pipeline Template uses the correct tools and settings for the application. For example, here's a Pipeline Configuration that specifies the npm library should be loaded. pipeline_config.groovy libraries { npm } When a pipeline using JTE runs with this configuration and template, the steps from the npm library are loaded. This means that the unit_test and build steps within the template will use the unit_test and build definitions provided by the npm library! By swapping out Pipeline Configurations , a single Pipeline Template can be used across multiple teams, supporting multiple tech stacks . Tip: Design Patterns in Action There are two general software architecture design patterns that might help in understanding the way JTE works. The first is the Template Method Design Pattern . You could think of the pipeline template as the AbstractClass that defines the scaffold of an algorithm (pipeline) and libraries as various implementations of a ConcreteClass that bring implementations of steps in the algorithm. The second is Dependency Injection . You could think of the JTE framework as the Injector , the pipeline template the Client , and each step a Dependency being injected.","title":"Pipeline Configuration"},{"location":"concepts/framework-overview/key-benefits/","text":"Key Benefits \u00b6 Here's a distilled explanation of why you should use JTE. JTE is a pipeline development framework for creating tool-agnostic, templated workflows that can be shared across teams creating applications with different technologies. This approach separates the business logic ( Pipeline Template ) from the technical implementation ( Pipeline Primitives ) allowing teams to configure their pipelines rather than build them from scratch. Business Value Organizational Governance Optimize Pipeline Code Reuse Simplify Pipeline Maintainability The elements of Pipeline Governance in JTE allow organizations to scale DevSecOps and have assurances that required security gates are being performed. The plug-and-play nature of Pipeline Primitives helps keep Pipeline Templates DRY. When managing more than a couple pipelines, it's simpler to manage a set of reusable Pipeline Templates in a Pipeline Catalog with modularized Pipeline Libraries than it is to copy and paste a Jenkinsfile into every repository and tweak it for the new application.","title":"Key Benefits"},{"location":"concepts/framework-overview/key-benefits/#key-benefits","text":"Here's a distilled explanation of why you should use JTE. JTE is a pipeline development framework for creating tool-agnostic, templated workflows that can be shared across teams creating applications with different technologies. This approach separates the business logic ( Pipeline Template ) from the technical implementation ( Pipeline Primitives ) allowing teams to configure their pipelines rather than build them from scratch. Business Value Organizational Governance Optimize Pipeline Code Reuse Simplify Pipeline Maintainability The elements of Pipeline Governance in JTE allow organizations to scale DevSecOps and have assurances that required security gates are being performed. The plug-and-play nature of Pipeline Primitives helps keep Pipeline Templates DRY. When managing more than a couple pipelines, it's simpler to manage a set of reusable Pipeline Templates in a Pipeline Catalog with modularized Pipeline Libraries than it is to copy and paste a Jenkinsfile into every repository and tweak it for the new application.","title":"Key Benefits"},{"location":"concepts/framework-overview/top-down/","text":"Top-Down Explanation \u00b6 Welcome ! This explanation is best suited for more experienced Jenkins users that are familiar with Jenkins pipeline's scripted syntax or software developers familiar with software design patterns. Overview \u00b6 To put it simply, the problem JTE is trying to solve is: How can organizations stop building pipelines for each application individually? The answer comes from the idea that within an organization, software development processes can be distilled into a subset of generic workflows. Regardless of which tools are being used, the process often says the same. Teams are typically going to run unit tests, build a software artifact, scan it, deploy it somewhere, test it some more, and promote that artifact to higher Application Environments. Some teams do more, some teams do less, but it doesn't matter if that process uses npm , sonarqube , docker , and helm or gradle , fortify , and ansible ; the process is the same. As depicted in Figure 1, JTE allows you to take that process and represent it as a tool-agnostic Pipeline Template. This abstract Pipeline Template can then be made concrete by loading Pipeline Primitives such as steps. Which Pipeline Primitives to inject are determined by a Pipeline Configuration. Figure 1 Tip: Design Patterns in Action There are two general software architecture design patterns that might help in understanding the way JTE works. The first is the Template Method Design Pattern . You could think of the pipeline template as the AbstractClass that defines the scaffold of an algorithm (pipeline) and libraries as various implementations of a ConcreteClass that bring implementations of steps in the algorithm. The second is Dependency Injection . You could think of the JTE framework as the Injector , the pipeline template the Client , and each step a Dependency being injected. Pipeline Templates \u00b6 A Pipeline Template is nothing more than a Jenkinsfile with some stuff added at runtime. Everything that can be done in a Jenkinsfile can be done in a Pipeline Template. The only thing that makes a template a template is the use of Pipeline Primitives such that a single template can be used for multiple teams and multiple tools. A visualization of a Pipeline Template with steps being swapped in an out Pipeline Primitives \u00b6 Pipeline Primitives are contributed by the JTE framework and help make templates reusable. Pipeline Templates will typically make use of identically named Pipeline Primitives, such as step called build() , to become reusable. Pipeline Configuration \u00b6 The Pipeline Configuration is where teams declare which Pipeline Primitives should be injected into the Pipeline Template for their application(s).","title":"Top-Down Explanation"},{"location":"concepts/framework-overview/top-down/#top-down-explanation","text":"Welcome ! This explanation is best suited for more experienced Jenkins users that are familiar with Jenkins pipeline's scripted syntax or software developers familiar with software design patterns.","title":"Top-Down Explanation"},{"location":"concepts/framework-overview/top-down/#overview","text":"To put it simply, the problem JTE is trying to solve is: How can organizations stop building pipelines for each application individually? The answer comes from the idea that within an organization, software development processes can be distilled into a subset of generic workflows. Regardless of which tools are being used, the process often says the same. Teams are typically going to run unit tests, build a software artifact, scan it, deploy it somewhere, test it some more, and promote that artifact to higher Application Environments. Some teams do more, some teams do less, but it doesn't matter if that process uses npm , sonarqube , docker , and helm or gradle , fortify , and ansible ; the process is the same. As depicted in Figure 1, JTE allows you to take that process and represent it as a tool-agnostic Pipeline Template. This abstract Pipeline Template can then be made concrete by loading Pipeline Primitives such as steps. Which Pipeline Primitives to inject are determined by a Pipeline Configuration. Figure 1 Tip: Design Patterns in Action There are two general software architecture design patterns that might help in understanding the way JTE works. The first is the Template Method Design Pattern . You could think of the pipeline template as the AbstractClass that defines the scaffold of an algorithm (pipeline) and libraries as various implementations of a ConcreteClass that bring implementations of steps in the algorithm. The second is Dependency Injection . You could think of the JTE framework as the Injector , the pipeline template the Client , and each step a Dependency being injected.","title":"Overview"},{"location":"concepts/framework-overview/top-down/#pipeline-templates","text":"A Pipeline Template is nothing more than a Jenkinsfile with some stuff added at runtime. Everything that can be done in a Jenkinsfile can be done in a Pipeline Template. The only thing that makes a template a template is the use of Pipeline Primitives such that a single template can be used for multiple teams and multiple tools. A visualization of a Pipeline Template with steps being swapped in an out","title":"Pipeline Templates"},{"location":"concepts/framework-overview/top-down/#pipeline-primitives","text":"Pipeline Primitives are contributed by the JTE framework and help make templates reusable. Pipeline Templates will typically make use of identically named Pipeline Primitives, such as step called build() , to become reusable.","title":"Pipeline Primitives"},{"location":"concepts/framework-overview/top-down/#pipeline-configuration","text":"The Pipeline Configuration is where teams declare which Pipeline Primitives should be injected into the Pipeline Template for their application(s).","title":"Pipeline Configuration"},{"location":"concepts/framework-overview/snippets/design-patterns/","text":"Tip: Design Patterns in Action There are two general software architecture design patterns that might help in understanding the way JTE works. The first is the Template Method Design Pattern . You could think of the pipeline template as the AbstractClass that defines the scaffold of an algorithm (pipeline) and libraries as various implementations of a ConcreteClass that bring implementations of steps in the algorithm. The second is Dependency Injection . You could think of the JTE framework as the Injector , the pipeline template the Client , and each step a Dependency being injected.","title":"Design patterns"},{"location":"concepts/framework-overview/snippets/getting-started/","text":"banana \u00b6","title":"banana"},{"location":"concepts/framework-overview/snippets/getting-started/#banana","text":"","title":"banana"},{"location":"concepts/library-development/","text":"Overview \u00b6 Pipeline Libraries are the foundation of the Jenkins Templating Engine to allow Pipeline Templates to be shared across teams. Libraries provide steps that can be invoked from templates. Library Repository Scaffold Check out this starter repository to help you get off on the right foot. Loading Libraries \u00b6 The libraries{} block within the Pipeline Configuration defines which libraries will be loaded for a particular Pipeline Run. Learn More \u00b6 Topic Description Library Structure Learn how files are organized within a library. Library Steps Learn how to add steps to a library. Library Resources Learn how to use static assets from within Library Steps Library Classes Learn how to define classes within a library. Parameterizing Libraries Learn how to make libraries configurable from the Pipeline Configuration Library Configuration File Learn how to validate library parameters using the library configuration file Library Sources Learn how to make a library discoverable for loading. Lifecycle Hooks Learn how to register steps for implicit invocation. Multi-Method Library Steps Learn how to define multiple methods per step. Step Aliasing Learn how to invoke the same step by multiple names.","title":"Overview"},{"location":"concepts/library-development/#overview","text":"Pipeline Libraries are the foundation of the Jenkins Templating Engine to allow Pipeline Templates to be shared across teams. Libraries provide steps that can be invoked from templates. Library Repository Scaffold Check out this starter repository to help you get off on the right foot.","title":"Overview"},{"location":"concepts/library-development/#loading-libraries","text":"The libraries{} block within the Pipeline Configuration defines which libraries will be loaded for a particular Pipeline Run.","title":"Loading Libraries"},{"location":"concepts/library-development/#learn-more","text":"Topic Description Library Structure Learn how files are organized within a library. Library Steps Learn how to add steps to a library. Library Resources Learn how to use static assets from within Library Steps Library Classes Learn how to define classes within a library. Parameterizing Libraries Learn how to make libraries configurable from the Pipeline Configuration Library Configuration File Learn how to validate library parameters using the library configuration file Library Sources Learn how to make a library discoverable for loading. Lifecycle Hooks Learn how to register steps for implicit invocation. Multi-Method Library Steps Learn how to define multiple methods per step. Step Aliasing Learn how to invoke the same step by multiple names.","title":"Learn More"},{"location":"concepts/library-development/library-classes/","text":"Library Classes \u00b6 Libraries have the option to provide groovy classes. These classes should be placed in the src directory of the library. Tip Each loaded library's src directory contents are synchronized to a common directory. Take care not to load two libraries that provide the same class. One way to avoid this is to name the package after the contributing library. For example, if the library's name was example then put the library's classes in the src/example/ directory with package example at the top of the class files. Class Serializability \u00b6 Library classes should implement the Serializable interface whenever possible. For example, a Utility class coming from an example library: example.groovy package example class Utility implements Serializable {} This is because Jenkins pipeline's implement a design pattern called Continuation Passing Style (CPS) so that individual Pipeline Runs can resume progress. Note To learn more, check out Best Practices for Avoiding Serializability Exceptions Classpath \u00b6 Classes contributed by loaded libraries can be imported from the library's steps, steps from other libraries, and the Pipeline Template. Warning Importing a library class from a Pipeline Template or from a step outside the library will lead to tight coupling. In general, library classes should be utilized within steps from the same library Jenkins Step Resolution \u00b6 Library classes can not resolve Jenkins pipeline DSL functions such as sh or echo . A work around for this is to pass the steps variable to the class constructor to store on a field or through a method parameter. For example, to use the echo pipeline step one could do the following: Utility Class Library Step example.groovy package example class Utility implements Serializable { void doThing ( steps ){ steps . echo \"message from the Utility class\" } } echo_example.groovy import example.Utility void call (){ Utility u = new Utility () u . doThing ( steps ) } Accessing the Library Configuration \u00b6 Unlike with library steps, the config and pipelineConfig variables aren't autowired to library classes. To access these variables, they can be passed to the class through constructor or method parameters. Utility Class Library Step example.groovy package example class Utility implements Serializable { def config Utility ( config ){ this . config = config } void doThing ( steps ){ steps . echo \"library config: ${config}\" } } get_config.groovy import example.Utility void call (){ Utility u = new Utility ( config ) u . doThing ( steps ) }","title":"Library Classes"},{"location":"concepts/library-development/library-classes/#library-classes","text":"Libraries have the option to provide groovy classes. These classes should be placed in the src directory of the library. Tip Each loaded library's src directory contents are synchronized to a common directory. Take care not to load two libraries that provide the same class. One way to avoid this is to name the package after the contributing library. For example, if the library's name was example then put the library's classes in the src/example/ directory with package example at the top of the class files.","title":"Library Classes"},{"location":"concepts/library-development/library-classes/#class-serializability","text":"Library classes should implement the Serializable interface whenever possible. For example, a Utility class coming from an example library: example.groovy package example class Utility implements Serializable {} This is because Jenkins pipeline's implement a design pattern called Continuation Passing Style (CPS) so that individual Pipeline Runs can resume progress. Note To learn more, check out Best Practices for Avoiding Serializability Exceptions","title":"Class Serializability"},{"location":"concepts/library-development/library-classes/#classpath","text":"Classes contributed by loaded libraries can be imported from the library's steps, steps from other libraries, and the Pipeline Template. Warning Importing a library class from a Pipeline Template or from a step outside the library will lead to tight coupling. In general, library classes should be utilized within steps from the same library","title":"Classpath"},{"location":"concepts/library-development/library-classes/#jenkins-step-resolution","text":"Library classes can not resolve Jenkins pipeline DSL functions such as sh or echo . A work around for this is to pass the steps variable to the class constructor to store on a field or through a method parameter. For example, to use the echo pipeline step one could do the following: Utility Class Library Step example.groovy package example class Utility implements Serializable { void doThing ( steps ){ steps . echo \"message from the Utility class\" } } echo_example.groovy import example.Utility void call (){ Utility u = new Utility () u . doThing ( steps ) }","title":"Jenkins Step Resolution"},{"location":"concepts/library-development/library-classes/#accessing-the-library-configuration","text":"Unlike with library steps, the config and pipelineConfig variables aren't autowired to library classes. To access these variables, they can be passed to the class through constructor or method parameters. Utility Class Library Step example.groovy package example class Utility implements Serializable { def config Utility ( config ){ this . config = config } void doThing ( steps ){ steps . echo \"library config: ${config}\" } } get_config.groovy import example.Utility void call (){ Utility u = new Utility ( config ) u . doThing ( steps ) }","title":"Accessing the Library Configuration"},{"location":"concepts/library-development/library-configuration-file/","text":"Library Configuration File \u00b6 The root of a library can contain an optional library_config.groovy file that captures metadata about the library. Library Parameter Validation \u00b6 Currently, the library configuration file is only used to validate library configurations. Reference A comprehensive overview of the library configuration schema can be found in the Reference section. Advanced Library Validations \u00b6 For library parameter validations that are more complex than what can be accomplished through the library configuration functionality, library developers can alternatively create a step annotated with the @Validate Lifecycle Hook . Methods within steps annotated with @Validate will execute before the Pipeline Template. For example, if a library wanted to validate a more complex use case such as ensuring a library parameter named threshold was greater than or equal to zero but less than or equal to 100 the following could be implemented: threshold_check.groovy @Validate // (1) void call ( context ){ // (2) if ( config . threshold < 0 || config . threshold > 100 ){ // (3) error \"Library parameter 'threshold' must be within the range of: 0 <= threshold <= 100\" // (4) } } The @Validate annotation marks a method defined within a step to be invoked before template execution. This example defines a call() method, but the method name can be any valid Groovy method name. Here, a Groovy if statement is used to validate that the threshold parameter fall within a certain range. If the threshold variable doesn't meet the criteria, the Jenkins pipeline error step is used to fail the build. The warning step could also be used if the pipeline user should be notified but the build should continue. This approach allows library developers to use Groovy to validate arbitrarily complex library parameter constraints. The method annotated with @Validate can be in its own step file or added as an additional method within an existing step file. Note The example above assumes that the threshold library parameter has been configured as part of the Pipeline Configuration. This could be also be validated using Groovy or by combining the functionality of the library configuration file to set the threshold parameter as a required field that must be a Number.","title":"Library Configuration File"},{"location":"concepts/library-development/library-configuration-file/#library-configuration-file","text":"The root of a library can contain an optional library_config.groovy file that captures metadata about the library.","title":"Library Configuration File"},{"location":"concepts/library-development/library-configuration-file/#library-parameter-validation","text":"Currently, the library configuration file is only used to validate library configurations. Reference A comprehensive overview of the library configuration schema can be found in the Reference section.","title":"Library Parameter Validation"},{"location":"concepts/library-development/library-configuration-file/#advanced-library-validations","text":"For library parameter validations that are more complex than what can be accomplished through the library configuration functionality, library developers can alternatively create a step annotated with the @Validate Lifecycle Hook . Methods within steps annotated with @Validate will execute before the Pipeline Template. For example, if a library wanted to validate a more complex use case such as ensuring a library parameter named threshold was greater than or equal to zero but less than or equal to 100 the following could be implemented: threshold_check.groovy @Validate // (1) void call ( context ){ // (2) if ( config . threshold < 0 || config . threshold > 100 ){ // (3) error \"Library parameter 'threshold' must be within the range of: 0 <= threshold <= 100\" // (4) } } The @Validate annotation marks a method defined within a step to be invoked before template execution. This example defines a call() method, but the method name can be any valid Groovy method name. Here, a Groovy if statement is used to validate that the threshold parameter fall within a certain range. If the threshold variable doesn't meet the criteria, the Jenkins pipeline error step is used to fail the build. The warning step could also be used if the pipeline user should be notified but the build should continue. This approach allows library developers to use Groovy to validate arbitrarily complex library parameter constraints. The method annotated with @Validate can be in its own step file or added as an additional method within an existing step file. Note The example above assumes that the threshold library parameter has been configured as part of the Pipeline Configuration. This could be also be validated using Groovy or by combining the functionality of the library configuration file to set the threshold parameter as a required field that must be a Number.","title":"Advanced Library Validations"},{"location":"concepts/library-development/library-resources/","text":"Library Resources \u00b6 Libraries can store static assets, such as shell scripts or YAML files, in a resources directory. Accessing A Library Resource \u00b6 Within a Library Step , the resource(String relativePath) method can be used to return the file contents of a resource as a String . Example In the following example, a Library Step is created that executes a script from the root of the resources directory and then reads in data from a YAML file nested within the resources directory. Library Structure Library Step exampleLibraryName \u251c\u2500\u2500 steps \u2502 \u2514\u2500\u2500 step.groovy \u251c\u2500\u2500 resources \u2502 \u251c\u2500\u2500 doSomething.sh \u2502 \u2514\u2500\u2500 nested \u2502 \u2514\u2500\u2500 data.yaml \u2514\u2500\u2500 library_config.groovy step.groovy void call (){ String script = resource ( \"doSomething.sh\" ) def data = readYaml text: resource ( \"nested/data.yaml\" ) } Important The path parameter passed to the resource method must be a relative path within the resources directory Only steps within a library can access the library's resources (no cross-library resource fetching) The resource() method is only available within Library Steps and can't be invoked from the Pipeline Template","title":"Library Resources"},{"location":"concepts/library-development/library-resources/#library-resources","text":"Libraries can store static assets, such as shell scripts or YAML files, in a resources directory.","title":"Library Resources"},{"location":"concepts/library-development/library-resources/#accessing-a-library-resource","text":"Within a Library Step , the resource(String relativePath) method can be used to return the file contents of a resource as a String . Example In the following example, a Library Step is created that executes a script from the root of the resources directory and then reads in data from a YAML file nested within the resources directory. Library Structure Library Step exampleLibraryName \u251c\u2500\u2500 steps \u2502 \u2514\u2500\u2500 step.groovy \u251c\u2500\u2500 resources \u2502 \u251c\u2500\u2500 doSomething.sh \u2502 \u2514\u2500\u2500 nested \u2502 \u2514\u2500\u2500 data.yaml \u2514\u2500\u2500 library_config.groovy step.groovy void call (){ String script = resource ( \"doSomething.sh\" ) def data = readYaml text: resource ( \"nested/data.yaml\" ) } Important The path parameter passed to the resource method must be a relative path within the resources directory Only steps within a library can access the library's resources (no cross-library resource fetching) The resource() method is only available within Library Steps and can't be invoked from the Pipeline Template","title":"Accessing A Library Resource"},{"location":"concepts/library-development/library-source/","text":"Library Source \u00b6 A Library Source is a reference to a location where one or more libraries can be fetched. Library Source Structure \u00b6 Within a configured Library Source, a library is a directory . The name of the directory is the identifier that would be declared in the libraries{} block of the Pipeline Configuration. Library Providers \u00b6 The Jenkins Templating Engine plugin provides an interface to create Library Sources. The plugin comes with two types of Library Sources built-in: SCM Library Sources and Plugin Library Sources. Source Code Library Source \u00b6 The SCM Library Source is used to fetch libraries from a source code repository. This repository can be a local directory with a .git directory accessible from the Jenkins Controller or a remote repository. Plugin Library Source \u00b6 The Plugin Library Source is used when libraries have been bundled into a separate plugin. This option will only be available in the Jenkins UI when a plugin has been installed that can serve as a library-providing plugin. Learn More To learn more, check out How to Package a Library Source as a Plugin","title":"Library Source"},{"location":"concepts/library-development/library-source/#library-source","text":"A Library Source is a reference to a location where one or more libraries can be fetched.","title":"Library Source"},{"location":"concepts/library-development/library-source/#library-source-structure","text":"Within a configured Library Source, a library is a directory . The name of the directory is the identifier that would be declared in the libraries{} block of the Pipeline Configuration.","title":"Library Source Structure"},{"location":"concepts/library-development/library-source/#library-providers","text":"The Jenkins Templating Engine plugin provides an interface to create Library Sources. The plugin comes with two types of Library Sources built-in: SCM Library Sources and Plugin Library Sources.","title":"Library Providers"},{"location":"concepts/library-development/library-source/#source-code-library-source","text":"The SCM Library Source is used to fetch libraries from a source code repository. This repository can be a local directory with a .git directory accessible from the Jenkins Controller or a remote repository.","title":"Source Code Library Source"},{"location":"concepts/library-development/library-source/#plugin-library-source","text":"The Plugin Library Source is used when libraries have been bundled into a separate plugin. This option will only be available in the Jenkins UI when a plugin has been installed that can serve as a library-providing plugin. Learn More To learn more, check out How to Package a Library Source as a Plugin","title":"Plugin Library Source"},{"location":"concepts/library-development/library-steps/","text":"Library Steps \u00b6 Library Steps are a mechanism for modularizing pipeline functionality. Naming A Step \u00b6 By default, the name of the step that's loaded is based on the filename without the .groovy extension. This can be modified using Step Aliasing . Best Practice It's recommended that Step Aliasing only be used when actually necessary. The call Method \u00b6 Most steps should implement the call method. library_step.groovy void call (){} This makes it such that the step can be invoked via its name. For example, a step named build.groovy that has implemented a call method can be invoked via build() . Why the Call Method? Curious readers commonly ask, \"Why the call method?\" The answer comes from the Groovy Call Operator . Essentially, build() is equivalent to build.call() in groovy. Autowired Variables \u00b6 All Library Steps are autowired with several variables: Variable Description config The library's block of configuration for the library that contributed the step. stepContext Information about the step that's currently running. stageContext Information about the current Stage , if applicable hookContext If this step was triggered by a Lifecycle Hook , information about the trigger Reference For more information, check out the Autowired Variables page Method Parameters \u00b6 Library Steps can accept method parameters just like any other method. Library Step Method Parameters Library Step Step Invocation printMessage.groovy void call ( String message ){ println \"here's your message: ${message}\" } printMessage ( \"hello, world!\" ) Be Careful! Library Steps that accept method parameters run a high risk of breaking the interoperability of the Pipeline Template . Imagine the scenario where the Pipeline Template invokes a build() step and the same template is intended to be used across teams that may be using different tools, such as gradle and npm . If the gradle library's build() step accepts a set of parameters and the npm library's build() step doesn't then you won't be able to swap out the libraries interchangeably. Instead of method parameters, consider passing steps information via the Pipeline Configuration using the config variable. Check out the Parameterizing Libraries page to learn more. The exception to the rule of thumb regarding method parameters is when the method parameters are Pipeline Primitives. This works because the parameters can then be interchanged safely along with the implementation of the step that's accepting them as an argument. The most common example is creating a deployment step. Frequently, teams will create a deploy_to step that accepts an Application Environment as an argument. Deployment Steps The following Pipeline Template, Pipeline Configuration, and Deployment step demonstrate a safe use of a library step accepting a method parameter. Pipeline Template Pipeline Configuration Deployment Step Jenkinsfile unit_test () build () deploy_to dev smoke_test () deploy_to prod pipeline_config.groovy libraries { npm // contributes unit_test, build cypress // contributes integration_test ansible // contributes deploy_to } application_environments { dev { ip = \"1.1.1.1\" } prod { ip = \"2.2.2.2\" } } ansible/steps/deploy_to.groovy void call ( app_env ){ println \"deploying to the ip: ${app_env.ip}\" } Advanced Topics \u00b6 This page has covered the basics, if you're ready for more check out the following pages: Topic Description Lifecycle Hooks Learn how to trigger Library Steps implicitly. Multi-Method Library Steps Learn how to define more than one method in a step. Step Aliasing Learn how to call the same step by multiple names.","title":"Library Steps"},{"location":"concepts/library-development/library-steps/#library-steps","text":"Library Steps are a mechanism for modularizing pipeline functionality.","title":"Library Steps"},{"location":"concepts/library-development/library-steps/#naming-a-step","text":"By default, the name of the step that's loaded is based on the filename without the .groovy extension. This can be modified using Step Aliasing . Best Practice It's recommended that Step Aliasing only be used when actually necessary.","title":"Naming A Step"},{"location":"concepts/library-development/library-steps/#the-call-method","text":"Most steps should implement the call method. library_step.groovy void call (){} This makes it such that the step can be invoked via its name. For example, a step named build.groovy that has implemented a call method can be invoked via build() . Why the Call Method? Curious readers commonly ask, \"Why the call method?\" The answer comes from the Groovy Call Operator . Essentially, build() is equivalent to build.call() in groovy.","title":"The call Method"},{"location":"concepts/library-development/library-steps/#autowired-variables","text":"All Library Steps are autowired with several variables: Variable Description config The library's block of configuration for the library that contributed the step. stepContext Information about the step that's currently running. stageContext Information about the current Stage , if applicable hookContext If this step was triggered by a Lifecycle Hook , information about the trigger Reference For more information, check out the Autowired Variables page","title":"Autowired Variables"},{"location":"concepts/library-development/library-steps/#method-parameters","text":"Library Steps can accept method parameters just like any other method. Library Step Method Parameters Library Step Step Invocation printMessage.groovy void call ( String message ){ println \"here's your message: ${message}\" } printMessage ( \"hello, world!\" ) Be Careful! Library Steps that accept method parameters run a high risk of breaking the interoperability of the Pipeline Template . Imagine the scenario where the Pipeline Template invokes a build() step and the same template is intended to be used across teams that may be using different tools, such as gradle and npm . If the gradle library's build() step accepts a set of parameters and the npm library's build() step doesn't then you won't be able to swap out the libraries interchangeably. Instead of method parameters, consider passing steps information via the Pipeline Configuration using the config variable. Check out the Parameterizing Libraries page to learn more. The exception to the rule of thumb regarding method parameters is when the method parameters are Pipeline Primitives. This works because the parameters can then be interchanged safely along with the implementation of the step that's accepting them as an argument. The most common example is creating a deployment step. Frequently, teams will create a deploy_to step that accepts an Application Environment as an argument. Deployment Steps The following Pipeline Template, Pipeline Configuration, and Deployment step demonstrate a safe use of a library step accepting a method parameter. Pipeline Template Pipeline Configuration Deployment Step Jenkinsfile unit_test () build () deploy_to dev smoke_test () deploy_to prod pipeline_config.groovy libraries { npm // contributes unit_test, build cypress // contributes integration_test ansible // contributes deploy_to } application_environments { dev { ip = \"1.1.1.1\" } prod { ip = \"2.2.2.2\" } } ansible/steps/deploy_to.groovy void call ( app_env ){ println \"deploying to the ip: ${app_env.ip}\" }","title":"Method Parameters"},{"location":"concepts/library-development/library-steps/#advanced-topics","text":"This page has covered the basics, if you're ready for more check out the following pages: Topic Description Lifecycle Hooks Learn how to trigger Library Steps implicitly. Multi-Method Library Steps Learn how to define more than one method in a step. Step Aliasing Learn how to call the same step by multiple names.","title":"Advanced Topics"},{"location":"concepts/library-development/library-structure/","text":"Library Structure \u00b6 Overview \u00b6 Each directory within a Library Source is a different library that can be loaded via the Pipeline Configuration The name of the directory is the library identifier used within the Pipeline Configuration libraries{} block when loading the library. Path Description steps/**/*.groovy groovy files under the steps directory will be loaded as steps where the basename of the file will be the name of the function made available in the pipeline resources/**/* any file under the resources directory will be accessible from within Library Steps via the resource() step src/**/* Classes contributed by the library that can be imported from within Pipeline Templates and steps library_config.groovy the library configuration file Example Library Structure \u00b6 exampleLibraryName # (1) \u251c\u2500\u2500 steps # (2) \u2502 \u2514\u2500\u2500 step1.groovy # (3) \u2502 \u2514\u2500\u2500 step2.groovy \u251c\u2500\u2500 resources # (4) \u2502 \u251c\u2500\u2500 someResource.txt # (5) \u2502 \u2514\u2500\u2500 nested \u2502 \u2514\u2500\u2500 anotherResource.json # (6) \u251c\u2500\u2500 src # (7) \u2502 \u2514\u2500\u2500 example \u2502 \u2514\u2500\u2500 Utility.groovy # (8) \u2514\u2500\u2500 library_config.groovy # (9) This library would be loaded via the exampleLibraryName identifier in the libraries{} block All steps contributed by the library goes in the steps directory An example step. A step1 step would be added to the pipeline All library resources go in the resources directory A root level resource. The contents could be fetched from step1 or step2 via resource(\"someResource.txt\") A nested resource. The contents could be fetched from step1 or step2 via resource(\"nested/anotherResource.json\") File paths within the src directory must be unique across libraries loaded and will be made available to the Class Loader for both steps and templates A class file containing the example.Utility class. The library configuration file","title":"Library Structure"},{"location":"concepts/library-development/library-structure/#library-structure","text":"","title":"Library Structure"},{"location":"concepts/library-development/library-structure/#overview","text":"Each directory within a Library Source is a different library that can be loaded via the Pipeline Configuration The name of the directory is the library identifier used within the Pipeline Configuration libraries{} block when loading the library. Path Description steps/**/*.groovy groovy files under the steps directory will be loaded as steps where the basename of the file will be the name of the function made available in the pipeline resources/**/* any file under the resources directory will be accessible from within Library Steps via the resource() step src/**/* Classes contributed by the library that can be imported from within Pipeline Templates and steps library_config.groovy the library configuration file","title":"Overview"},{"location":"concepts/library-development/library-structure/#example-library-structure","text":"exampleLibraryName # (1) \u251c\u2500\u2500 steps # (2) \u2502 \u2514\u2500\u2500 step1.groovy # (3) \u2502 \u2514\u2500\u2500 step2.groovy \u251c\u2500\u2500 resources # (4) \u2502 \u251c\u2500\u2500 someResource.txt # (5) \u2502 \u2514\u2500\u2500 nested \u2502 \u2514\u2500\u2500 anotherResource.json # (6) \u251c\u2500\u2500 src # (7) \u2502 \u2514\u2500\u2500 example \u2502 \u2514\u2500\u2500 Utility.groovy # (8) \u2514\u2500\u2500 library_config.groovy # (9) This library would be loaded via the exampleLibraryName identifier in the libraries{} block All steps contributed by the library goes in the steps directory An example step. A step1 step would be added to the pipeline All library resources go in the resources directory A root level resource. The contents could be fetched from step1 or step2 via resource(\"someResource.txt\") A nested resource. The contents could be fetched from step1 or step2 via resource(\"nested/anotherResource.json\") File paths within the src directory must be unique across libraries loaded and will be made available to the Class Loader for both steps and templates A class file containing the example.Utility class. The library configuration file","title":"Example Library Structure"},{"location":"concepts/library-development/lifecycle-hooks/","text":"Lifecycle Hooks \u00b6 Sometimes it's necessary to trigger specific pipeline actions at certain times during pipeline execution. For example, if you wanted to send multiple notification types after a particular pipeline step or at the conclusion of a pipeline if the build was failure. JTE supports this type of Aspect Oriented Programming style event handling through annotation markers that can be placed on methods defined within Library Steps. Hook Types \u00b6 The following lifecycle hook annotations are available: Annotation Trigger @Validate Beginning of a Pipeline Run, before the Pipeline Template @Init After all @Validate hooks, before the Pipeline Template @BeforeStep During template execution, before every Library Step @AfterStep During template execution, after every Library Step @CleanUp After template execution @Notify During template execution after every Library Step and after template execution Hook Context \u00b6 Lifecycle Hook annotations can be placed on any method inside a step. Every step has an autowired hookContext variable which provides steps with relevant information about what triggered the hook. Property Type Description library String The library that contributed the step that triggered the Lifecycle Hook. Is null when the Lifecycle Hook wasn't triggered by a step. step String The name of the Library Step that triggered the Lifecycle Hook. Is null when the Lifecycle Hook wasn't triggered by a step. methodName String The name of the method within the step that was invoked to trigger the Lifecycle Hook. Is null when the Lifecycle Hook wasn't triggered by a step. exceptionThrown Boolean When the hook is triggered by a step, this refers to if the step triggering step threw an exception. When the hook is triggered by template completion, refers to if there is an uncaught exception that will fail the pipeline. Conditional Hook Execution \u00b6 Sometimes you'll only want to invoke the Hook when certain conditions are met, such as a build failure or in relation to another step (like before static code analysis). Each annotation accepts a Closure parameter. If the return object of this closure is truthy then the hook will be executed. While executing, the code within the Closure parameter will be able to resolve the hookContext variable, the library configuration of the library that loads the step via the config variable, and the currentBuild variable made available in Jenkins Pipelines. Example Hook Usage library_step.groovy @BeforeStep ({ hookContext . step . equals ( \"build\" ) }) void call (){ // execute something right before the Library Step called build is executed. } Note The closure parameter is optional. If omitted, the hook will always be executed.","title":"Lifecycle Hooks"},{"location":"concepts/library-development/lifecycle-hooks/#lifecycle-hooks","text":"Sometimes it's necessary to trigger specific pipeline actions at certain times during pipeline execution. For example, if you wanted to send multiple notification types after a particular pipeline step or at the conclusion of a pipeline if the build was failure. JTE supports this type of Aspect Oriented Programming style event handling through annotation markers that can be placed on methods defined within Library Steps.","title":"Lifecycle Hooks"},{"location":"concepts/library-development/lifecycle-hooks/#hook-types","text":"The following lifecycle hook annotations are available: Annotation Trigger @Validate Beginning of a Pipeline Run, before the Pipeline Template @Init After all @Validate hooks, before the Pipeline Template @BeforeStep During template execution, before every Library Step @AfterStep During template execution, after every Library Step @CleanUp After template execution @Notify During template execution after every Library Step and after template execution","title":"Hook Types"},{"location":"concepts/library-development/lifecycle-hooks/#hook-context","text":"Lifecycle Hook annotations can be placed on any method inside a step. Every step has an autowired hookContext variable which provides steps with relevant information about what triggered the hook. Property Type Description library String The library that contributed the step that triggered the Lifecycle Hook. Is null when the Lifecycle Hook wasn't triggered by a step. step String The name of the Library Step that triggered the Lifecycle Hook. Is null when the Lifecycle Hook wasn't triggered by a step. methodName String The name of the method within the step that was invoked to trigger the Lifecycle Hook. Is null when the Lifecycle Hook wasn't triggered by a step. exceptionThrown Boolean When the hook is triggered by a step, this refers to if the step triggering step threw an exception. When the hook is triggered by template completion, refers to if there is an uncaught exception that will fail the pipeline.","title":"Hook Context"},{"location":"concepts/library-development/lifecycle-hooks/#conditional-hook-execution","text":"Sometimes you'll only want to invoke the Hook when certain conditions are met, such as a build failure or in relation to another step (like before static code analysis). Each annotation accepts a Closure parameter. If the return object of this closure is truthy then the hook will be executed. While executing, the code within the Closure parameter will be able to resolve the hookContext variable, the library configuration of the library that loads the step via the config variable, and the currentBuild variable made available in Jenkins Pipelines. Example Hook Usage library_step.groovy @BeforeStep ({ hookContext . step . equals ( \"build\" ) }) void call (){ // execute something right before the Library Step called build is executed. } Note The closure parameter is optional. If omitted, the hook will always be executed.","title":"Conditional Hook Execution"},{"location":"concepts/library-development/multi-method-steps/","text":"Multi-Method Library Steps \u00b6 Typically, Library Steps define a call() method that allows the step to be invoked via its name (such as build() ). This isn't required. Groovy's Call Operator 1 means that invoking build() functionally equivalent to invoking build.call() . Steps, therefore, can define alternative methods beyond just the call() method. Use Case: Utility Steps \u00b6 Multi-Method Library Steps are most useful when creating Library Steps that wrap a particular utility. The methods on the step can then represent different actions the utility can take. Utility Step Example: Git Git Utility Step Usage git.groovy void add ( String files ){ sh \"git add ${files}\" } void commit ( String message ){ sh \"git commit -m ${message}\" } void push (){ sh \"git push\" } Jenkinsfile node { checkout scm writeFile file: 'test.txt' , text: 'hello, world' git . add ( 'test.txt' ) git . commit ( 'add test file' ) git . push () } Groovy Command Chain \u00b6 Expressive DSLs can be created when coupling multi-method steps with Groovy's Command Chain feature. Using Command Chains With The Git Utility Command Chains could be used to improve upon the previous example. Without Command Chains With Command Chains Jenkinsfile node { checkout scm writeFile file: 'test.txt' , text: 'hello, world' git . add ( 'test.txt' ) git . commit ( 'add test file' ) git . push () } Jenkinsfile node { checkout scm writeFile file: 'test.txt' , text: 'hello, world' git add 'test.txt' git commit 'add test file' git . push () } Groovy Call Operator \u21a9","title":"Multi-Method Library Steps"},{"location":"concepts/library-development/multi-method-steps/#multi-method-library-steps","text":"Typically, Library Steps define a call() method that allows the step to be invoked via its name (such as build() ). This isn't required. Groovy's Call Operator 1 means that invoking build() functionally equivalent to invoking build.call() . Steps, therefore, can define alternative methods beyond just the call() method.","title":"Multi-Method Library Steps"},{"location":"concepts/library-development/multi-method-steps/#use-case-utility-steps","text":"Multi-Method Library Steps are most useful when creating Library Steps that wrap a particular utility. The methods on the step can then represent different actions the utility can take. Utility Step Example: Git Git Utility Step Usage git.groovy void add ( String files ){ sh \"git add ${files}\" } void commit ( String message ){ sh \"git commit -m ${message}\" } void push (){ sh \"git push\" } Jenkinsfile node { checkout scm writeFile file: 'test.txt' , text: 'hello, world' git . add ( 'test.txt' ) git . commit ( 'add test file' ) git . push () }","title":"Use Case: Utility Steps"},{"location":"concepts/library-development/multi-method-steps/#groovy-command-chain","text":"Expressive DSLs can be created when coupling multi-method steps with Groovy's Command Chain feature. Using Command Chains With The Git Utility Command Chains could be used to improve upon the previous example. Without Command Chains With Command Chains Jenkinsfile node { checkout scm writeFile file: 'test.txt' , text: 'hello, world' git . add ( 'test.txt' ) git . commit ( 'add test file' ) git . push () } Jenkinsfile node { checkout scm writeFile file: 'test.txt' , text: 'hello, world' git add 'test.txt' git commit 'add test file' git . push () } Groovy Call Operator \u21a9","title":"Groovy Command Chain"},{"location":"concepts/library-development/parameterizing-libraries/","text":"Parameterizing Libraries \u00b6 One of the major benefits of organizing your pipeline code into libraries is the ability to reuse these libraries across different teams. To achieve this level of reusability, it's best to externalize hard coded values as parameters that can be set from the Pipeline Configuration repository. Pass Parameters Through the Pipeline Configuration \u00b6 When specifying a library to be loaded, users can pass arbitrary configurations to the library: pipeline_config.groovy libraries { example { // (1) someField = \"my value\" // (2) nested { // (3) someOtherField = 11 // (4) } } } The name of the library to be loaded A root level library configuration option A block name to pass nested configuration A nested library configuration Parameter Structure and Type \u00b6 Library parameters can take an arbitrary structure. All parameters can be at the root level or a nested structure can be created to group related configurations together. Library parameter values can be any serializable Groovy primitive. Typically, parameters are boolean, numeric, String, or array. Accessing Library Configurations Within Steps \u00b6 The Jenkins Templating Engine injects a config variable into each step. This config variable is a map whose keys are the library parameters that have been provided through the Pipeline Configuration. The config variable is only resolvable within a Library Step and only contains the configuration for the step's library. Note If you need to access the entire aggregated Pipeline Configuration, JTE injects a pipelineConfig variable that can be accessed anywhere. Validating Library Configurations \u00b6 The Pipeline Configuration doesn't inherently perform type checking or validation. Library developers can choose to provide a Library Configuration File at the root of the library's directory which will assist with library parameter validation.","title":"Parameterizing Libraries"},{"location":"concepts/library-development/parameterizing-libraries/#parameterizing-libraries","text":"One of the major benefits of organizing your pipeline code into libraries is the ability to reuse these libraries across different teams. To achieve this level of reusability, it's best to externalize hard coded values as parameters that can be set from the Pipeline Configuration repository.","title":"Parameterizing Libraries"},{"location":"concepts/library-development/parameterizing-libraries/#pass-parameters-through-the-pipeline-configuration","text":"When specifying a library to be loaded, users can pass arbitrary configurations to the library: pipeline_config.groovy libraries { example { // (1) someField = \"my value\" // (2) nested { // (3) someOtherField = 11 // (4) } } } The name of the library to be loaded A root level library configuration option A block name to pass nested configuration A nested library configuration","title":"Pass Parameters Through the Pipeline Configuration"},{"location":"concepts/library-development/parameterizing-libraries/#parameter-structure-and-type","text":"Library parameters can take an arbitrary structure. All parameters can be at the root level or a nested structure can be created to group related configurations together. Library parameter values can be any serializable Groovy primitive. Typically, parameters are boolean, numeric, String, or array.","title":"Parameter Structure and Type"},{"location":"concepts/library-development/parameterizing-libraries/#accessing-library-configurations-within-steps","text":"The Jenkins Templating Engine injects a config variable into each step. This config variable is a map whose keys are the library parameters that have been provided through the Pipeline Configuration. The config variable is only resolvable within a Library Step and only contains the configuration for the step's library. Note If you need to access the entire aggregated Pipeline Configuration, JTE injects a pipelineConfig variable that can be accessed anywhere.","title":"Accessing Library Configurations Within Steps"},{"location":"concepts/library-development/parameterizing-libraries/#validating-library-configurations","text":"The Pipeline Configuration doesn't inherently perform type checking or validation. Library developers can choose to provide a Library Configuration File at the root of the library's directory which will assist with library parameter validation.","title":"Validating Library Configurations"},{"location":"concepts/library-development/step-aliasing/","text":"Step Aliasing \u00b6 Step Aliasing allows library developers to cast the same step to one or more step names at runtime by using the @StepAlias annotation. By default, steps will assume the basename of the files that define them. i.e, a build.groovy step file will create a build step. Step Aliasing allows you to change the name (or names) of the step that's going to be created. This annotation is automatically imported, just like lifecycle hooks . Overview \u00b6 The use case often arises where a library has multiple steps that are all essentially the same thing. Step Aliases allow you to write a step one time and invoke it using multiple names. Steps have access to a stepContext variable to determine the current context of the step, such as the name being used and whether the step is an alias. Static Step Aliases \u00b6 Static step aliases are static lists of strings to cast the step to at runtime. Single Static Alias \u00b6 @StepAlias can take a String parameter to change the name of the step at runtime. generic.groovy @StepAlias ( \"build\" ) // (1) void call (){ println \"running as build!\" } generic.groovy will be invocable at runtime via build() Multiple Static Aliases \u00b6 @StepAlias can also accept an array of Strings to alias the step to multiple names. generic.groovy @StepAlias ([ \"build\" , \"unit_test\" ]) // (1) void call (){ println \"running as either build or unit_test!\" } generic.groovy can be used in the pipeline as either build() or unit_test() Dynamic Step Aliases \u00b6 Sometimes, aliases should themselves be determined at runtime. This can be accomplished by providing a dynamic parameter that should be a Closure that returns a string or list of strings. For example, if a library called alias had a step called generic.groovy then an aliases library parameter could be created: pipeline_config.groovy libraries { alias { aliases = [ \"build\" , \"unit_test\" ] // (1) } } defines a string or list of strings to alias the generic step to This aliases parameter can then be consumed within the dynamic step alias closure: generic.groovy @StepAlias ( dynamic = { return config . aliases }) // (1) void call (){ println \"running as ${stepContext.name}!\" } generic.groovy can be used in the pipeline as either build() or unit_test() Keeping the Original Step \u00b6 By default, when @StepAlias is present in a step file, a step with the original name won't be created. This behavior can be overridden via the keepOriginal annotation parameter. generic.groovy @StepAlias ( value = \"build\" , keepOriginal = true ) // (1) void call (){ println \"running as either build() or generic()\" } The keepOriginal parameter can be used if a step with the original step name should be created Note When passing multiple annotation parameters, the default static aliases parameter should be passed as value .","title":"Step Aliasing"},{"location":"concepts/library-development/step-aliasing/#step-aliasing","text":"Step Aliasing allows library developers to cast the same step to one or more step names at runtime by using the @StepAlias annotation. By default, steps will assume the basename of the files that define them. i.e, a build.groovy step file will create a build step. Step Aliasing allows you to change the name (or names) of the step that's going to be created. This annotation is automatically imported, just like lifecycle hooks .","title":"Step Aliasing"},{"location":"concepts/library-development/step-aliasing/#overview","text":"The use case often arises where a library has multiple steps that are all essentially the same thing. Step Aliases allow you to write a step one time and invoke it using multiple names. Steps have access to a stepContext variable to determine the current context of the step, such as the name being used and whether the step is an alias.","title":"Overview"},{"location":"concepts/library-development/step-aliasing/#static-step-aliases","text":"Static step aliases are static lists of strings to cast the step to at runtime.","title":"Static Step Aliases"},{"location":"concepts/library-development/step-aliasing/#single-static-alias","text":"@StepAlias can take a String parameter to change the name of the step at runtime. generic.groovy @StepAlias ( \"build\" ) // (1) void call (){ println \"running as build!\" } generic.groovy will be invocable at runtime via build()","title":"Single Static Alias"},{"location":"concepts/library-development/step-aliasing/#multiple-static-aliases","text":"@StepAlias can also accept an array of Strings to alias the step to multiple names. generic.groovy @StepAlias ([ \"build\" , \"unit_test\" ]) // (1) void call (){ println \"running as either build or unit_test!\" } generic.groovy can be used in the pipeline as either build() or unit_test()","title":"Multiple Static Aliases"},{"location":"concepts/library-development/step-aliasing/#dynamic-step-aliases","text":"Sometimes, aliases should themselves be determined at runtime. This can be accomplished by providing a dynamic parameter that should be a Closure that returns a string or list of strings. For example, if a library called alias had a step called generic.groovy then an aliases library parameter could be created: pipeline_config.groovy libraries { alias { aliases = [ \"build\" , \"unit_test\" ] // (1) } } defines a string or list of strings to alias the generic step to This aliases parameter can then be consumed within the dynamic step alias closure: generic.groovy @StepAlias ( dynamic = { return config . aliases }) // (1) void call (){ println \"running as ${stepContext.name}!\" } generic.groovy can be used in the pipeline as either build() or unit_test()","title":"Dynamic Step Aliases"},{"location":"concepts/library-development/step-aliasing/#keeping-the-original-step","text":"By default, when @StepAlias is present in a step file, a step with the original name won't be created. This behavior can be overridden via the keepOriginal annotation parameter. generic.groovy @StepAlias ( value = \"build\" , keepOriginal = true ) // (1) void call (){ println \"running as either build() or generic()\" } The keepOriginal parameter can be used if a step with the original step name should be created Note When passing multiple annotation parameters, the default static aliases parameter should be passed as value .","title":"Keeping the Original Step"},{"location":"concepts/pipeline-configuration/","text":"Overview \u00b6 Pipeline Templates are generic, tool-agnostic workflows that utilize Pipeline Primitives to become concrete for specific teams. The Pipeline Configuration is what determines for a given Pipeline Run which Pipeline Template and which Pipeline Primitives should be used. Structure \u00b6 JTE's Pipeline Configuration uses a custom Domain-Specific Language (DSL) which after being parsed by JTE builds a Map . This DSL is a dynamic builder language. It doesn't validate that block names and fields align to the Pipeline Configuration Schema in any way. If a block or field is declared that's not in the schema, it will simply be ignored during Pipeline Initialization such that no Pipeline Primitives are created. The incorrect fields will still be accessible on the pipelineConfig autowired variable Script Security \u00b6 The Pipeline Configuration file is parsed by executing it within the same Groovy Sandbox that Jenkins pipelines use as well. Pipeline Configuration Location \u00b6 Configuration Hierarchy \u00b6 Pipeline Configurations can be stored in the Configuration Hierarchy on Governance Tiers . Merging Pipeline Configurations When more than one Pipeline Configuration is present for a given Pipeline Run, they're merged according to the rules outlined on Merging Configurations . Job-Level Pipeline Configurations \u00b6 Pipeline Configurations can be stored in a couple different locations depending on the Job Type. Job Type Pipeline Configuration Location Pipeline Job Either in the Jenkins UI or at the root of a remote source code repository as a file called pipeline_config.groovy Multi-Branch Project At the root of the repository in a file named pipeline_config.groovy (or to any arbitrary path in the repository as configured by configurationPath ) in the branch job that was created as part of the Multi-Branch Project","title":"Overview"},{"location":"concepts/pipeline-configuration/#overview","text":"Pipeline Templates are generic, tool-agnostic workflows that utilize Pipeline Primitives to become concrete for specific teams. The Pipeline Configuration is what determines for a given Pipeline Run which Pipeline Template and which Pipeline Primitives should be used.","title":"Overview"},{"location":"concepts/pipeline-configuration/#structure","text":"JTE's Pipeline Configuration uses a custom Domain-Specific Language (DSL) which after being parsed by JTE builds a Map . This DSL is a dynamic builder language. It doesn't validate that block names and fields align to the Pipeline Configuration Schema in any way. If a block or field is declared that's not in the schema, it will simply be ignored during Pipeline Initialization such that no Pipeline Primitives are created. The incorrect fields will still be accessible on the pipelineConfig autowired variable","title":"Structure"},{"location":"concepts/pipeline-configuration/#script-security","text":"The Pipeline Configuration file is parsed by executing it within the same Groovy Sandbox that Jenkins pipelines use as well.","title":"Script Security"},{"location":"concepts/pipeline-configuration/#pipeline-configuration-location","text":"","title":"Pipeline Configuration Location"},{"location":"concepts/pipeline-configuration/#configuration-hierarchy","text":"Pipeline Configurations can be stored in the Configuration Hierarchy on Governance Tiers . Merging Pipeline Configurations When more than one Pipeline Configuration is present for a given Pipeline Run, they're merged according to the rules outlined on Merging Configurations .","title":"Configuration Hierarchy"},{"location":"concepts/pipeline-configuration/#job-level-pipeline-configurations","text":"Pipeline Configurations can be stored in a couple different locations depending on the Job Type. Job Type Pipeline Configuration Location Pipeline Job Either in the Jenkins UI or at the root of a remote source code repository as a file called pipeline_config.groovy Multi-Branch Project At the root of the repository in a file named pipeline_config.groovy (or to any arbitrary path in the repository as configured by configurationPath ) in the branch job that was created as part of the Multi-Branch Project","title":"Job-Level Pipeline Configurations"},{"location":"concepts/pipeline-configuration/configuration-dsl/","text":"Pipeline Configuration Syntax \u00b6 This page will cover the mechanics of JTE's Pipeline Configuration DSL. Motivation \u00b6 Originally, the JTE Pipeline Configuration was written in more standard structures like JSON or YAML. Structure Challenge JSON Too verbose to be comfortable writing. YAML Users would frequently make errors with YAML syntax that resulted in a different configuration than expected. In the end, a Groovy DSL provided the best of both words in terms of verbosity and forgiveness. Over time, the features made available through a custom DSL became useful. Data Structure Storage \u00b6 While not required to understand the DSL, it can accelerate your learning if you're familiar with LinkedHashMaps The Pipeline Configuration syntax is a nested builder language that relies on Blocks and Properties to build a LinkedHashMap representing the configuration. Property Setting \u00b6 Properties of the Pipeline Configuration are set using Groovy's Variable Assignment syntax. Pipeline Configuration DSL Resulting pipelineConfig pipeline_config.groovy foo = \"bar\" assert pipelineConfig == [ foo: \"bar\" ] Don't Declare Variables The DSL relies on setProperty(String propertyName, Object value) being executed to persist the Pipeline Configuration property values. Take the following example: Pipeline Configuration DSL Resulting pipelineConfig pipeline_config.groovy x = \"x\" String y = \"y\" assert pipelineConfig == [ x: \"x\" ] The y value is not persisted. Blocks \u00b6 The Pipeline Configuration DSL supports nested properties using Blocks. Pipeline Configuration Resulting pipelineConfig pipeline_config.groovy a { x = 1 , y = 2 } assert pipelineConfig == [ a: [ x: 1 y: 2 ] ] Empty Blocks and Unset Properties \u00b6 A special case is empty blocks and unset properties. Both situations result in an empty map being set in the Pipeline Configuration. Pipeline Config Resulting pipelineConfig pipeline_config.groovy a { x = 1 y {} z } assert pipelineConfig == [ a: [ x: 1 , y: [:], z: [:] ] ] Pipeline Governance Annotations \u00b6 To support Pipeline Governance , the Pipeline Configuration DSL uses special annotations to control which aspects of the configuration the next configuration in the Configuration Hierarchy is able to modify. These annotations are called @override and @merge and both can be placed on a block and property. Pipeline Configuration pipeline_config.groovy @merge a { x = 1 @override y = 2 } Learn More To learn more about how these annotations work, check out Merging Pipeline Configurations","title":"Pipeline Configuration Syntax"},{"location":"concepts/pipeline-configuration/configuration-dsl/#pipeline-configuration-syntax","text":"This page will cover the mechanics of JTE's Pipeline Configuration DSL.","title":"Pipeline Configuration Syntax"},{"location":"concepts/pipeline-configuration/configuration-dsl/#motivation","text":"Originally, the JTE Pipeline Configuration was written in more standard structures like JSON or YAML. Structure Challenge JSON Too verbose to be comfortable writing. YAML Users would frequently make errors with YAML syntax that resulted in a different configuration than expected. In the end, a Groovy DSL provided the best of both words in terms of verbosity and forgiveness. Over time, the features made available through a custom DSL became useful.","title":"Motivation"},{"location":"concepts/pipeline-configuration/configuration-dsl/#data-structure-storage","text":"While not required to understand the DSL, it can accelerate your learning if you're familiar with LinkedHashMaps The Pipeline Configuration syntax is a nested builder language that relies on Blocks and Properties to build a LinkedHashMap representing the configuration.","title":"Data Structure Storage"},{"location":"concepts/pipeline-configuration/configuration-dsl/#property-setting","text":"Properties of the Pipeline Configuration are set using Groovy's Variable Assignment syntax. Pipeline Configuration DSL Resulting pipelineConfig pipeline_config.groovy foo = \"bar\" assert pipelineConfig == [ foo: \"bar\" ] Don't Declare Variables The DSL relies on setProperty(String propertyName, Object value) being executed to persist the Pipeline Configuration property values. Take the following example: Pipeline Configuration DSL Resulting pipelineConfig pipeline_config.groovy x = \"x\" String y = \"y\" assert pipelineConfig == [ x: \"x\" ] The y value is not persisted.","title":"Property Setting"},{"location":"concepts/pipeline-configuration/configuration-dsl/#blocks","text":"The Pipeline Configuration DSL supports nested properties using Blocks. Pipeline Configuration Resulting pipelineConfig pipeline_config.groovy a { x = 1 , y = 2 } assert pipelineConfig == [ a: [ x: 1 y: 2 ] ]","title":"Blocks"},{"location":"concepts/pipeline-configuration/configuration-dsl/#empty-blocks-and-unset-properties","text":"A special case is empty blocks and unset properties. Both situations result in an empty map being set in the Pipeline Configuration. Pipeline Config Resulting pipelineConfig pipeline_config.groovy a { x = 1 y {} z } assert pipelineConfig == [ a: [ x: 1 , y: [:], z: [:] ] ]","title":"Empty Blocks and Unset Properties"},{"location":"concepts/pipeline-configuration/configuration-dsl/#pipeline-governance-annotations","text":"To support Pipeline Governance , the Pipeline Configuration DSL uses special annotations to control which aspects of the configuration the next configuration in the Configuration Hierarchy is able to modify. These annotations are called @override and @merge and both can be placed on a block and property. Pipeline Configuration pipeline_config.groovy @merge a { x = 1 @override y = 2 } Learn More To learn more about how these annotations work, check out Merging Pipeline Configurations","title":"Pipeline Governance Annotations"},{"location":"concepts/pipeline-configuration/merging-configs/","text":"Merging Configuration Files \u00b6 During Pipeline Initialization , JTE collects every Pipeline Configuration in the Configuration Hierarchy for the Pipeline Run. This Pipeline Configuration chain is then sequentially merged, starting with the top-most configuration and ending with the most granular. The following guidelines explain how two Pipeline Configurations are merged together. The First Pipeline Configuration \u00b6 The first Pipeline Configuration in the configuration chain can define any blocks and properties. Merging When A Parent Pipeline Configuration Is Present \u00b6 After the first Pipeline Configuration has been set, each subsequent Pipeline Configuration can define root-level blocks and properties but can't modify properties or blocks that were previously set unless explicitly permitted by the previous configuration. Permitting Modifications \u00b6 Pipeline Configurations must explicitly define which blocks and properties can be modified by the next configuration in the configuration chain. This is done through the @override and @merge annotations. @override \u00b6 The @override annotation is used to permit block-level changes or to permit specific properties to be changed. Setting @override on a block will allow the next configuration to change any property in the block. Setting @override on a property will allow the next configuration to change that property. @merge \u00b6 The @merge annotation is used at the block-level to allow the next configuration in the configuration chain to append properties to the block but not change inherited properties.","title":"Merging Configuration Files"},{"location":"concepts/pipeline-configuration/merging-configs/#merging-configuration-files","text":"During Pipeline Initialization , JTE collects every Pipeline Configuration in the Configuration Hierarchy for the Pipeline Run. This Pipeline Configuration chain is then sequentially merged, starting with the top-most configuration and ending with the most granular. The following guidelines explain how two Pipeline Configurations are merged together.","title":"Merging Configuration Files"},{"location":"concepts/pipeline-configuration/merging-configs/#the-first-pipeline-configuration","text":"The first Pipeline Configuration in the configuration chain can define any blocks and properties.","title":"The First Pipeline Configuration"},{"location":"concepts/pipeline-configuration/merging-configs/#merging-when-a-parent-pipeline-configuration-is-present","text":"After the first Pipeline Configuration has been set, each subsequent Pipeline Configuration can define root-level blocks and properties but can't modify properties or blocks that were previously set unless explicitly permitted by the previous configuration.","title":"Merging When A Parent Pipeline Configuration Is Present"},{"location":"concepts/pipeline-configuration/merging-configs/#permitting-modifications","text":"Pipeline Configurations must explicitly define which blocks and properties can be modified by the next configuration in the configuration chain. This is done through the @override and @merge annotations.","title":"Permitting Modifications"},{"location":"concepts/pipeline-configuration/merging-configs/#override","text":"The @override annotation is used to permit block-level changes or to permit specific properties to be changed. Setting @override on a block will allow the next configuration to change any property in the block. Setting @override on a property will allow the next configuration to change that property.","title":"@override"},{"location":"concepts/pipeline-configuration/merging-configs/#merge","text":"The @merge annotation is used at the block-level to allow the next configuration in the configuration chain to append properties to the block but not change inherited properties.","title":"@merge"},{"location":"concepts/pipeline-governance/","text":"Overview \u00b6 One of the Key Benefits of using JTE is the governance it can bring to software delivery. JTE achieves this governance by creating a Configuration Hierarchy using Jenkins global settings and Folder properties. The nodes of this hierarchy, called Governance Tiers , store Pipeline Configurations , a Pipeline Catalog , and Library Sources . Teams can create arbitrarily complex governance hierarchies simply by organizing jobs in Jenkins into the appropriate Folders. Learn More \u00b6 Page Description Configuration Hierarchy Learn how to set up hierarchical Pipeline Configurations Governance Tier Learn how to configure a node of the Configuration Hierarchy Pipeline Template Selection Learn how JTE determines which Pipeline Template to use for a given Pipeline Run Library Resolution Learn how JTE choose which library to load when there are multiple choices within the available Library Sources Governance Tier Learn how to configure a Governance Tier","title":"Overview"},{"location":"concepts/pipeline-governance/#overview","text":"One of the Key Benefits of using JTE is the governance it can bring to software delivery. JTE achieves this governance by creating a Configuration Hierarchy using Jenkins global settings and Folder properties. The nodes of this hierarchy, called Governance Tiers , store Pipeline Configurations , a Pipeline Catalog , and Library Sources . Teams can create arbitrarily complex governance hierarchies simply by organizing jobs in Jenkins into the appropriate Folders.","title":"Overview"},{"location":"concepts/pipeline-governance/#learn-more","text":"Page Description Configuration Hierarchy Learn how to set up hierarchical Pipeline Configurations Governance Tier Learn how to configure a node of the Configuration Hierarchy Pipeline Template Selection Learn how JTE determines which Pipeline Template to use for a given Pipeline Run Library Resolution Learn how JTE choose which library to load when there are multiple choices within the available Library Sources Governance Tier Learn how to configure a Governance Tier","title":"Learn More"},{"location":"concepts/pipeline-governance/configuration-hierarchy/","text":"Configuration Hierarchy \u00b6 The Configuration Hierarchy is created by configuring these Governance Tiers on Folders 1 and in the Jenkins Global Configuration 2 . Pipelines using JTE inherit Pipeline Configuration , Pipeline Catalogs , and Library Sources from their parent Governance Tiers as determined by the hierarchy. Figure 1. Creating a Configuration Hierarchy The Folders Plugin allows users to define custom taxonomies. \u21a9 You can find the Global Configuration, if you have permission, by navigating to Manage Jenkins > Configure System \u21a9","title":"Configuration Hierarchy"},{"location":"concepts/pipeline-governance/configuration-hierarchy/#configuration-hierarchy","text":"The Configuration Hierarchy is created by configuring these Governance Tiers on Folders 1 and in the Jenkins Global Configuration 2 . Pipelines using JTE inherit Pipeline Configuration , Pipeline Catalogs , and Library Sources from their parent Governance Tiers as determined by the hierarchy. Figure 1. Creating a Configuration Hierarchy The Folders Plugin allows users to define custom taxonomies. \u21a9 You can find the Global Configuration, if you have permission, by navigating to Manage Jenkins > Configure System \u21a9","title":"Configuration Hierarchy"},{"location":"concepts/pipeline-governance/governance-tier/","text":"Governance Tier \u00b6 Governance Tiers are nodes in the Configuration Hierarchy that store the following: Governance Tier Data Description Pipeline Catalog A set of Pipeline Templates Pipeline Configuration A Pipeline Configuration that will be inherited Library Sources A list of Library Sources Reference: Governance Tier Structure To learn more about how to configure a Governance Tier, check out the Governance Tier Reference Page","title":"Governance Tier"},{"location":"concepts/pipeline-governance/governance-tier/#governance-tier","text":"Governance Tiers are nodes in the Configuration Hierarchy that store the following: Governance Tier Data Description Pipeline Catalog A set of Pipeline Templates Pipeline Configuration A Pipeline Configuration that will be inherited Library Sources A list of Library Sources Reference: Governance Tier Structure To learn more about how to configure a Governance Tier, check out the Governance Tier Reference Page","title":"Governance Tier"},{"location":"concepts/pipeline-governance/library-resolution/","text":"Library Resolution \u00b6 Each Governance Tier in the Configuration Hierarchy can store a list of Library Sources . This page explains the order in which JTE will try to resolve and load a Library if multiple Library Sources have the same library. Default Resolution Order \u00b6 JTE will search for Libraries within Library Sources starting with the Governance Tier most proximal to the Job in the taxonomy. The Library Sources will be queried for the library starting with the first Library Source in the list on the Governance Tier before proceeding to any subsequent Library Sources. If the Library Sources configured on the first Governance Tier don't have the Library being loaded, JTE will then check the parent Governance Tier. If the Library can't be found after searching every Governance Tier, the Pipeline Run will fail. Inverting Resolution Order \u00b6 To invert this resolution order, set jte.reverse_library_resolution to True .","title":"Library Resolution"},{"location":"concepts/pipeline-governance/library-resolution/#library-resolution","text":"Each Governance Tier in the Configuration Hierarchy can store a list of Library Sources . This page explains the order in which JTE will try to resolve and load a Library if multiple Library Sources have the same library.","title":"Library Resolution"},{"location":"concepts/pipeline-governance/library-resolution/#default-resolution-order","text":"JTE will search for Libraries within Library Sources starting with the Governance Tier most proximal to the Job in the taxonomy. The Library Sources will be queried for the library starting with the first Library Source in the list on the Governance Tier before proceeding to any subsequent Library Sources. If the Library Sources configured on the first Governance Tier don't have the Library being loaded, JTE will then check the parent Governance Tier. If the Library can't be found after searching every Governance Tier, the Pipeline Run will fail.","title":"Default Resolution Order"},{"location":"concepts/pipeline-governance/library-resolution/#inverting-resolution-order","text":"To invert this resolution order, set jte.reverse_library_resolution to True .","title":"Inverting Resolution Order"},{"location":"concepts/pipeline-governance/pipeline-template-selection/","text":"Pipeline Template Selection \u00b6 Pipeline Template Selection is the name of the process that determines which Pipeline Template to use for a given Pipeline Run. Figure 1 visualizes this process in a flow chart. Figure 1. Pipeline Template Selection Flow Chart Job Type Matters \u00b6 Ad Hoc Pipeline Jobs \u00b6 JTE treats ad hoc Pipeline Jobs a little differently than Pipeline Jobs that have been created by a Multibranch Project. For Pipeline Jobs, if a Pipeline Template has been configured, it will be used. If not, JTE will follow the flow described throughout the rest of this document. Rationale The rationale for using the configured template without falling back to the rest of the Pipeline Template Selection process is that if a user has permissions to create and configure their own Jenkins job, Pipeline Governance is already gone. Multibranch Project Pipeline Jobs \u00b6 For Multibranch Project Pipeline Jobs, if the source code repository has a Jenkinsfile at the root (or at any arbitrary path in the repository as configured by scriptPath ) and jte.allow_scm_jenkinsfile is set to True , then the repository Jenkinsfile will be used as the Pipeline Template. Disabling Repository Jenkinsfiles It's important that when trying to enforce a certain set of Pipeline Templates are used that jte.allow_scm_jenkinsfile is set to False . Otherwise, developers will be able to write whatever Pipeline Template they want to. Named Pipeline Templates \u00b6 The next possibility is that the aggregated Pipeline Configuration has configured JTE to look for a Named Pipeline Template from the Pipeline Catalog . If this is the case, JTE will recursively search each Governance Tier in the Configuration Hierarchy looking for the Named Pipeline Template. The order of this search will be from most-granular Governance Tier to the Global Governance Tier. If the Named Pipeline Template can't be found, the Pipeline Run will fail. Finding the Default Pipeline Template \u00b6 Finally, if the job doesn't have a configured Pipeline Template and the Pipeline Configuration hasn't defined a Named Pipeline Template to use then JTE will search to find a Default Pipeline Template. In this case, JTE will recursively search each Governance Tier in the Configuration Hierarchy looking for a Default Pipeline Template. The order of this search will be from most-granular Governance Tier to the Global Governance Tier. If a Default Pipeline Template can't be found, the Pipeline Run will fail.","title":"Pipeline Template Selection"},{"location":"concepts/pipeline-governance/pipeline-template-selection/#pipeline-template-selection","text":"Pipeline Template Selection is the name of the process that determines which Pipeline Template to use for a given Pipeline Run. Figure 1 visualizes this process in a flow chart. Figure 1. Pipeline Template Selection Flow Chart","title":"Pipeline Template Selection"},{"location":"concepts/pipeline-governance/pipeline-template-selection/#job-type-matters","text":"","title":"Job Type Matters"},{"location":"concepts/pipeline-governance/pipeline-template-selection/#ad-hoc-pipeline-jobs","text":"JTE treats ad hoc Pipeline Jobs a little differently than Pipeline Jobs that have been created by a Multibranch Project. For Pipeline Jobs, if a Pipeline Template has been configured, it will be used. If not, JTE will follow the flow described throughout the rest of this document. Rationale The rationale for using the configured template without falling back to the rest of the Pipeline Template Selection process is that if a user has permissions to create and configure their own Jenkins job, Pipeline Governance is already gone.","title":"Ad Hoc Pipeline Jobs"},{"location":"concepts/pipeline-governance/pipeline-template-selection/#multibranch-project-pipeline-jobs","text":"For Multibranch Project Pipeline Jobs, if the source code repository has a Jenkinsfile at the root (or at any arbitrary path in the repository as configured by scriptPath ) and jte.allow_scm_jenkinsfile is set to True , then the repository Jenkinsfile will be used as the Pipeline Template. Disabling Repository Jenkinsfiles It's important that when trying to enforce a certain set of Pipeline Templates are used that jte.allow_scm_jenkinsfile is set to False . Otherwise, developers will be able to write whatever Pipeline Template they want to.","title":"Multibranch Project Pipeline Jobs"},{"location":"concepts/pipeline-governance/pipeline-template-selection/#named-pipeline-templates","text":"The next possibility is that the aggregated Pipeline Configuration has configured JTE to look for a Named Pipeline Template from the Pipeline Catalog . If this is the case, JTE will recursively search each Governance Tier in the Configuration Hierarchy looking for the Named Pipeline Template. The order of this search will be from most-granular Governance Tier to the Global Governance Tier. If the Named Pipeline Template can't be found, the Pipeline Run will fail.","title":"Named Pipeline Templates"},{"location":"concepts/pipeline-governance/pipeline-template-selection/#finding-the-default-pipeline-template","text":"Finally, if the job doesn't have a configured Pipeline Template and the Pipeline Configuration hasn't defined a Named Pipeline Template to use then JTE will search to find a Default Pipeline Template. In this case, JTE will recursively search each Governance Tier in the Configuration Hierarchy looking for a Default Pipeline Template. The order of this search will be from most-granular Governance Tier to the Global Governance Tier. If a Default Pipeline Template can't be found, the Pipeline Run will fail.","title":"Finding the Default Pipeline Template"},{"location":"concepts/pipeline-primitives/","text":"Overview \u00b6 Pipeline Primitives are objects that can be defined from the Pipeline Configuration and accessed from a Pipeline Template . Pipeline Primitives exist to make Pipeline Templates easier to write, easier to read, and easier to share across teams. Pipeline Primitive Types \u00b6 Primitive Type Description Steps Define a step of the pipeline, typically to be invoked from the Pipeline Template. Stages Group steps together to keep templates DRY. Application Environments Encapsulate environmental context Keywords Declare variables from the Pipeline Configuration for use in Pipeline Templates and steps","title":"Overview"},{"location":"concepts/pipeline-primitives/#overview","text":"Pipeline Primitives are objects that can be defined from the Pipeline Configuration and accessed from a Pipeline Template . Pipeline Primitives exist to make Pipeline Templates easier to write, easier to read, and easier to share across teams.","title":"Overview"},{"location":"concepts/pipeline-primitives/#pipeline-primitive-types","text":"Primitive Type Description Steps Define a step of the pipeline, typically to be invoked from the Pipeline Template. Stages Group steps together to keep templates DRY. Application Environments Encapsulate environmental context Keywords Declare variables from the Pipeline Configuration for use in Pipeline Templates and steps","title":"Pipeline Primitive Types"},{"location":"concepts/pipeline-primitives/application-environments/","text":"Application Environments \u00b6 The Application Environment primitive allows users to encapsulate environmental context. Users can define custom fields from the Pipeline Configuration . Defining Application Environments \u00b6 The application_environments{} block is used to define Application Environments. Within the Application Environments block, environments are defined through nested keys. For example, the following code block would create dev and test variables, each referencing an Application Environment object. These variables can be resolved within the Pipeline Template or Library Steps. pipeline_config.groovy application_environments { dev test } Default Fields \u00b6 Application Environments can define the optional fields short_name and long_name . If not declared, these fields will default to the Application Environment key. For example: pipeline_config.groovy application_environments { dev test { short_name = \"t\" } staging { long_name = \"Staging\" } prod { short_name = \"p\" long_name = \"Production\" } } This block defines dev , test , and prod Application Environments. The following table outlines the values of short_name and long_name for each Application Environment. Application Environment Short Name Long Name dev \"dev\" \"dev\" test \"t\" \"test\" staging \"staging\" \"Staging\" prod \"p\" \"Production\" Determining Application Environment Order \u00b6 The order Application Environments are defined within the Pipeline Configuration are used to define previous and next properties. For example, defining the following Application Environments pipeline_config.groovy application_environments { dev test prod } would result in the following values for previous and next on each Application Environment: Application Environment previous next dev null test test dev prod prod test null Note These properties are automatically configured based upon the declaration order within the Pipeline Configuration. If you try to set the previous and next properties in the environment's definition an exception will be thrown. Custom Fields \u00b6 Application Environments accept custom fields. These custom fields can be used to capture characteristics about the Application Environment that should be used from the pipeline. Examples include AWS tags to use when querying infrastructure, kubernetes cluster API endpoints, IP addresses, etc. For example, if there were IP addresses that the pipeline needed to access during execution: pipeline_config.groovy application_environments { dev { ip_addresses = [ \"1.2.3.4\" , \"1.2.3.5\" ] } test prod { ip_addresses = [ \"1.2.3.6\" , \"1.2.3.7\" ] } } This would add an ip_addresses property to the dev and prod objects while test.ip_addresses would be null . Using Application Environments in Deployment Steps \u00b6 A common pattern is to use Application Environments in conjunction with steps that perform automated deployments. If a library were to contribute a deploy_to step that accepted an Application Environment as an input parameter, then a Pipeline Template could be created that leverages these variables. Jenkinsfile do_some_tests () deploy_to dev deploy_to prod A contrived example of a Library Step that follows this pattern is below. deploy_to.groovy void call ( app_env ){ // use the default long_name property to dynamically name the stage stage ( \"Deploy to ${app_env.long_name}\" ){ // iterate over the environment's ip addresses and print a statement app_env . ip_addresses . each { ip -> println \"publishing artifact to ${ip}\" } } } Note You may have noticed that the template didn't use parenthesis when invoking the deploy_to method: deploy_to dev . This has nothing to do with JTE. Groovy allows you to omit parentheses when passing parameters to a method.","title":"Application Environments"},{"location":"concepts/pipeline-primitives/application-environments/#application-environments","text":"The Application Environment primitive allows users to encapsulate environmental context. Users can define custom fields from the Pipeline Configuration .","title":"Application Environments"},{"location":"concepts/pipeline-primitives/application-environments/#defining-application-environments","text":"The application_environments{} block is used to define Application Environments. Within the Application Environments block, environments are defined through nested keys. For example, the following code block would create dev and test variables, each referencing an Application Environment object. These variables can be resolved within the Pipeline Template or Library Steps. pipeline_config.groovy application_environments { dev test }","title":"Defining Application Environments"},{"location":"concepts/pipeline-primitives/application-environments/#default-fields","text":"Application Environments can define the optional fields short_name and long_name . If not declared, these fields will default to the Application Environment key. For example: pipeline_config.groovy application_environments { dev test { short_name = \"t\" } staging { long_name = \"Staging\" } prod { short_name = \"p\" long_name = \"Production\" } } This block defines dev , test , and prod Application Environments. The following table outlines the values of short_name and long_name for each Application Environment. Application Environment Short Name Long Name dev \"dev\" \"dev\" test \"t\" \"test\" staging \"staging\" \"Staging\" prod \"p\" \"Production\"","title":"Default Fields"},{"location":"concepts/pipeline-primitives/application-environments/#determining-application-environment-order","text":"The order Application Environments are defined within the Pipeline Configuration are used to define previous and next properties. For example, defining the following Application Environments pipeline_config.groovy application_environments { dev test prod } would result in the following values for previous and next on each Application Environment: Application Environment previous next dev null test test dev prod prod test null Note These properties are automatically configured based upon the declaration order within the Pipeline Configuration. If you try to set the previous and next properties in the environment's definition an exception will be thrown.","title":"Determining Application Environment Order"},{"location":"concepts/pipeline-primitives/application-environments/#custom-fields","text":"Application Environments accept custom fields. These custom fields can be used to capture characteristics about the Application Environment that should be used from the pipeline. Examples include AWS tags to use when querying infrastructure, kubernetes cluster API endpoints, IP addresses, etc. For example, if there were IP addresses that the pipeline needed to access during execution: pipeline_config.groovy application_environments { dev { ip_addresses = [ \"1.2.3.4\" , \"1.2.3.5\" ] } test prod { ip_addresses = [ \"1.2.3.6\" , \"1.2.3.7\" ] } } This would add an ip_addresses property to the dev and prod objects while test.ip_addresses would be null .","title":"Custom Fields"},{"location":"concepts/pipeline-primitives/application-environments/#using-application-environments-in-deployment-steps","text":"A common pattern is to use Application Environments in conjunction with steps that perform automated deployments. If a library were to contribute a deploy_to step that accepted an Application Environment as an input parameter, then a Pipeline Template could be created that leverages these variables. Jenkinsfile do_some_tests () deploy_to dev deploy_to prod A contrived example of a Library Step that follows this pattern is below. deploy_to.groovy void call ( app_env ){ // use the default long_name property to dynamically name the stage stage ( \"Deploy to ${app_env.long_name}\" ){ // iterate over the environment's ip addresses and print a statement app_env . ip_addresses . each { ip -> println \"publishing artifact to ${ip}\" } } } Note You may have noticed that the template didn't use parenthesis when invoking the deploy_to method: deploy_to dev . This has nothing to do with JTE. Groovy allows you to omit parentheses when passing parameters to a method.","title":"Using Application Environments in Deployment Steps"},{"location":"concepts/pipeline-primitives/keywords/","text":"Keywords \u00b6 Keywords let users declare variables from the Pipeline Configuration that can be resolved from the Pipeline Template or Library Steps . Defining Keywords \u00b6 Keywords are defined via the keywords{} block in the Pipeline Configuration. For example, pipeline_config.groovy keywords { foo = \"bar\" } would then result in a foo variable with the value \"bar\" . Use Cases \u00b6 Global Variables \u00b6 Keywords can be used to define a globals variable accessible from the Pipeline Template and Library Steps. pipeline_config.groovy keywords { globals { one = 1 two = 2 } } Regular Expressions for Conditionals \u00b6 Keywords can be used to define regular expressions corresponding to common branch names for use from the Pipeline Template to keep the template easy to read. Pipeline Configuration Pipeline Template pipeline_config.groovy keywords { main = ~ /^[mM]a(in|ster)$/ develop = ~ /^[Dd]evelop(ment|er|)$/ } Jenkinsfile on_pull_request to: develop , { /* execute on a PR to branches matching the regular expression defined by the \"develop\" keyword */ } on_pull_request to: main , from: develop , { /* execute on a PR from a branch matching the regular expression defined by the \"develop\" keyword to a branch matching the regular expression defined by the \"main\" keyword */ } on_merge to: main , { /* execute when a PR is merged into a branch that matches the regular expression defined by the \"main\" keyword */ } Note The steps in this example ( on_pull_request and on_merge ) aren't a part of the Jenkins Templating Engine.","title":"Keywords"},{"location":"concepts/pipeline-primitives/keywords/#keywords","text":"Keywords let users declare variables from the Pipeline Configuration that can be resolved from the Pipeline Template or Library Steps .","title":"Keywords"},{"location":"concepts/pipeline-primitives/keywords/#defining-keywords","text":"Keywords are defined via the keywords{} block in the Pipeline Configuration. For example, pipeline_config.groovy keywords { foo = \"bar\" } would then result in a foo variable with the value \"bar\" .","title":"Defining Keywords"},{"location":"concepts/pipeline-primitives/keywords/#use-cases","text":"","title":"Use Cases"},{"location":"concepts/pipeline-primitives/keywords/#global-variables","text":"Keywords can be used to define a globals variable accessible from the Pipeline Template and Library Steps. pipeline_config.groovy keywords { globals { one = 1 two = 2 } }","title":"Global Variables"},{"location":"concepts/pipeline-primitives/keywords/#regular-expressions-for-conditionals","text":"Keywords can be used to define regular expressions corresponding to common branch names for use from the Pipeline Template to keep the template easy to read. Pipeline Configuration Pipeline Template pipeline_config.groovy keywords { main = ~ /^[mM]a(in|ster)$/ develop = ~ /^[Dd]evelop(ment|er|)$/ } Jenkinsfile on_pull_request to: develop , { /* execute on a PR to branches matching the regular expression defined by the \"develop\" keyword */ } on_pull_request to: main , from: develop , { /* execute on a PR from a branch matching the regular expression defined by the \"develop\" keyword to a branch matching the regular expression defined by the \"main\" keyword */ } on_merge to: main , { /* execute when a PR is merged into a branch that matches the regular expression defined by the \"main\" keyword */ } Note The steps in this example ( on_pull_request and on_merge ) aren't a part of the Jenkins Templating Engine.","title":"Regular Expressions for Conditionals"},{"location":"concepts/pipeline-primitives/primitive-namespace/","text":"Primitive Namespace \u00b6 The Pipeline Primitive Namespace is an Autowired Variable called jte that's accessible everywhere. It can be used to access all the loaded Pipeline Primitives for a given Pipeline Run. Accessing Libraries and Steps \u00b6 If libraries were loaded, the jte variable will have a libraries property that stores the library's steps. Invoking a Step Using The Primitive Namespace Pipeline Configuration Pipeline Template pipeline_config.groovy libraries { npm // contributes a build() step } Jenkinsfile jte . libraries . npm . build () Accessing Keywords \u00b6 If Keywords were defined, the jte variable will have a keywords property that stores the Keywords. Accessing Keywords Pipeline Configuration Pipeline Template pipeline_config.groovy keywords { foo = \"bar\" } Jenkinsfile assert jte . keywords . foo == \"bar\" Accessing Application Environments \u00b6 If Application Environments were defined, the jte variable will have an application_environments property that stores the Application Environments. Accessing Application Environments Pipeline Configuration Pipeline Template pipeline_config.groovy application_environments { dev { ip = \"1.1.1.1\" } prod { ip = \"2.2.2.2\" } } Jenkinsfile assert jte . application_environments . dev . ip == \"1.1.1.1\" assert jte . application_environments . prod . ip == \"2.2.2.2\" Accessing Stages \u00b6 If Stages were defined, the jte variable will have an stages property that stores the Stages. Accessing Application Environments Pipeline Configuration Pipeline Template pipeline_config.groovy libraries { npm // contributes unit_test, build } stages { continuous_integration { unit_test build } } Jenkinsfile jte . stages . continuous_integration ()","title":"Primitive Namespace"},{"location":"concepts/pipeline-primitives/primitive-namespace/#primitive-namespace","text":"The Pipeline Primitive Namespace is an Autowired Variable called jte that's accessible everywhere. It can be used to access all the loaded Pipeline Primitives for a given Pipeline Run.","title":"Primitive Namespace"},{"location":"concepts/pipeline-primitives/primitive-namespace/#accessing-libraries-and-steps","text":"If libraries were loaded, the jte variable will have a libraries property that stores the library's steps. Invoking a Step Using The Primitive Namespace Pipeline Configuration Pipeline Template pipeline_config.groovy libraries { npm // contributes a build() step } Jenkinsfile jte . libraries . npm . build ()","title":"Accessing Libraries and Steps"},{"location":"concepts/pipeline-primitives/primitive-namespace/#accessing-keywords","text":"If Keywords were defined, the jte variable will have a keywords property that stores the Keywords. Accessing Keywords Pipeline Configuration Pipeline Template pipeline_config.groovy keywords { foo = \"bar\" } Jenkinsfile assert jte . keywords . foo == \"bar\"","title":"Accessing Keywords"},{"location":"concepts/pipeline-primitives/primitive-namespace/#accessing-application-environments","text":"If Application Environments were defined, the jte variable will have an application_environments property that stores the Application Environments. Accessing Application Environments Pipeline Configuration Pipeline Template pipeline_config.groovy application_environments { dev { ip = \"1.1.1.1\" } prod { ip = \"2.2.2.2\" } } Jenkinsfile assert jte . application_environments . dev . ip == \"1.1.1.1\" assert jte . application_environments . prod . ip == \"2.2.2.2\"","title":"Accessing Application Environments"},{"location":"concepts/pipeline-primitives/primitive-namespace/#accessing-stages","text":"If Stages were defined, the jte variable will have an stages property that stores the Stages. Accessing Application Environments Pipeline Configuration Pipeline Template pipeline_config.groovy libraries { npm // contributes unit_test, build } stages { continuous_integration { unit_test build } } Jenkinsfile jte . stages . continuous_integration ()","title":"Accessing Stages"},{"location":"concepts/pipeline-primitives/stages/","text":"Stages \u00b6 Stages help keep Pipeline Templates DRY by grouping steps together for execution. Defining Stages \u00b6 Stages are defined through the stages{} block. Each subkey references a step to be executed. Stage Context \u00b6 The stageContext variable allows a step to determine if it's being executed as part of a stage. Property Description stageContext.name The name of the stage being executed. Is set to null when the step execution is outside of a stage. stageContext.args A map of named parameters passed to the stage. Is equal to an empty map when not within a stage execution or if no parameters were provided. stageContext Example \u00b6 Assume a library called demo is available within a configured Library Source . Pipeline Configuration Pipeline Template Library Step pipeline_config.groovy stages { continuous_integration { unit_test } } libraries { demo } Jenkinsfile continuous_integration param1: \"foo\" , param2: \"bar\" unit_test () demo/steps/unit_test.groovy void call (){ println \"stage name = ${stepContext.name}\" println \"param1 = ${stageContext.args.param1}\" println \"param2 = ${stageContext.args.param2}\" } The console log from this pipeline would look similar to: ... stage name = continuous_integration param1 = foo param2 = bar ... stage name = null param1 = null param2 = null Use Cases \u00b6 Continuous Integration \u00b6 A common example would be to create a continuous integration stage to keep templates DRY. Pipeline Configuration Pipeline Template pipeline_config.groovy ... stages { continuous_integration { unit_test static_code_analysis build scan_artifact } } Jenkinsfile on_pull_request to: develop , { continuous_integration () } on_merge to: develop , { continuous_integration () deploy_to dev } on_merge to: main , { deploy_to prod }","title":"Stages"},{"location":"concepts/pipeline-primitives/stages/#stages","text":"Stages help keep Pipeline Templates DRY by grouping steps together for execution.","title":"Stages"},{"location":"concepts/pipeline-primitives/stages/#defining-stages","text":"Stages are defined through the stages{} block. Each subkey references a step to be executed.","title":"Defining Stages"},{"location":"concepts/pipeline-primitives/stages/#stage-context","text":"The stageContext variable allows a step to determine if it's being executed as part of a stage. Property Description stageContext.name The name of the stage being executed. Is set to null when the step execution is outside of a stage. stageContext.args A map of named parameters passed to the stage. Is equal to an empty map when not within a stage execution or if no parameters were provided.","title":"Stage Context"},{"location":"concepts/pipeline-primitives/stages/#stagecontext-example","text":"Assume a library called demo is available within a configured Library Source . Pipeline Configuration Pipeline Template Library Step pipeline_config.groovy stages { continuous_integration { unit_test } } libraries { demo } Jenkinsfile continuous_integration param1: \"foo\" , param2: \"bar\" unit_test () demo/steps/unit_test.groovy void call (){ println \"stage name = ${stepContext.name}\" println \"param1 = ${stageContext.args.param1}\" println \"param2 = ${stageContext.args.param2}\" } The console log from this pipeline would look similar to: ... stage name = continuous_integration param1 = foo param2 = bar ... stage name = null param1 = null param2 = null","title":"stageContext Example"},{"location":"concepts/pipeline-primitives/stages/#use-cases","text":"","title":"Use Cases"},{"location":"concepts/pipeline-primitives/stages/#continuous-integration","text":"A common example would be to create a continuous integration stage to keep templates DRY. Pipeline Configuration Pipeline Template pipeline_config.groovy ... stages { continuous_integration { unit_test static_code_analysis build scan_artifact } } Jenkinsfile on_pull_request to: develop , { continuous_integration () } on_merge to: develop , { continuous_integration () deploy_to dev } on_merge to: main , { deploy_to prod }","title":"Continuous Integration"},{"location":"concepts/pipeline-primitives/steps/","text":"Steps \u00b6 Pipeline Templates represent generic software delivery workflows. Pipeline Templates use Steps to represent tasks in that workflow. Best Practice It is recommended that Steps are named generically. For example, rather than npm_build() the Step should be named build() . By naming Steps generically, multiple libraries can implement the same Step. This allows teams to share the same Pipeline Template by loading different libraries via the Pipeline Configuration . Placeholder Steps \u00b6 Users can create Placeholder Steps that do nothing and serve as a no-op 1 Step. The primary purpose of these Placeholder Steps is to avoid a NoSuchMethodError being thrown when the Pipeline Template attempts to invoke a Step that hasn't been contributed by a library. To define Placeholder Steps, use the template_methods{} block. Example: Defining Placeholder Steps In the following example, a Pipeline Template expects to invoke a unit_test() and a build() step. It's expected that users will declare in their Pipeline Configurations libraries that implement these steps. In case that's not true, the template_methods{} block has been configured to substitute Placeholder Steps to avoid an exception being thrown. Pipeline Template Pipeline Configuration Build Log Jenkinsfile unit_test () build () pipeline_config.groovy template_methods { unit_test build } [Pipeline] Start of Pipeline [JTE][Step - null/unit_test.call()] [Pipeline] echo Step unit_test is not implemented. [JTE][Step - null/build.call()] [Pipeline] echo Step build is not implemented. [Pipeline] End of Pipeline Finished: SUCCESS Library Steps \u00b6 Library Steps are contributed by libraries. Users define in the Pipeline Configuration which libraries to load, if any. Learn More Learn more about how to create libraries over in the Library Development section. No Operation : a command that does nothing. \u21a9","title":"Steps"},{"location":"concepts/pipeline-primitives/steps/#steps","text":"Pipeline Templates represent generic software delivery workflows. Pipeline Templates use Steps to represent tasks in that workflow. Best Practice It is recommended that Steps are named generically. For example, rather than npm_build() the Step should be named build() . By naming Steps generically, multiple libraries can implement the same Step. This allows teams to share the same Pipeline Template by loading different libraries via the Pipeline Configuration .","title":"Steps"},{"location":"concepts/pipeline-primitives/steps/#placeholder-steps","text":"Users can create Placeholder Steps that do nothing and serve as a no-op 1 Step. The primary purpose of these Placeholder Steps is to avoid a NoSuchMethodError being thrown when the Pipeline Template attempts to invoke a Step that hasn't been contributed by a library. To define Placeholder Steps, use the template_methods{} block. Example: Defining Placeholder Steps In the following example, a Pipeline Template expects to invoke a unit_test() and a build() step. It's expected that users will declare in their Pipeline Configurations libraries that implement these steps. In case that's not true, the template_methods{} block has been configured to substitute Placeholder Steps to avoid an exception being thrown. Pipeline Template Pipeline Configuration Build Log Jenkinsfile unit_test () build () pipeline_config.groovy template_methods { unit_test build } [Pipeline] Start of Pipeline [JTE][Step - null/unit_test.call()] [Pipeline] echo Step unit_test is not implemented. [JTE][Step - null/build.call()] [Pipeline] echo Step build is not implemented. [Pipeline] End of Pipeline Finished: SUCCESS","title":"Placeholder Steps"},{"location":"concepts/pipeline-primitives/steps/#library-steps","text":"Library Steps are contributed by libraries. Users define in the Pipeline Configuration which libraries to load, if any. Learn More Learn more about how to create libraries over in the Library Development section. No Operation : a command that does nothing. \u21a9","title":"Library Steps"},{"location":"concepts/pipeline-templates/","text":"Overview \u00b6 In JTE, Pipeline Templates are used to define tool-agnostic workflows that can be shared across teams. Pipeline Templates make use of Pipeline Primitives to become reusable. Just A Jenkinsfile \u00b6 A Pipeline Template is executed exactly like a Jenkinsfile . In fact, there's almost no functional difference between a Jenkinsfile and a Pipeline Template in JTE. Regular Jenkins DSL pipeline steps like node , sh , and echo will all work as expected from a Pipeline Template. What's different, though, is what happens before the template is executed. During Pipeline Initialization , Pipeline Primitives are created and made available to the Pipeline Template. Defining Workflows, not Tech Stacks \u00b6 While creating JTE, it was envisioned that a Pipeline Template represents a workflow - not a pipeline for a specific tech stack. Be careful of the common pitfall of creating an npm template for all your NPM applications and a java template for all your Java applications. If you find yourself doing this - compare those templates and see if there would be a way to make converge with more general step names in a common workflow. One example of having multiple workflows would be if there were two branching strategies used throughout the organization or if JTE was being used for infrastructure pipelines as well as application pipelines. Do whatever works for you At the end of the day, JTE's goal is to make pipeline development easier at scale. Do whatever works best for your organization. Creating a contract between the pipeline and teams \u00b6 One way to think of a Pipeline Template is that it creates an \"API contract\" or interface between the pipeline and development teams. The Pipeline Configuration is what \"hydrates\" the template to make it concrete by declaring which Pipeline Primitives should be loaded. Learn More \u00b6 Page Description Pipeline Catalog Learn about how to build a catalog of Pipeline Templates teams can choose from Declarative Syntax Support Learn how to write templates using Jenkins Declarative Syntax","title":"Overview"},{"location":"concepts/pipeline-templates/#overview","text":"In JTE, Pipeline Templates are used to define tool-agnostic workflows that can be shared across teams. Pipeline Templates make use of Pipeline Primitives to become reusable.","title":"Overview"},{"location":"concepts/pipeline-templates/#just-a-jenkinsfile","text":"A Pipeline Template is executed exactly like a Jenkinsfile . In fact, there's almost no functional difference between a Jenkinsfile and a Pipeline Template in JTE. Regular Jenkins DSL pipeline steps like node , sh , and echo will all work as expected from a Pipeline Template. What's different, though, is what happens before the template is executed. During Pipeline Initialization , Pipeline Primitives are created and made available to the Pipeline Template.","title":"Just A Jenkinsfile"},{"location":"concepts/pipeline-templates/#defining-workflows-not-tech-stacks","text":"While creating JTE, it was envisioned that a Pipeline Template represents a workflow - not a pipeline for a specific tech stack. Be careful of the common pitfall of creating an npm template for all your NPM applications and a java template for all your Java applications. If you find yourself doing this - compare those templates and see if there would be a way to make converge with more general step names in a common workflow. One example of having multiple workflows would be if there were two branching strategies used throughout the organization or if JTE was being used for infrastructure pipelines as well as application pipelines. Do whatever works for you At the end of the day, JTE's goal is to make pipeline development easier at scale. Do whatever works best for your organization.","title":"Defining Workflows, not Tech Stacks"},{"location":"concepts/pipeline-templates/#creating-a-contract-between-the-pipeline-and-teams","text":"One way to think of a Pipeline Template is that it creates an \"API contract\" or interface between the pipeline and development teams. The Pipeline Configuration is what \"hydrates\" the template to make it concrete by declaring which Pipeline Primitives should be loaded.","title":"Creating a contract between the pipeline and teams"},{"location":"concepts/pipeline-templates/#learn-more","text":"Page Description Pipeline Catalog Learn about how to build a catalog of Pipeline Templates teams can choose from Declarative Syntax Support Learn how to write templates using Jenkins Declarative Syntax","title":"Learn More"},{"location":"concepts/pipeline-templates/declarative-syntax/","text":"Declarative Syntax Support \u00b6 JTE has supported writing Pipeline Templates in Declarative Syntax since version 2.0 . Some Background \u00b6 JTE hasn't always supported Declarative Syntax. With JTE, pipeline authors can create Pipeline Templates that look like a custom DSL. Take the following Pipeline Template and Pipeline Configuration for example: Pipeline Template Pipeline Configuration Jenkinsfile on_pull_request to: develop , { continuous_integration () } on_merge to: develop , { continuous_integration () deploy_to dev penetration_test () integration_test () performance_test () } on_merge to: main , { deploy_to prod } pipeline_config.groovy libraries { git // supplies on_pull_request, on_merge docker // supplies build npm // supplies unit test sonarqube // supplies static_code_analysis helm // supplies deploy_to zap // supplies penetration_test cypress // supplies integration_test jmeter // supplies performance_test() } stages { continuous_integration { build unit_test static_code_analysis } } application_environments { dev prod } keywords { develop = ~ /^[Dd]ev(elop|elopment|eloper|)$/ main = ~ /^[Mm](ain|aster)$/ } Many users, however, would still prefer to write Pipeline Templates in Declarative Syntax. Motivation \u00b6 As it's a fully featured programming environment, Scripted Pipeline offers a tremendous amount of flexibility and extensibility to Jenkins users. The Groovy learning-curve isn\u2019t typically desirable for all members of a given team, so Declarative Pipeline was created to offer a simpler and more opinionated syntax for authoring Jenkins Pipeline 1 . Declarative Syntax offers a simpler and more opinionated way to write Jenkins pipelines. Users familiar with Declarative Syntax can get started using JTE. Pipeline Primitives , including Library Steps , can be resolved from a Pipeline Template written in Declarative Syntax. Step Resolution \u00b6 There is one minor behavioral difference between Pipeline Templates written in Scripted Pipeline Syntax vs Declarative Pipeline Syntax in regard to Step Resolution. When a Library Step is loaded that overwrites a Jenkins DSL step, such as sh , then in Scripted Pipeline Templates the Library Step will take precedence whereas in Declarative Pipeline Templates the original sh implementation will take precedence. The way to bypass this in Declarative Syntax to invoke the Library Step is to invoke it from a script block. Declarative Step Resolution Example Declarative Pipeline Syntax Assume a sh Library Step has been loaded. Jenkinsfile pipeline { agent any stages { stage ( \"Example\" ){ steps { sh \"some script\" // (1) script { sh \"some script\" // (2) } } } } } This sh call would invoke the original Jenkins DSL Pipeline Step This sh call, in the script{} block, would invoke the loaded JTE Library Step Taken from the Declarative Syntax documentation. \u21a9","title":"Declarative Syntax Support"},{"location":"concepts/pipeline-templates/declarative-syntax/#declarative-syntax-support","text":"JTE has supported writing Pipeline Templates in Declarative Syntax since version 2.0 .","title":"Declarative Syntax Support"},{"location":"concepts/pipeline-templates/declarative-syntax/#some-background","text":"JTE hasn't always supported Declarative Syntax. With JTE, pipeline authors can create Pipeline Templates that look like a custom DSL. Take the following Pipeline Template and Pipeline Configuration for example: Pipeline Template Pipeline Configuration Jenkinsfile on_pull_request to: develop , { continuous_integration () } on_merge to: develop , { continuous_integration () deploy_to dev penetration_test () integration_test () performance_test () } on_merge to: main , { deploy_to prod } pipeline_config.groovy libraries { git // supplies on_pull_request, on_merge docker // supplies build npm // supplies unit test sonarqube // supplies static_code_analysis helm // supplies deploy_to zap // supplies penetration_test cypress // supplies integration_test jmeter // supplies performance_test() } stages { continuous_integration { build unit_test static_code_analysis } } application_environments { dev prod } keywords { develop = ~ /^[Dd]ev(elop|elopment|eloper|)$/ main = ~ /^[Mm](ain|aster)$/ } Many users, however, would still prefer to write Pipeline Templates in Declarative Syntax.","title":"Some Background"},{"location":"concepts/pipeline-templates/declarative-syntax/#motivation","text":"As it's a fully featured programming environment, Scripted Pipeline offers a tremendous amount of flexibility and extensibility to Jenkins users. The Groovy learning-curve isn\u2019t typically desirable for all members of a given team, so Declarative Pipeline was created to offer a simpler and more opinionated syntax for authoring Jenkins Pipeline 1 . Declarative Syntax offers a simpler and more opinionated way to write Jenkins pipelines. Users familiar with Declarative Syntax can get started using JTE. Pipeline Primitives , including Library Steps , can be resolved from a Pipeline Template written in Declarative Syntax.","title":"Motivation"},{"location":"concepts/pipeline-templates/declarative-syntax/#step-resolution","text":"There is one minor behavioral difference between Pipeline Templates written in Scripted Pipeline Syntax vs Declarative Pipeline Syntax in regard to Step Resolution. When a Library Step is loaded that overwrites a Jenkins DSL step, such as sh , then in Scripted Pipeline Templates the Library Step will take precedence whereas in Declarative Pipeline Templates the original sh implementation will take precedence. The way to bypass this in Declarative Syntax to invoke the Library Step is to invoke it from a script block. Declarative Step Resolution Example Declarative Pipeline Syntax Assume a sh Library Step has been loaded. Jenkinsfile pipeline { agent any stages { stage ( \"Example\" ){ steps { sh \"some script\" // (1) script { sh \"some script\" // (2) } } } } } This sh call would invoke the original Jenkins DSL Pipeline Step This sh call, in the script{} block, would invoke the loaded JTE Library Step Taken from the Declarative Syntax documentation. \u21a9","title":"Step Resolution"},{"location":"concepts/pipeline-templates/pipeline-catalog/","text":"Pipeline Catalog \u00b6 It's unlikely, especially when first starting out, that all your organization's pipelines will map to a single Pipeline Template . JTE supports having multiple Pipeline Templates that teams can choose from. These templates are organized into Pipeline Catalogs configured on Governance Tiers . Default Pipeline Template \u00b6 Pipeline Catalogs can have a default Pipeline Template . Named Pipeline Templates \u00b6 Additional Pipeline Templates are called Named Pipeline Templates . Learn More For more information, you can learn where templates are organized and how to create multiple templates over on the Governance Tier Reference Page page. You can learn more about how JTE chooses the Pipeline Template for a run over at Pipeline Template Selection .","title":"Pipeline Catalog"},{"location":"concepts/pipeline-templates/pipeline-catalog/#pipeline-catalog","text":"It's unlikely, especially when first starting out, that all your organization's pipelines will map to a single Pipeline Template . JTE supports having multiple Pipeline Templates that teams can choose from. These templates are organized into Pipeline Catalogs configured on Governance Tiers .","title":"Pipeline Catalog"},{"location":"concepts/pipeline-templates/pipeline-catalog/#default-pipeline-template","text":"Pipeline Catalogs can have a default Pipeline Template .","title":"Default Pipeline Template"},{"location":"concepts/pipeline-templates/pipeline-catalog/#named-pipeline-templates","text":"Additional Pipeline Templates are called Named Pipeline Templates . Learn More For more information, you can learn where templates are organized and how to create multiple templates over on the Governance Tier Reference Page page. You can learn more about how JTE chooses the Pipeline Template for a run over at Pipeline Template Selection .","title":"Named Pipeline Templates"},{"location":"contributing/building/","text":"Building the Plugin \u00b6 To build the JPI, run: just jpi Once built, the JPI will be located at build/libs/templating-engine.jpi","title":"Building the Plugin"},{"location":"contributing/building/#building-the-plugin","text":"To build the JPI, run: just jpi Once built, the JPI will be located at build/libs/templating-engine.jpi","title":"Building the Plugin"},{"location":"contributing/fork-based/","text":"Fork-Based Contribution Model \u00b6 JTE follows a fork-based contribution model, also called a Fork and Pull Model . Contributors work in their own Forks on Feature Branches and send changes through Pull Requests to the main branch of the upstream JTE repository . Add the Upstream Remote \u00b6 After forking the upstream repository, it's helpful to add the upstream repository as a remote. This can be done by running the command: git remote add upstream https://github.com/jenkinsci/templating-engine-plugin.git Contribution Workflow \u00b6 After creating your fork and adding the upstream remote, use the following workflow to submit changes to JTE: Step Description Git Commands 1 Create a Feature Branch Create a branch specifically for your change git checkout -B <branch_name> git push --set-upstream origin <branch_name> 2 Push Changes Push incremental changes to your feature branch git add <files> git commit -m <message> git push origin <branch_name> 3 Submit a Pull Request Open a Pull Request to the main branch of the JTE repository Done via the GitHub web interface 4 Incorporate Feedback The JTE maintainers may provide some feedback. git add <files> git commit -m <message> git push origin <branch_name> 5 Merge Once accepted, JTE maintainers will merge the PR no action on contributor's part 6 Resynchronize with upstream The merge will squash the commits, so realign your fork with upstream git checkout main git fetch --all git merge upstream/main git push origin main 7 Delete your Feature Branch Your feature branch can now be removed git branch -d <branch_name> git push origin --delete <branch_name>","title":"Fork-Based Contribution Model"},{"location":"contributing/fork-based/#fork-based-contribution-model","text":"JTE follows a fork-based contribution model, also called a Fork and Pull Model . Contributors work in their own Forks on Feature Branches and send changes through Pull Requests to the main branch of the upstream JTE repository .","title":"Fork-Based Contribution Model"},{"location":"contributing/fork-based/#add-the-upstream-remote","text":"After forking the upstream repository, it's helpful to add the upstream repository as a remote. This can be done by running the command: git remote add upstream https://github.com/jenkinsci/templating-engine-plugin.git","title":"Add the Upstream Remote"},{"location":"contributing/fork-based/#contribution-workflow","text":"After creating your fork and adding the upstream remote, use the following workflow to submit changes to JTE: Step Description Git Commands 1 Create a Feature Branch Create a branch specifically for your change git checkout -B <branch_name> git push --set-upstream origin <branch_name> 2 Push Changes Push incremental changes to your feature branch git add <files> git commit -m <message> git push origin <branch_name> 3 Submit a Pull Request Open a Pull Request to the main branch of the JTE repository Done via the GitHub web interface 4 Incorporate Feedback The JTE maintainers may provide some feedback. git add <files> git commit -m <message> git push origin <branch_name> 5 Merge Once accepted, JTE maintainers will merge the PR no action on contributor's part 6 Resynchronize with upstream The merge will squash the commits, so realign your fork with upstream git checkout main git fetch --all git merge upstream/main git push origin main 7 Delete your Feature Branch Your feature branch can now be removed git branch -d <branch_name> git push origin --delete <branch_name>","title":"Contribution Workflow"},{"location":"contributing/linting/","text":"Linting \u00b6 This project uses Spotless and CodeNarc to perform linting. The CodeNarc rule sets for src/main and src/test can be found in config/codenarc/rules.groovy and config/codenarc/rulesTest.groovy , respectively. To execute linting, run: just lint-code Once executed, the reports can be found at build/reports/codenarc/main.html and build/reports/codenarc/test.html .","title":"Linting"},{"location":"contributing/linting/#linting","text":"This project uses Spotless and CodeNarc to perform linting. The CodeNarc rule sets for src/main and src/test can be found in config/codenarc/rules.groovy and config/codenarc/rulesTest.groovy , respectively. To execute linting, run: just lint-code Once executed, the reports can be found at build/reports/codenarc/main.html and build/reports/codenarc/test.html .","title":"Linting"},{"location":"contributing/releasing/","text":"Publishing A Release \u00b6 Prerequisites \u00b6 Release Permissions \u00b6 Permissions are managed here . You'll need sign-off from one of the existing maintainers to be added. Credentials \u00b6 The gradle plugin JTE uses to publish releases expects a file .jenkins-ci.org to be present in the user's home directory with the credentials to authenticate to the Jenkins Artifactory instance. Cutting A Release \u00b6 If you have the permission , you can cut a new release of JTE by running just release <versionNumber> . For example: just release 2 .0.4 This will: create a release/2.0.4 branch update the version in the build.gradle publish a docs release to GitHub pages update the bug issue template version dropdown push those changes create a 2.0.4 tag publish the JPI Don't forget to go to the Release Page to officially release JTE with the current change log based off the most recent tag. Release Drafter is used to maintain release notes for JTE.","title":"Publishing A Release"},{"location":"contributing/releasing/#publishing-a-release","text":"","title":"Publishing A Release"},{"location":"contributing/releasing/#prerequisites","text":"","title":"Prerequisites"},{"location":"contributing/releasing/#release-permissions","text":"Permissions are managed here . You'll need sign-off from one of the existing maintainers to be added.","title":"Release Permissions"},{"location":"contributing/releasing/#credentials","text":"The gradle plugin JTE uses to publish releases expects a file .jenkins-ci.org to be present in the user's home directory with the credentials to authenticate to the Jenkins Artifactory instance.","title":"Credentials"},{"location":"contributing/releasing/#cutting-a-release","text":"If you have the permission , you can cut a new release of JTE by running just release <versionNumber> . For example: just release 2 .0.4 This will: create a release/2.0.4 branch update the version in the build.gradle publish a docs release to GitHub pages update the bug issue template version dropdown push those changes create a 2.0.4 tag publish the JPI Don't forget to go to the Release Page to officially release JTE with the current change log based off the most recent tag. Release Drafter is used to maintain release notes for JTE.","title":"Cutting A Release"},{"location":"contributing/running-tests/","text":"Running Tests \u00b6 Unit tests for JTE are written using Spock . To run all the tests, run: just test The gradle test report is published to build/reports/tests/test/index.html Execute tests for a specific class \u00b6 To run tests for a specific Class, StepWrapperSpec for example, run: just test '*.StepWrapperSpec' Code Coverage \u00b6 By default, JaCoCo is enabled when running test. Once executed, the JaCoCo coverage report can be found at: build/reports/jacoco/test/html/index.html To disable this, run: just --set coverage false test","title":"Running Tests"},{"location":"contributing/running-tests/#running-tests","text":"Unit tests for JTE are written using Spock . To run all the tests, run: just test The gradle test report is published to build/reports/tests/test/index.html","title":"Running Tests"},{"location":"contributing/running-tests/#execute-tests-for-a-specific-class","text":"To run tests for a specific Class, StepWrapperSpec for example, run: just test '*.StepWrapperSpec'","title":"Execute tests for a specific class"},{"location":"contributing/running-tests/#code-coverage","text":"By default, JaCoCo is enabled when running test. Once executed, the JaCoCo coverage report can be found at: build/reports/jacoco/test/html/index.html To disable this, run: just --set coverage false test","title":"Code Coverage"},{"location":"contributing/docs/acronyms/","text":"On Acronyms \u00b6 Acronyms have a place in documentation but please follow the guidance provided by the Microsoft Writing Style Guide . If a new acronym is introduced, update the glossary at docs/glossary.md . Updating the glossary will provide hover-over expansion of the acronym. Glossary in Action Check out what happens when using acronyms from the glossary: DSL, JTE, SCM, IDE","title":"On Acronyms"},{"location":"contributing/docs/acronyms/#on-acronyms","text":"Acronyms have a place in documentation but please follow the guidance provided by the Microsoft Writing Style Guide . If a new acronym is introduced, update the glossary at docs/glossary.md . Updating the glossary will provide hover-over expansion of the acronym. Glossary in Action Check out what happens when using acronyms from the glossary: DSL, JTE, SCM, IDE","title":"On Acronyms"},{"location":"contributing/docs/add-or-remove-pages/","text":"Add or Remove Pages \u00b6 The page tree for this docs site is maintained in the mkdocs.yml file at the root of the repository. The structure nav section of the file defines how pages are organized. Important When adding or removing pages from the docs site, make sure to update mkdocs.yml to update the page tree.","title":"Add or Remove Pages"},{"location":"contributing/docs/add-or-remove-pages/#add-or-remove-pages","text":"The page tree for this docs site is maintained in the mkdocs.yml file at the root of the repository. The structure nav section of the file defines how pages are organized. Important When adding or removing pages from the docs site, make sure to update mkdocs.yml to update the page tree.","title":"Add or Remove Pages"},{"location":"contributing/docs/documentation-structure/","text":"Documentation Structure \u00b6 The JTE documentation is organized into the following sections: Concepts , Reference , Tutorials , & How-To Guides . Section Overview \u00b6 Oriented To Must Form Concepts understanding explain an article Reference information describe specifics no-fluff specifications Tutorials learning help new users get started a hands-on lesson How-To Guides a goal solve a specific problem a step-by-step walkthrough Note Huge shout out to Divio for formalizing this approach here . The previous table comes from the introduction to this documentation system. Finding What You Need \u00b6 The MkDocs config file ( mkdocs.yml ) at the root of the repository defines the page tree for the entire docs site. The following table outlines where the categories of documentation are stored. Section Location Concepts docs/concepts Reference docs/reference Tutorials docs/tutorials How-To Guides docs/how-to Contributing docs/contributing","title":"Documentation Structure"},{"location":"contributing/docs/documentation-structure/#documentation-structure","text":"The JTE documentation is organized into the following sections: Concepts , Reference , Tutorials , & How-To Guides .","title":"Documentation Structure"},{"location":"contributing/docs/documentation-structure/#section-overview","text":"Oriented To Must Form Concepts understanding explain an article Reference information describe specifics no-fluff specifications Tutorials learning help new users get started a hands-on lesson How-To Guides a goal solve a specific problem a step-by-step walkthrough Note Huge shout out to Divio for formalizing this approach here . The previous table comes from the introduction to this documentation system.","title":"Section Overview"},{"location":"contributing/docs/documentation-structure/#finding-what-you-need","text":"The MkDocs config file ( mkdocs.yml ) at the root of the repository defines the page tree for the entire docs site. The following table outlines where the categories of documentation are stored. Section Location Concepts docs/concepts Reference docs/reference Tutorials docs/tutorials How-To Guides docs/how-to Contributing docs/contributing","title":"Finding What You Need"},{"location":"contributing/docs/getting-started/","text":"Getting Started \u00b6 Presumably you're here because you want to help by updating the JTE documentation, so thank you! Tools \u00b6 These docs are written in Markdown 1 and compiled using MkDocs with the Material for MkDocs theme. Development activities take place within containers and are orchestrated using Just . Tool Description Just a task runner similar to Make with a simpler syntax Docker runtime environments are encapsulated in container images MkDocs Documentation framework Material for MkDocs Documentation styling Mike Documentation versioning Markdownlint Markdown Linter Vale Prose Linter Visual Studio Code Recommended IDE for docs development Required Prerequisites You can get by with only Just and Docker. Local installations of the other tools may prove useful for development but aren't required. Learn More \u00b6 Topic Description Documentation Structure Learn how JTE's docs are organized Local Development Learn how to make changes to the docs locally Markdownlint Learn more about the Markdown linter Prose Linting with Vale Learn more about the Prose linter Add or Remove Pages Learn how to update the page tree Markdown Cheatsheet Learn more about Markdown Syntax for this site Markdown \u21a9","title":"Getting Started"},{"location":"contributing/docs/getting-started/#getting-started","text":"Presumably you're here because you want to help by updating the JTE documentation, so thank you!","title":"Getting Started"},{"location":"contributing/docs/getting-started/#tools","text":"These docs are written in Markdown 1 and compiled using MkDocs with the Material for MkDocs theme. Development activities take place within containers and are orchestrated using Just . Tool Description Just a task runner similar to Make with a simpler syntax Docker runtime environments are encapsulated in container images MkDocs Documentation framework Material for MkDocs Documentation styling Mike Documentation versioning Markdownlint Markdown Linter Vale Prose Linter Visual Studio Code Recommended IDE for docs development Required Prerequisites You can get by with only Just and Docker. Local installations of the other tools may prove useful for development but aren't required.","title":"Tools"},{"location":"contributing/docs/getting-started/#learn-more","text":"Topic Description Documentation Structure Learn how JTE's docs are organized Local Development Learn how to make changes to the docs locally Markdownlint Learn more about the Markdown linter Prose Linting with Vale Learn more about the Prose linter Add or Remove Pages Learn how to update the page tree Markdown Cheatsheet Learn more about Markdown Syntax for this site Markdown \u21a9","title":"Learn More"},{"location":"contributing/docs/local-development/","text":"Local Development \u00b6 Local Docs Server \u00b6 MkDocs supports a local development server so that changes can be viewed in the browser in real-time as they're made. To see changes in real-time, run just serve . After a few seconds, a local version of the docs will be hosted at http://localhost:8000 . Prerequisites Check out the Prerequisites to make sure you've got the required tools installed. Integrated Development Environment Integration \u00b6 Visual Studio Code is the recommended IDE for updating the docs as it has extensions for both linting tools used: markdownlint Vale Local Tool Installation To use the VS Code extensions for markdownlint and vale you'll have to install those tools locally.","title":"Local Development"},{"location":"contributing/docs/local-development/#local-development","text":"","title":"Local Development"},{"location":"contributing/docs/local-development/#local-docs-server","text":"MkDocs supports a local development server so that changes can be viewed in the browser in real-time as they're made. To see changes in real-time, run just serve . After a few seconds, a local version of the docs will be hosted at http://localhost:8000 . Prerequisites Check out the Prerequisites to make sure you've got the required tools installed.","title":"Local Docs Server"},{"location":"contributing/docs/local-development/#integrated-development-environment-integration","text":"Visual Studio Code is the recommended IDE for updating the docs as it has extensions for both linting tools used: markdownlint Vale Local Tool Installation To use the VS Code extensions for markdownlint and vale you'll have to install those tools locally.","title":"Integrated Development Environment Integration"},{"location":"contributing/docs/markdown-cheatsheet/","text":"Markdown Cheatsheet \u00b6 Headers \u00b6 # H1 ## H2 ### H3 #### H4 ##### H5 ###### H6 Emphasis \u00b6 Style Markdown Italics *Italics* Bold **Bold** Bold and Italics **_Bold and Italics_** Links \u00b6 There are 3 primary ways embed a hyperlink. Markdown Rendered 1. you can use an [ inline link ]( https://google.com ) 2. you can use a [ link by reference ][ 1 ] 3. you can use the [link text itself] as the reference [ 1 ]: https://google.com [ link text itself ]: https://google.com you can use an inline link you can use a link by reference you can use the link text itself as the reference Tables \u00b6 Tip Tables are kind of a pain in markdown. This tool can help to generate your markdown tables for you. The Markdown Table Prettifier extension for VS Code is also pretty great. Markdown Rendered | column 1 | column 2 | column 3 | | ---------- | :---------: | -----------: | | column 1 | column 2 | column 3 is | | is left | is | is right | | aligned | centered | aligned | column 1 column 2 column 3 column 1 column 2 column 3 is is left is is right aligned centered aligned Inline Code Snippets \u00b6 Markdown Rendered Inline `code` snippets use `backticks` around them Inline code snippets use backticks around them Code Blocks \u00b6 code blocks use three backticks and the language name for syntax highlighting: Markdown Rendered ``` groovy title=\"filename.groovy\" def s = [ 1, 2, 3] s.each{ item -> println item } ``` filename.groovy def s = [ 1 , 2 , 3 ] s . each { item -> println item } Admonitions \u00b6 Squidfunk covers this on the Admonitions page of the Material for MkDocs docs site. Please use consistent admonitions based upon the type of content being added. Admonition Type Description example Examples of what's being discussed tip A recommendation from the maintainers danger Call outs for common gotchas info Redirect users to more information Emojis \u00b6 To embed an emoji, simply surround the emoji name with two colons: :emoji-name: . A list of the available emojis can be found at emojipedia . Content Tabs \u00b6 Content Tabs have been used throughout this page. === \"Tab Title A\" some markdown content === \"Tab Title B\" some other markdown content Tab Title A Tab Title B some markdown content some other markdown content Footnotes \u00b6 Markdown Rendered Footnotes[^1] are supported. [ ^1 ]: some footnote information Footnotes 1 are supported. here's some footnote text \u21a9","title":"Markdown Cheatsheet"},{"location":"contributing/docs/markdown-cheatsheet/#markdown-cheatsheet","text":"","title":"Markdown Cheatsheet"},{"location":"contributing/docs/markdown-cheatsheet/#headers","text":"# H1 ## H2 ### H3 #### H4 ##### H5 ###### H6","title":"Headers"},{"location":"contributing/docs/markdown-cheatsheet/#emphasis","text":"Style Markdown Italics *Italics* Bold **Bold** Bold and Italics **_Bold and Italics_**","title":"Emphasis"},{"location":"contributing/docs/markdown-cheatsheet/#links","text":"There are 3 primary ways embed a hyperlink. Markdown Rendered 1. you can use an [ inline link ]( https://google.com ) 2. you can use a [ link by reference ][ 1 ] 3. you can use the [link text itself] as the reference [ 1 ]: https://google.com [ link text itself ]: https://google.com you can use an inline link you can use a link by reference you can use the link text itself as the reference","title":"Links"},{"location":"contributing/docs/markdown-cheatsheet/#tables","text":"Tip Tables are kind of a pain in markdown. This tool can help to generate your markdown tables for you. The Markdown Table Prettifier extension for VS Code is also pretty great. Markdown Rendered | column 1 | column 2 | column 3 | | ---------- | :---------: | -----------: | | column 1 | column 2 | column 3 is | | is left | is | is right | | aligned | centered | aligned | column 1 column 2 column 3 column 1 column 2 column 3 is is left is is right aligned centered aligned","title":"Tables"},{"location":"contributing/docs/markdown-cheatsheet/#inline-code-snippets","text":"Markdown Rendered Inline `code` snippets use `backticks` around them Inline code snippets use backticks around them","title":"Inline Code Snippets"},{"location":"contributing/docs/markdown-cheatsheet/#code-blocks","text":"code blocks use three backticks and the language name for syntax highlighting: Markdown Rendered ``` groovy title=\"filename.groovy\" def s = [ 1, 2, 3] s.each{ item -> println item } ``` filename.groovy def s = [ 1 , 2 , 3 ] s . each { item -> println item }","title":"Code Blocks"},{"location":"contributing/docs/markdown-cheatsheet/#admonitions","text":"Squidfunk covers this on the Admonitions page of the Material for MkDocs docs site. Please use consistent admonitions based upon the type of content being added. Admonition Type Description example Examples of what's being discussed tip A recommendation from the maintainers danger Call outs for common gotchas info Redirect users to more information","title":"Admonitions"},{"location":"contributing/docs/markdown-cheatsheet/#emojis","text":"To embed an emoji, simply surround the emoji name with two colons: :emoji-name: . A list of the available emojis can be found at emojipedia .","title":"Emojis"},{"location":"contributing/docs/markdown-cheatsheet/#content-tabs","text":"Content Tabs have been used throughout this page. === \"Tab Title A\" some markdown content === \"Tab Title B\" some other markdown content Tab Title A Tab Title B some markdown content some other markdown content","title":"Content Tabs"},{"location":"contributing/docs/markdown-cheatsheet/#footnotes","text":"Markdown Rendered Footnotes[^1] are supported. [ ^1 ]: some footnote information Footnotes 1 are supported. here's some footnote text \u21a9","title":"Footnotes"},{"location":"contributing/docs/markdown-lint/","text":"Markdownlint \u00b6 Markdownlint is used to ensure consistency in the structure of the Markdown files across the docs pages. Perform Linting \u00b6 To run the markdownlint linter run: just lint-markdown . Rules \u00b6 The Rules for markdownlint explain what's enforced with examples. Ignoring Violations \u00b6 In rare cases, it's necessary to ignore markdown lint violations . Configuration \u00b6 The .markdownlint-cli2.yaml configuration file is used for IDE integration and for the checks that run during a Pull Request. IDE Integration Check out the Local Development page to learn more about IDE integration for markdownlint.","title":"Markdownlint"},{"location":"contributing/docs/markdown-lint/#markdownlint","text":"Markdownlint is used to ensure consistency in the structure of the Markdown files across the docs pages.","title":"Markdownlint"},{"location":"contributing/docs/markdown-lint/#perform-linting","text":"To run the markdownlint linter run: just lint-markdown .","title":"Perform Linting"},{"location":"contributing/docs/markdown-lint/#rules","text":"The Rules for markdownlint explain what's enforced with examples.","title":"Rules"},{"location":"contributing/docs/markdown-lint/#ignoring-violations","text":"In rare cases, it's necessary to ignore markdown lint violations .","title":"Ignoring Violations"},{"location":"contributing/docs/markdown-lint/#configuration","text":"The .markdownlint-cli2.yaml configuration file is used for IDE integration and for the checks that run during a Pull Request. IDE Integration Check out the Local Development page to learn more about IDE integration for markdownlint.","title":"Configuration"},{"location":"contributing/docs/vale/","text":"Prose Linting with Vale \u00b6 These docs use Vale to ensure consistency of prose style. Perform Linting \u00b6 Run just lint-prose to specifically lint the documentation prose. Style Guide \u00b6 Vale uses the Microsoft Writing Style Guide . The styles for Vale can be found in docs/styles/Microsoft and were taken from here . Configuration \u00b6 The .vale.ini file at the root of the repository is used to configure Vale for both IDE integration and for the checks that run during a Pull Request. IDE Integration Check out the Local Development page to learn more about IDE integration for Vale.","title":"Prose Linting with Vale"},{"location":"contributing/docs/vale/#prose-linting-with-vale","text":"These docs use Vale to ensure consistency of prose style.","title":"Prose Linting with Vale"},{"location":"contributing/docs/vale/#perform-linting","text":"Run just lint-prose to specifically lint the documentation prose.","title":"Perform Linting"},{"location":"contributing/docs/vale/#style-guide","text":"Vale uses the Microsoft Writing Style Guide . The styles for Vale can be found in docs/styles/Microsoft and were taken from here .","title":"Style Guide"},{"location":"contributing/docs/vale/#configuration","text":"The .vale.ini file at the root of the repository is used to configure Vale for both IDE integration and for the checks that run during a Pull Request. IDE Integration Check out the Local Development page to learn more about IDE integration for Vale.","title":"Configuration"},{"location":"developer/","text":"Developer Docs \u00b6 Local Development Environment \u00b6 Tool Purpose Gradle Used to run unit tests, package the JPI, and publish the plugin Just A task runner. Used here to automate common commands used during development. Docker Used to build the documentation for local preview Topics \u00b6 Topic Description Pipeline Initialization The bulk of JTE - covers what JTE does before a Pipeline Run Jenkins Configuration Explains how JTE is configured through the Jenkins UI Testing Covers how unit testing is done for JTE","title":"Developer Docs"},{"location":"developer/#developer-docs","text":"","title":"Developer Docs"},{"location":"developer/#local-development-environment","text":"Tool Purpose Gradle Used to run unit tests, package the JPI, and publish the plugin Just A task runner. Used here to automate common commands used during development. Docker Used to build the documentation for local preview","title":"Local Development Environment"},{"location":"developer/#topics","text":"Topic Description Pipeline Initialization The bulk of JTE - covers what JTE does before a Pipeline Run Jenkins Configuration Explains how JTE is configured through the Jenkins UI Testing Covers how unit testing is done for JTE","title":"Topics"},{"location":"developer/architecture-overview/","text":"Pipeline Initialization \u00b6 flowchart TD subgraph External to JTE WorkflowRun end subgraph JTE WorkflowRun-->|create|TemplateFlowDefinition TemplateFlowDefinition-->PipelineConfigurationAggregator TemplateFlowDefinition-->PipelineTemplateResolver TemplateFlowDefinition-->TemplatePrimitiveInjector end Often, knowing which file to start with can be the biggest hurdle to understanding a new codebase. In JTE, the class that kicks everything off is TemplateFlowDefinition . When a pipeline kicks off, a WorkflowRun is created and eventually the run() method is called. The run() method fetches the job's FlowDefinition and invokes create() to return a FlowExecution . JTE's TemplateFlowDefinition.create() is where Pipeline Initialization is performed. Pipeline Initialization Phases \u00b6 The steps JTE performs before the pipeline starts is referred to as Pipeline Initialization . Pipeline Initialization Stage Description Aggregate Pipeline Configurations Fetches the run's Pipeline Configuration files, parses them, and then merges them to create the Aggregated Pipeline Configuration Determine the Pipeline Template Reads the Aggregated Pipeline Configuration and fetches the appropriate Pipeline Template Create the Pipeline Primitives Parses the Aggregated Pipeline Configuration to create the Pipeline Primitives","title":"Pipeline Initialization"},{"location":"developer/architecture-overview/#pipeline-initialization","text":"flowchart TD subgraph External to JTE WorkflowRun end subgraph JTE WorkflowRun-->|create|TemplateFlowDefinition TemplateFlowDefinition-->PipelineConfigurationAggregator TemplateFlowDefinition-->PipelineTemplateResolver TemplateFlowDefinition-->TemplatePrimitiveInjector end Often, knowing which file to start with can be the biggest hurdle to understanding a new codebase. In JTE, the class that kicks everything off is TemplateFlowDefinition . When a pipeline kicks off, a WorkflowRun is created and eventually the run() method is called. The run() method fetches the job's FlowDefinition and invokes create() to return a FlowExecution . JTE's TemplateFlowDefinition.create() is where Pipeline Initialization is performed.","title":"Pipeline Initialization"},{"location":"developer/architecture-overview/#pipeline-initialization-phases","text":"The steps JTE performs before the pipeline starts is referred to as Pipeline Initialization . Pipeline Initialization Stage Description Aggregate Pipeline Configurations Fetches the run's Pipeline Configuration files, parses them, and then merges them to create the Aggregated Pipeline Configuration Determine the Pipeline Template Reads the Aggregated Pipeline Configuration and fetches the appropriate Pipeline Template Create the Pipeline Primitives Parses the Aggregated Pipeline Configuration to create the Pipeline Primitives","title":"Pipeline Initialization Phases"},{"location":"developer/jenkins-config/","text":"Jenkins Configurations \u00b6 There are two ways JTE contributes to the Jenkins User Interface to expose configurations to users. UI Configuration Description Governance Tiers lets users configure a hierarchy of Pipeline Configurations and Library Sources. Jobs and Folders Pipeline Jobs, Folders, Multi-Branch Jobs, and Organization Jobs","title":"Jenkins Configurations"},{"location":"developer/jenkins-config/#jenkins-configurations","text":"There are two ways JTE contributes to the Jenkins User Interface to expose configurations to users. UI Configuration Description Governance Tiers lets users configure a hierarchy of Pipeline Configurations and Library Sources. Jobs and Folders Pipeline Jobs, Folders, Multi-Branch Jobs, and Organization Jobs","title":"Jenkins Configurations"},{"location":"developer/jenkins-config/governance-tiers/","text":"Governance Tiers \u00b6 Governance Tiers allow users to configure hierarchies of Pipeline Configurations and Library Sources . Adding Governance Tiers to the Jenkins User Interface \u00b6 Governance Tiers can be configured globally in Manage Jenkins > Configure System and as a Folder property. The below class structure shows how JTE adds a Governance Tier to these locations. flowchart GovernanceTier-->|encapsulated by|TemplateGlobalConfig TemplateGlobalConfig-->|contributes to|GlobalConfiguration GovernanceTier-->|ecncapsulated by|TemplateConfigFolderProperty TemplateConfigFolderProperty-->|contributes to|Folder Folder-->|inherits|OrganizationFolder Folder-->|inherits|MultiBranchProject Implementing Configuration Sources \u00b6 Governance Tiers define two Extension Points. Extension Point Description PipelineConfigurationProvider defines an interface for fetching Pipeline Configurations and Pipeline Templates LibraryProvider defines an interface for fetching libraries from a Library Source Note Jenkins makes heavy use of ExtensionPoint to allow plugin classes to register themselves and augment functionality. classDiagram class GovernanceTier{ PipelineConfigurationProvider configurationProvider List~LibrarySource~ librarySources getHierarchy()$ List~GovernanceTier~ +getLibrarySources() List~LibrarySource~ +getConfig() PipelineConfigurationObject +getJenkinsfile() String +getTemplate(String template) String } class PipelineConfigurationProvider{ <<abstract>> +PipelineConfigurationObject getConfig() +String getJenkinsfile() +String getTemplate() } GovernanceTier <.. PipelineConfigurationProvider class LibrarySource{ -LibraryProvider libraryProvider +getLibraryProvider() LibraryProvider } class LibraryProvider{ <<abstract>> +hasLibrary(String libName) boolean +getLibrarySchema(String libName) String +loadLibrary(String libName) } LibrarySource <.. LibraryProvider GovernanceTier <.. LibrarySource Existing Extensions \u00b6 PipelineConfigurationProvider \u00b6 Concrete Extension Description NullPipelineConfigurationProvider the no-op default. provides no configuration. ConsolePipelineConfigurationProvider define configuration in the Jenkins UI SCMPipelineConfigurationProvider define configuration in a remote repository LibraryProvider \u00b6 Concrete Extension Description ScmLibraryProvider fetch libraries from a remote repository PluginLibraryProvider fetch libraries from a Jenkins plugin","title":"Governance Tiers"},{"location":"developer/jenkins-config/governance-tiers/#governance-tiers","text":"Governance Tiers allow users to configure hierarchies of Pipeline Configurations and Library Sources .","title":"Governance Tiers"},{"location":"developer/jenkins-config/governance-tiers/#adding-governance-tiers-to-the-jenkins-user-interface","text":"Governance Tiers can be configured globally in Manage Jenkins > Configure System and as a Folder property. The below class structure shows how JTE adds a Governance Tier to these locations. flowchart GovernanceTier-->|encapsulated by|TemplateGlobalConfig TemplateGlobalConfig-->|contributes to|GlobalConfiguration GovernanceTier-->|ecncapsulated by|TemplateConfigFolderProperty TemplateConfigFolderProperty-->|contributes to|Folder Folder-->|inherits|OrganizationFolder Folder-->|inherits|MultiBranchProject","title":"Adding Governance Tiers to the Jenkins User Interface"},{"location":"developer/jenkins-config/governance-tiers/#implementing-configuration-sources","text":"Governance Tiers define two Extension Points. Extension Point Description PipelineConfigurationProvider defines an interface for fetching Pipeline Configurations and Pipeline Templates LibraryProvider defines an interface for fetching libraries from a Library Source Note Jenkins makes heavy use of ExtensionPoint to allow plugin classes to register themselves and augment functionality. classDiagram class GovernanceTier{ PipelineConfigurationProvider configurationProvider List~LibrarySource~ librarySources getHierarchy()$ List~GovernanceTier~ +getLibrarySources() List~LibrarySource~ +getConfig() PipelineConfigurationObject +getJenkinsfile() String +getTemplate(String template) String } class PipelineConfigurationProvider{ <<abstract>> +PipelineConfigurationObject getConfig() +String getJenkinsfile() +String getTemplate() } GovernanceTier <.. PipelineConfigurationProvider class LibrarySource{ -LibraryProvider libraryProvider +getLibraryProvider() LibraryProvider } class LibraryProvider{ <<abstract>> +hasLibrary(String libName) boolean +getLibrarySchema(String libName) String +loadLibrary(String libName) } LibrarySource <.. LibraryProvider GovernanceTier <.. LibrarySource","title":"Implementing Configuration Sources"},{"location":"developer/jenkins-config/governance-tiers/#existing-extensions","text":"","title":"Existing Extensions"},{"location":"developer/jenkins-config/governance-tiers/#pipelineconfigurationprovider","text":"Concrete Extension Description NullPipelineConfigurationProvider the no-op default. provides no configuration. ConsolePipelineConfigurationProvider define configuration in the Jenkins UI SCMPipelineConfigurationProvider define configuration in a remote repository","title":"PipelineConfigurationProvider"},{"location":"developer/jenkins-config/governance-tiers/#libraryprovider","text":"Concrete Extension Description ScmLibraryProvider fetch libraries from a remote repository PluginLibraryProvider fetch libraries from a Jenkins plugin","title":"LibraryProvider"},{"location":"developer/jenkins-config/job-types/","text":"Job Types \u00b6 TemplateFlowDefinition is a custom implementation of FlowDefinition that allows JTE to perform Pipeline Initialization . Class Structure \u00b6 flowchart TD subgraph Organization Job TemplateMultiBranchProjectFactory end subgraph MultiBranch Job TemplateBranchProjectFactory-->|creates|MultiBranchTemplateFlowDefinition end subgraph Pipeline Job ConsoleAdHocTemplateFlowDefinitionConfiguration-.->|configures|AdHocTemplateFlowDefinition ScmAdHocTemplateFlowDefinitionConfiguration-.->|configures|AdHocTemplateFlowDefinition end TemplateFlowDefinition AdHocTemplateFlowDefinition-->|extends|TemplateFlowDefinition MultiBranchTemplateFlowDefinition-->|extends|TemplateFlowDefinition TemplateMultiBranchProjectFactory-->|creates|TemplateBranchProjectFactory Class Purpose TemplateFlowDefinition Makes a pipeline a JTE pipeline AdHocTemplateFlowDefinition Adds JTE as an option to Pipeline Jobs MultiBranchTemplateFlowDefinition Enables Multi-Branch Pipelines to use JTE TemplateBranchProjectFactory Adds JTE as an option to create Multi-Branch Projects TemplateMultiBranchProjectFactory Adds JTE as an option to create Organization Projects","title":"Job Types"},{"location":"developer/jenkins-config/job-types/#job-types","text":"TemplateFlowDefinition is a custom implementation of FlowDefinition that allows JTE to perform Pipeline Initialization .","title":"Job Types"},{"location":"developer/jenkins-config/job-types/#class-structure","text":"flowchart TD subgraph Organization Job TemplateMultiBranchProjectFactory end subgraph MultiBranch Job TemplateBranchProjectFactory-->|creates|MultiBranchTemplateFlowDefinition end subgraph Pipeline Job ConsoleAdHocTemplateFlowDefinitionConfiguration-.->|configures|AdHocTemplateFlowDefinition ScmAdHocTemplateFlowDefinitionConfiguration-.->|configures|AdHocTemplateFlowDefinition end TemplateFlowDefinition AdHocTemplateFlowDefinition-->|extends|TemplateFlowDefinition MultiBranchTemplateFlowDefinition-->|extends|TemplateFlowDefinition TemplateMultiBranchProjectFactory-->|creates|TemplateBranchProjectFactory Class Purpose TemplateFlowDefinition Makes a pipeline a JTE pipeline AdHocTemplateFlowDefinition Adds JTE as an option to Pipeline Jobs MultiBranchTemplateFlowDefinition Enables Multi-Branch Pipelines to use JTE TemplateBranchProjectFactory Adds JTE as an option to create Multi-Branch Projects TemplateMultiBranchProjectFactory Adds JTE as an option to create Organization Projects","title":"Class Structure"},{"location":"developer/pipeline-initialization/","text":"Pipeline Initialization \u00b6 flowchart TD subgraph External to JTE WorkflowRun end subgraph JTE WorkflowRun-->|create|TemplateFlowDefinition TemplateFlowDefinition-->PipelineConfigurationAggregator TemplateFlowDefinition-->PipelineTemplateResolver TemplateFlowDefinition-->TemplatePrimitiveInjector end Often, knowing which file to start with can be the biggest hurdle to understanding a new codebase. In JTE, the class that kicks everything off is TemplateFlowDefinition . When a pipeline kicks off, a WorkflowRun is created and eventually the run() method is called. The run() method fetches the job's FlowDefinition and invokes create() to return a FlowExecution . JTE's TemplateFlowDefinition.create() is where Pipeline Initialization is performed. Stages of Pipeline Initialization \u00b6 The steps JTE performs before the pipeline starts is referred to as Pipeline Initialization . Pipeline Initialization Stage Description Aggregate Pipeline Configurations Fetches the run's Pipeline Configuration files, parses them, and then merges them to create the Aggregated Pipeline Configuration Determine the Pipeline Template Reads the Aggregated Pipeline Configuration and fetches the appropriate Pipeline Template Create the Pipeline Primitives Parses the Aggregated Pipeline Configuration to create the Pipeline Primitives","title":"Pipeline Initialization"},{"location":"developer/pipeline-initialization/#pipeline-initialization","text":"flowchart TD subgraph External to JTE WorkflowRun end subgraph JTE WorkflowRun-->|create|TemplateFlowDefinition TemplateFlowDefinition-->PipelineConfigurationAggregator TemplateFlowDefinition-->PipelineTemplateResolver TemplateFlowDefinition-->TemplatePrimitiveInjector end Often, knowing which file to start with can be the biggest hurdle to understanding a new codebase. In JTE, the class that kicks everything off is TemplateFlowDefinition . When a pipeline kicks off, a WorkflowRun is created and eventually the run() method is called. The run() method fetches the job's FlowDefinition and invokes create() to return a FlowExecution . JTE's TemplateFlowDefinition.create() is where Pipeline Initialization is performed.","title":"Pipeline Initialization"},{"location":"developer/pipeline-initialization/#stages-of-pipeline-initialization","text":"The steps JTE performs before the pipeline starts is referred to as Pipeline Initialization . Pipeline Initialization Stage Description Aggregate Pipeline Configurations Fetches the run's Pipeline Configuration files, parses them, and then merges them to create the Aggregated Pipeline Configuration Determine the Pipeline Template Reads the Aggregated Pipeline Configuration and fetches the appropriate Pipeline Template Create the Pipeline Primitives Parses the Aggregated Pipeline Configuration to create the Pipeline Primitives","title":"Stages of Pipeline Initialization"},{"location":"developer/pipeline-initialization/aggregate-configs/","text":"Aggregating Pipeline Configuration \u00b6 Build the Aggregated Pipeline Configuration \u00b6 PipelineConfigurationAggregator handles fetching the configuration hierarchy from Governance Tiers and the job itself. For example, a given job would get its parent chain all the way up to the Jenkins instance's Global Configuration. flowchart TD g[Global Configuration] g --> f_a[Folder A] g --> f_b[Folder B] f_a --> j_a[Job A] f_a --> j_b[Job B] f_b --> j_c[JobC] f_b --> j_d[Job D] style j_b fill:#FFD580,stroke:#000000 Turns into: flowchart LR g[Global Configuration] g --> f_a[Folder A] f_a --> j_b[Job B] style j_b fill:#FFD580,stroke:#000000 If a Governance Tier or the job itself have a Pipeline Configuration, the DSL is parsed into a corresponding PipelineConfigurationObject . These PipelineConfigurationObject s are then added together to create the Aggregated Pipeline Configuration. flowchart LR g[Global Configuration] gc[Global Pipeline Configuration] ac[Folder A Pipeline Config] jc[Job Pipeline Config] agg[Aggregated Pipeline Config] g --> f_a[Folder A] f_a --> j_b[Job B] style j_b fill:#FFD580,stroke:#000000 gc <-.- g ac <-.- f_a jc <-.- j_b gc -.->| + |ac ac -.->| + |jc jc -.->| = |agg Note PipelineConfigurationObject overrides the plus operator so adding them together merges two configurations . Pipeline Configuration Parsing \u00b6 The Pipeline Configuration Syntax is a custom Domain-Specific Language (DSL). The org.boozallen.plugins.jte.init.governance.dsl package has all the classes related to Pipeline Configuration parsing. A Pipeline Configuration file is executed as a Groovy Script with a custom binding that makes the env variable available to the configuration file. The Script is compiled using PipelineConfigurationBuilder as the script base class which overrides methodMissing , propertyMissing , and setProperty to build the PipelineConfigurationObject defined by the DSL. This script is executed in the same groovy sandbox as Jenkins Pipelines to secure the script execution. Note To learn more about methodMissing , propertyMissing , and setProperty check out Groovy Runtime Metaprogramming .","title":"Aggregating Pipeline Configuration"},{"location":"developer/pipeline-initialization/aggregate-configs/#aggregating-pipeline-configuration","text":"","title":"Aggregating Pipeline Configuration"},{"location":"developer/pipeline-initialization/aggregate-configs/#build-the-aggregated-pipeline-configuration","text":"PipelineConfigurationAggregator handles fetching the configuration hierarchy from Governance Tiers and the job itself. For example, a given job would get its parent chain all the way up to the Jenkins instance's Global Configuration. flowchart TD g[Global Configuration] g --> f_a[Folder A] g --> f_b[Folder B] f_a --> j_a[Job A] f_a --> j_b[Job B] f_b --> j_c[JobC] f_b --> j_d[Job D] style j_b fill:#FFD580,stroke:#000000 Turns into: flowchart LR g[Global Configuration] g --> f_a[Folder A] f_a --> j_b[Job B] style j_b fill:#FFD580,stroke:#000000 If a Governance Tier or the job itself have a Pipeline Configuration, the DSL is parsed into a corresponding PipelineConfigurationObject . These PipelineConfigurationObject s are then added together to create the Aggregated Pipeline Configuration. flowchart LR g[Global Configuration] gc[Global Pipeline Configuration] ac[Folder A Pipeline Config] jc[Job Pipeline Config] agg[Aggregated Pipeline Config] g --> f_a[Folder A] f_a --> j_b[Job B] style j_b fill:#FFD580,stroke:#000000 gc <-.- g ac <-.- f_a jc <-.- j_b gc -.->| + |ac ac -.->| + |jc jc -.->| = |agg Note PipelineConfigurationObject overrides the plus operator so adding them together merges two configurations .","title":"Build the Aggregated Pipeline Configuration"},{"location":"developer/pipeline-initialization/aggregate-configs/#pipeline-configuration-parsing","text":"The Pipeline Configuration Syntax is a custom Domain-Specific Language (DSL). The org.boozallen.plugins.jte.init.governance.dsl package has all the classes related to Pipeline Configuration parsing. A Pipeline Configuration file is executed as a Groovy Script with a custom binding that makes the env variable available to the configuration file. The Script is compiled using PipelineConfigurationBuilder as the script base class which overrides methodMissing , propertyMissing , and setProperty to build the PipelineConfigurationObject defined by the DSL. This script is executed in the same groovy sandbox as Jenkins Pipelines to secure the script execution. Note To learn more about methodMissing , propertyMissing , and setProperty check out Groovy Runtime Metaprogramming .","title":"Pipeline Configuration Parsing"},{"location":"developer/pipeline-initialization/determine-template/","text":"Determining the Pipeline Template \u00b6 Pipeline Template Resolution \u00b6 PipelineTemplateResolver implements the Pipeline Template Selection flow . Pipeline Template Compilation \u00b6 PipelineTemplateCompiler intercepts the compilation of the resolved Pipeline Template to wrap it in a try-catch-finally block that invokes the Lifecycle Hooks .","title":"Determining the Pipeline Template"},{"location":"developer/pipeline-initialization/determine-template/#determining-the-pipeline-template","text":"","title":"Determining the Pipeline Template"},{"location":"developer/pipeline-initialization/determine-template/#pipeline-template-resolution","text":"PipelineTemplateResolver implements the Pipeline Template Selection flow .","title":"Pipeline Template Resolution"},{"location":"developer/pipeline-initialization/determine-template/#pipeline-template-compilation","text":"PipelineTemplateCompiler intercepts the compilation of the resolved Pipeline Template to wrap it in a try-catch-finally block that invokes the Lifecycle Hooks .","title":"Pipeline Template Compilation"},{"location":"developer/pipeline-initialization/inject-primitives/","text":"Inject Pipeline Primitives \u00b6 Pipeline Primitives \u00b6 Pipeline Primitives are objects that can be instantiated and \"injected\" into a JTE pipeline's execution to create concrete implementations of resources used in a Pipeline Template. Each primitive extends TemplatePrimitive which is a Serializable GlobalVariable to ensure they can be persisted and discovered. Primitives That Execute Pipeline Code \u00b6 Pipeline Primitives that execute pipeline code, such as Library Steps ( StepWrapper ) or Stages ( Stage ), should create a separate class in the resources directory and override the getValue(..) method such that what's actually returned to the pipeline is a CPS-transformed class. TemplatePrimitiveInjector \u00b6 Each Pipeline Primitive has a corresponding TemplatePrimitiveInjector that's responsible for parsing the Aggregated Pipeline Configuration and populating a TemplatePrimitiveNamespace holding the created TemplatePrimitves . Class Structure \u00b6 classDiagram class TemplatePrimitiveInjector{ <<abstract>> orchestrate(CpsFlowExecution exec, PipelineConfigurationObject config)$ +validateConfiguration(CpsFlowExecution exec, PipelineConfigurationObject config) +injectPrimitives(CpsFlowExecution exec, PipelineConfigurationObject config) TemplatePrimitiveNamespace +validatePrimitives(CpsFlowExecution exec, PipelineConfigurationObject config, TemplatePrimitiveCollector collector) } class TemplatePrimitiveNamespace{ +String name +List~TemplatePrimitive~ primitives +TemplatePrimitiveNamespace parent +getProperty(String property) TemplatePrimitive +methodMissing(String methodName, Object args) TemplatePrimitive } TemplatePrimitiveInjector \"0\" ..> \"many\"TemplatePrimitiveNamespace : produces class TemplatePrimitiveCollector{ +List~TemplatePrimitiveNamespace~ namespaces } TemplatePrimitiveNamespace --o TemplatePrimitiveCollector class TemplatePrimitiveProvider{ +forRun(Run run) List~GlobalVariable~ } TemplatePrimitiveProvider .. TemplatePrimitiveCollector : inner class class TemplatePrimitive{ +String name +TemplatePrimitiveNamespace parent +getValue(CpsScript script) TemplatePrimitive } TemplatePrimitive ..> TemplatePrimitiveNamespace : populates Functional Interface \u00b6 There are three phases to Pipeline Primitive Injection that are mapped to three methods a TemplatePrimitiveInjector can override. Phase Description validateConfiguration Receives the Aggregated Pipeline Configuration so that syntax validation can occur injectPrimitives Where injectors should instantiate Pipeline Primitives based on the Aggregated Pipeline Configuration validatePrimitives Where validations should occur on the Pipeline Primitives that have been instantiated Pipeline Primitive Resolution \u00b6 These TemplatePrimitiveNamespace s are collected in the TemplatePrimitiveCollector , which is persisted as an InvisibleAction on the Pipeline Run. TemplatePrimitiveCollector , then, has a subclass called TemplatePrimitiveProvider that extends GlobalVariableSet . When a Jenkins pipeline tries to resolve a method (in CpsScript.invokeMethod() ) it will check for the GlobalVariables available to the run and resolve the Pipeline Primitives that have been created. flowchart LR script[Jenkins Pipeline] subgraph CpsScript invokeMethod end script-->|invoke|CpsScript subgraph GlobalVariable byName --> forRun forRun --> GlobalVariableSet end invokeMethod --> GlobalVariable subgraph JTE TemplatePrimitiveProvider --> TemplatePrimitiveCollector TemplatePrimitiveCollector --> TemplatePrimitiveNamespace TemplatePrimitiveNamespace --> TemplatePrimitive end GlobalVariableSet-->JTE Injector Sequencing \u00b6 Each TemplatePrimitiveCollector can annotate the three methods of Pipeline Primitive Injection with a @RunAfter annotation that accepts the TemplatePrimitiveInjectors that this injector should run after. This approach has been easier to handle than ordinals for sequencing the injectors. When TemplatePrimitiveInjector.orchestrate() is called, each phase of Pipeline Primitive Injection is called in sequence. For each phase, a Directed Acyclic Graph of TemplatePrimitiveInjectors is created based on the dependencies defined by @RunAfter annotations. The injectors are then execution in the order determined by the graph. For example, for the injectPrimitives phase, the graph looks like: flowchart LR root --> ApplicationEnvironmentInjector root --> KeywordInjector root --> LibraryStepInjector LibraryStepInjector --> DefaultStepInjector DefaultStepInjector --> TemplateMethodInjector TemplateMethodInjector --> StageInjector","title":"Inject Pipeline Primitives"},{"location":"developer/pipeline-initialization/inject-primitives/#inject-pipeline-primitives","text":"","title":"Inject Pipeline Primitives"},{"location":"developer/pipeline-initialization/inject-primitives/#pipeline-primitives","text":"Pipeline Primitives are objects that can be instantiated and \"injected\" into a JTE pipeline's execution to create concrete implementations of resources used in a Pipeline Template. Each primitive extends TemplatePrimitive which is a Serializable GlobalVariable to ensure they can be persisted and discovered.","title":"Pipeline Primitives"},{"location":"developer/pipeline-initialization/inject-primitives/#primitives-that-execute-pipeline-code","text":"Pipeline Primitives that execute pipeline code, such as Library Steps ( StepWrapper ) or Stages ( Stage ), should create a separate class in the resources directory and override the getValue(..) method such that what's actually returned to the pipeline is a CPS-transformed class.","title":"Primitives That Execute Pipeline Code"},{"location":"developer/pipeline-initialization/inject-primitives/#templateprimitiveinjector","text":"Each Pipeline Primitive has a corresponding TemplatePrimitiveInjector that's responsible for parsing the Aggregated Pipeline Configuration and populating a TemplatePrimitiveNamespace holding the created TemplatePrimitves .","title":"TemplatePrimitiveInjector"},{"location":"developer/pipeline-initialization/inject-primitives/#class-structure","text":"classDiagram class TemplatePrimitiveInjector{ <<abstract>> orchestrate(CpsFlowExecution exec, PipelineConfigurationObject config)$ +validateConfiguration(CpsFlowExecution exec, PipelineConfigurationObject config) +injectPrimitives(CpsFlowExecution exec, PipelineConfigurationObject config) TemplatePrimitiveNamespace +validatePrimitives(CpsFlowExecution exec, PipelineConfigurationObject config, TemplatePrimitiveCollector collector) } class TemplatePrimitiveNamespace{ +String name +List~TemplatePrimitive~ primitives +TemplatePrimitiveNamespace parent +getProperty(String property) TemplatePrimitive +methodMissing(String methodName, Object args) TemplatePrimitive } TemplatePrimitiveInjector \"0\" ..> \"many\"TemplatePrimitiveNamespace : produces class TemplatePrimitiveCollector{ +List~TemplatePrimitiveNamespace~ namespaces } TemplatePrimitiveNamespace --o TemplatePrimitiveCollector class TemplatePrimitiveProvider{ +forRun(Run run) List~GlobalVariable~ } TemplatePrimitiveProvider .. TemplatePrimitiveCollector : inner class class TemplatePrimitive{ +String name +TemplatePrimitiveNamespace parent +getValue(CpsScript script) TemplatePrimitive } TemplatePrimitive ..> TemplatePrimitiveNamespace : populates","title":"Class Structure"},{"location":"developer/pipeline-initialization/inject-primitives/#functional-interface","text":"There are three phases to Pipeline Primitive Injection that are mapped to three methods a TemplatePrimitiveInjector can override. Phase Description validateConfiguration Receives the Aggregated Pipeline Configuration so that syntax validation can occur injectPrimitives Where injectors should instantiate Pipeline Primitives based on the Aggregated Pipeline Configuration validatePrimitives Where validations should occur on the Pipeline Primitives that have been instantiated","title":"Functional Interface"},{"location":"developer/pipeline-initialization/inject-primitives/#pipeline-primitive-resolution","text":"These TemplatePrimitiveNamespace s are collected in the TemplatePrimitiveCollector , which is persisted as an InvisibleAction on the Pipeline Run. TemplatePrimitiveCollector , then, has a subclass called TemplatePrimitiveProvider that extends GlobalVariableSet . When a Jenkins pipeline tries to resolve a method (in CpsScript.invokeMethod() ) it will check for the GlobalVariables available to the run and resolve the Pipeline Primitives that have been created. flowchart LR script[Jenkins Pipeline] subgraph CpsScript invokeMethod end script-->|invoke|CpsScript subgraph GlobalVariable byName --> forRun forRun --> GlobalVariableSet end invokeMethod --> GlobalVariable subgraph JTE TemplatePrimitiveProvider --> TemplatePrimitiveCollector TemplatePrimitiveCollector --> TemplatePrimitiveNamespace TemplatePrimitiveNamespace --> TemplatePrimitive end GlobalVariableSet-->JTE","title":"Pipeline Primitive Resolution"},{"location":"developer/pipeline-initialization/inject-primitives/#injector-sequencing","text":"Each TemplatePrimitiveCollector can annotate the three methods of Pipeline Primitive Injection with a @RunAfter annotation that accepts the TemplatePrimitiveInjectors that this injector should run after. This approach has been easier to handle than ordinals for sequencing the injectors. When TemplatePrimitiveInjector.orchestrate() is called, each phase of Pipeline Primitive Injection is called in sequence. For each phase, a Directed Acyclic Graph of TemplatePrimitiveInjectors is created based on the dependencies defined by @RunAfter annotations. The injectors are then execution in the order determined by the graph. For example, for the injectPrimitives phase, the graph looks like: flowchart LR root --> ApplicationEnvironmentInjector root --> KeywordInjector root --> LibraryStepInjector LibraryStepInjector --> DefaultStepInjector DefaultStepInjector --> TemplateMethodInjector TemplateMethodInjector --> StageInjector","title":"Injector Sequencing"},{"location":"developer/testing/","text":"Testing \u00b6 JTE tests are written using Spock and make heavy use of the Jenkins Test Harness . Mocking \u00b6 In general, mocking is discouraged. It is preferred that instead, the Jenkins Test Harness is used to create actual JTE Pipeline Runs that exercise the functionality to be tested. This approach results in tests that are easier to write, decoupled from the technical implementation of the feature, and produce higher confidence that the functionality will work in a Jenkins pipeline. Given the nuances of CPS - nothing beats the real thing when validating JTE is working as expected. Work in Progress You'll find examples of unit tests using Spock's mocking functionality. These tests should be refactored to use the Jenkins Test Harness as time allows. Test Utilities \u00b6 There are several utility test classes that help perform common setup operations for tests. Test Class Description TestFlowExecutionOwner simplifies mocking FlowExecutionOwner TestLibraryProvider a utility for creating libraries to test TestUtil a helper for creating JTE pipeline jobs to test Testing Git Repositories \u00b6 The GitSampleRepoRule from the git plugin is used to create local git repositories. See AdHocTemplateFlowDefinitionSpec for examples of usage. Testing Pipeline Resumability \u00b6 The RestartableJenkinsRule is used for testing that JTE pipelines can be successfully resumed after being paused or after a Jenkins graceful restart. See ResumabilitySpec for examples of usage.","title":"Testing"},{"location":"developer/testing/#testing","text":"JTE tests are written using Spock and make heavy use of the Jenkins Test Harness .","title":"Testing"},{"location":"developer/testing/#mocking","text":"In general, mocking is discouraged. It is preferred that instead, the Jenkins Test Harness is used to create actual JTE Pipeline Runs that exercise the functionality to be tested. This approach results in tests that are easier to write, decoupled from the technical implementation of the feature, and produce higher confidence that the functionality will work in a Jenkins pipeline. Given the nuances of CPS - nothing beats the real thing when validating JTE is working as expected. Work in Progress You'll find examples of unit tests using Spock's mocking functionality. These tests should be refactored to use the Jenkins Test Harness as time allows.","title":"Mocking"},{"location":"developer/testing/#test-utilities","text":"There are several utility test classes that help perform common setup operations for tests. Test Class Description TestFlowExecutionOwner simplifies mocking FlowExecutionOwner TestLibraryProvider a utility for creating libraries to test TestUtil a helper for creating JTE pipeline jobs to test","title":"Test Utilities"},{"location":"developer/testing/#testing-git-repositories","text":"The GitSampleRepoRule from the git plugin is used to create local git repositories. See AdHocTemplateFlowDefinitionSpec for examples of usage.","title":"Testing Git Repositories"},{"location":"developer/testing/#testing-pipeline-resumability","text":"The RestartableJenkinsRule is used for testing that JTE pipelines can be successfully resumed after being paused or after a Jenkins graceful restart. See ResumabilitySpec for examples of usage.","title":"Testing Pipeline Resumability"},{"location":"developer/testing/local-jenkins/","text":"Running A Local Jenkins \u00b6 It's often helpful to run Jenkins in a container locally to test various scenarios with JTE during development. just run With the default settings, this will expose jenkins on http://localhost:8080 Change the container name \u00b6 just --set container someName run Change the port forwarding target \u00b6 just --set port 9000 run Pass arbitrary flags to the container \u00b6 Parameters passed to just run are sent as flags to the docker run command. just run -e SOMEVAR = \"some var\" Mounting local libraries for testing \u00b6 Local directories can be configured as Git SCM Library Sources even if they don't have a remote repository. For example, if ~/local-libraries is a directory containing a local git repository then to mount it to the container you would run: just run -v ~/local-libraries:/local-libraries You could then configure a Library Source using the file protocol to specify the repository location at file:///local-libraries Tip When using this technique, changes to the libraries must be committed to be found. In a separate terminal, run: just watch ~/local-libraries to automatically commit changes to the libraries.","title":"Running A Local Jenkins"},{"location":"developer/testing/local-jenkins/#running-a-local-jenkins","text":"It's often helpful to run Jenkins in a container locally to test various scenarios with JTE during development. just run With the default settings, this will expose jenkins on http://localhost:8080","title":"Running A Local Jenkins"},{"location":"developer/testing/local-jenkins/#change-the-container-name","text":"just --set container someName run","title":"Change the container name"},{"location":"developer/testing/local-jenkins/#change-the-port-forwarding-target","text":"just --set port 9000 run","title":"Change the port forwarding target"},{"location":"developer/testing/local-jenkins/#pass-arbitrary-flags-to-the-container","text":"Parameters passed to just run are sent as flags to the docker run command. just run -e SOMEVAR = \"some var\"","title":"Pass arbitrary flags to the container"},{"location":"developer/testing/local-jenkins/#mounting-local-libraries-for-testing","text":"Local directories can be configured as Git SCM Library Sources even if they don't have a remote repository. For example, if ~/local-libraries is a directory containing a local git repository then to mount it to the container you would run: just run -v ~/local-libraries:/local-libraries You could then configure a Library Source using the file protocol to specify the repository location at file:///local-libraries Tip When using this technique, changes to the libraries must be committed to be found. In a separate terminal, run: just watch ~/local-libraries to automatically commit changes to the libraries.","title":"Mounting local libraries for testing"},{"location":"how-to/overview/","text":"Overview \u00b6 How-To Guides are goal oriented step-by-step instructions for specific problems. Coming Soon! Tutorials and How-To Guides are next up on the priority list after this initial release of the new docs site is over! How-To Guide Description Upgrade to 2.0 An overview of the differences between JTE 1.X and 2.X","title":"Overview"},{"location":"how-to/overview/#overview","text":"How-To Guides are goal oriented step-by-step instructions for specific problems. Coming Soon! Tutorials and How-To Guides are next up on the priority list after this initial release of the new docs site is over! How-To Guide Description Upgrade to 2.0 An overview of the differences between JTE 1.X and 2.X","title":"Overview"},{"location":"how-to/library-development/lifecycle-hooks-on-failure/","text":"Trigger Lifecycle Hooks On Failure \u00b6 JTE's Lifecycle Hooks allow library developers to register steps that should execute based on triggers. These triggers include the start of the pipeline ( @Init , @Validate ), before steps ( @BeforeStep ), after steps ( @AfterStep , @Notify ), and at the end of the pipeline ( @CleanUp , @Notify ). A common use case is using the conditional execution functionality of lifecycle hooks to trigger the hook when a step or the pipeline fail. Using the currentBuild variable won't work Often, developers will try to use the currentBuild.currentResult value to achieve this. Unfortunately, for the case of pipeline failures the Run's result is not set to FAILURE until the pipeline has completed. This is because exceptions thrown may be caught further up the call stack. Using @Notify for failure \u00b6 Typically, the @Notify hook should be used for pipeline code to send alerts. The @Notify steps runs both after steps and at the end of the pipeline. To ascertain which event triggered the hook, you can use the step field on the hookContext variable. Triggering On Failure of a Step Triggering On Failure of the Pipeline @Notify ({ hookContext . step && hookContext . exceptionThrown }) void call (){ println \"this will run after any step that throws an exception\" } @Notify ({ hookContext . step == null && hookContext . exceptionThrown }) void call (){ println \"this will run if the pipeline throws an uncaught exception\" }","title":"Trigger Lifecycle Hooks On Failure"},{"location":"how-to/library-development/lifecycle-hooks-on-failure/#trigger-lifecycle-hooks-on-failure","text":"JTE's Lifecycle Hooks allow library developers to register steps that should execute based on triggers. These triggers include the start of the pipeline ( @Init , @Validate ), before steps ( @BeforeStep ), after steps ( @AfterStep , @Notify ), and at the end of the pipeline ( @CleanUp , @Notify ). A common use case is using the conditional execution functionality of lifecycle hooks to trigger the hook when a step or the pipeline fail. Using the currentBuild variable won't work Often, developers will try to use the currentBuild.currentResult value to achieve this. Unfortunately, for the case of pipeline failures the Run's result is not set to FAILURE until the pipeline has completed. This is because exceptions thrown may be caught further up the call stack.","title":"Trigger Lifecycle Hooks On Failure"},{"location":"how-to/library-development/lifecycle-hooks-on-failure/#using-notify-for-failure","text":"Typically, the @Notify hook should be used for pipeline code to send alerts. The @Notify steps runs both after steps and at the end of the pipeline. To ascertain which event triggered the hook, you can use the step field on the hookContext variable. Triggering On Failure of a Step Triggering On Failure of the Pipeline @Notify ({ hookContext . step && hookContext . exceptionThrown }) void call (){ println \"this will run after any step that throws an exception\" } @Notify ({ hookContext . step == null && hookContext . exceptionThrown }) void call (){ println \"this will run if the pipeline throws an uncaught exception\" }","title":"Using @Notify for failure"},{"location":"how-to/library-development/package-libraries-as-plugin/","text":"Package a Library Source as a Plugin \u00b6 Users can package their Library Source as a stand-alone Jenkins Plugin. This has a couple advantages: Performance : Avoid fetching from a remote repository on every Pipeline Run Plugin Dependencies : Ensure the Jenkins Plugins your libraries depend upon are installed Gradle JTE Plugin \u00b6 To package your Library Source, the simplest path is to use the Gradle JTE Plugin . plugins { // used to packate these libraries as a jenkins plugin id \"io.jenkins.jte\" version \"0.2.0\" } You can use the configuraiton options defined in the README to configure the plugin. To package your Library Source, run ./gradlew jte . The hpi file will be output to your projects build directory. from there, you can install the hpi file via the Jenkins UI. Note To see an example Library Source, check out the jte-library-scaffold .","title":"Package a Library Source as a Plugin"},{"location":"how-to/library-development/package-libraries-as-plugin/#package-a-library-source-as-a-plugin","text":"Users can package their Library Source as a stand-alone Jenkins Plugin. This has a couple advantages: Performance : Avoid fetching from a remote repository on every Pipeline Run Plugin Dependencies : Ensure the Jenkins Plugins your libraries depend upon are installed","title":"Package a Library Source as a Plugin"},{"location":"how-to/library-development/package-libraries-as-plugin/#gradle-jte-plugin","text":"To package your Library Source, the simplest path is to use the Gradle JTE Plugin . plugins { // used to packate these libraries as a jenkins plugin id \"io.jenkins.jte\" version \"0.2.0\" } You can use the configuraiton options defined in the README to configure the plugin. To package your Library Source, run ./gradlew jte . The hpi file will be output to your projects build directory. from there, you can install the hpi file via the Jenkins UI. Note To see an example Library Source, check out the jte-library-scaffold .","title":"Gradle JTE Plugin"},{"location":"how-to/upgrade-2.0/","text":"2.0 Upgrade Guide \u00b6 This page is going to help walk you through the breaking changes associated with 2.0. Library File Structure \u00b6 To support Library Resources , the file structure of libraries has been reorganized. Pre-2.0 Post-2.0 . \u251c\u2500\u2500 libraryA // libraries are directories \u2502 \u251c\u2500\u2500 library_config.groovy // library config at root \u2502 \u2514\u2500\u2500 someOtherStep.groovy // step files at root of library directory \u2514\u2500\u2500 libraryB \u251c\u2500\u2500 library_config.groovy \u2514\u2500\u2500 someOtherStep.groovy . \u251c\u2500\u2500 libraryA // libraries are still directories \u2502 \u251c\u2500\u2500 library_config.groovy // library config still at root \u2502 \u251c\u2500\u2500 resources // new resources directory! \u2502 \u2502 \u2514\u2500\u2500 someScript.sh \u2502 \u2514\u2500\u2500 steps // steps are now in a steps directory \u2502 \u2514\u2500\u2500 someOtherStep.groovy \u2514\u2500\u2500 libraryB \u251c\u2500\u2500 library_config.groovy \u251c\u2500\u2500 resources \u2502 \u2514\u2500\u2500 someData.json \u2514\u2500\u2500 steps \u2514\u2500\u2500 someOtherStep.groovy @override & @merge annotations \u00b6 Previously, the ability to govern pipeline configuration changes were confined to the block-level - with no way to govern individual fields. To address this, JTE has pivoted from the flags merge=true & override=true to the annotations @merge & @override . These annotations can be placed on individual fields within a block, enabling field-level governance. Pre-2.0 Post-2.0 someBlock { merge = true // future configs can add fields to this block my_governed_field = \"some value\" // cannot be modified } anotherBlock { override = true // entire block can be overridden. no way to only override a field in a block. may_not_be_changed = true default_value_may_be_changed = true } @merge someBlock { // future configs can add fields to this block my_governed_field = \"some value\" } anotherBlock { // future configs can't add fields to this block may_not_be_changed = true // not modifiable @override default_value = true // may be overridden } Top level pipeline configuration values and the jte{} block \u00b6 Previously, there were top level configuration values like allow_scm_jenkinsfile and pipeline_template . These values are now in the jte block in the pipeline_config Pre-2.0 Post-2.0 pipeline_config.groovy allow_scm_jenkinsfile = false pipeline_template = \"my_template\" libraries {} // just here to show the relation to the root pipeline_config.groovy jte { allow_scm_jenkinsfile = false pipeline_template = \"my_template\" } libraries {} // just here to show relation to the root Lifecycle Hook: hookContext \u00b6 JTE provides some syntactic sugar by means of autowiring variables to library steps to simplify library development. Previously, library steps that implemented lifecycle hooks were required to accept a method parameter to accept the hook context. This parameter was typically called context but could be called anything. Pre-2.0 Post-2.0 @AfterStep ({ context . step == \"build\" }) // variable called context void call ( context ){ // hooks required to accept a method parameter println \"running after the ${context.step} step\" } @AfterStep ({ hookContext . step == \"build\" }) // variable called hookContext void call (){ // no method parameter required println \"running after the ${hookContext.step} step\" // hookContext variable autowired } Configuration Changes \u00b6 JTE 2.0 resulted in significant refactoring of the codebase and underlying package structure. Global Configurations \u00b6 There have been updates to the underlying class structure of the Global Governance Tier configured in Manage Jenkins > Configure System > Jenkins Templating Engine . This will impact the Jenkins Configuration as Code (JCasC) YAML schema used to configure JTE. Tip It's recommended to configure the Global Governance Tier manually the way you require and exporting the JCasC YAML to see the schema required to automate configuring JTE. Job Configurations \u00b6 There have been updates to the underlying package and class structure for JTE as a whole as well as feature development for ad hoc pipeline jobs. This impacts Job DSL scripts used to configure jobs utilizing JTE. JTE also now supports fetching the Pipeline Configuration and Pipeline Template for a one-off pipeline job, which results in some changes to the structure of Job DSL for ad hoc pipeline jobs. Tip Job DSL supports Dynamic DSL which means that Job DSL supports the Jenkins Templating Engine settings. It's recommended to utilize the Job DSL API Viewer on your Jenkins Instance once JTE 2.0 has been installed to see how to configure JTE settings.","title":"2.0 Upgrade Guide"},{"location":"how-to/upgrade-2.0/#20-upgrade-guide","text":"This page is going to help walk you through the breaking changes associated with 2.0.","title":"2.0 Upgrade Guide"},{"location":"how-to/upgrade-2.0/#library-file-structure","text":"To support Library Resources , the file structure of libraries has been reorganized. Pre-2.0 Post-2.0 . \u251c\u2500\u2500 libraryA // libraries are directories \u2502 \u251c\u2500\u2500 library_config.groovy // library config at root \u2502 \u2514\u2500\u2500 someOtherStep.groovy // step files at root of library directory \u2514\u2500\u2500 libraryB \u251c\u2500\u2500 library_config.groovy \u2514\u2500\u2500 someOtherStep.groovy . \u251c\u2500\u2500 libraryA // libraries are still directories \u2502 \u251c\u2500\u2500 library_config.groovy // library config still at root \u2502 \u251c\u2500\u2500 resources // new resources directory! \u2502 \u2502 \u2514\u2500\u2500 someScript.sh \u2502 \u2514\u2500\u2500 steps // steps are now in a steps directory \u2502 \u2514\u2500\u2500 someOtherStep.groovy \u2514\u2500\u2500 libraryB \u251c\u2500\u2500 library_config.groovy \u251c\u2500\u2500 resources \u2502 \u2514\u2500\u2500 someData.json \u2514\u2500\u2500 steps \u2514\u2500\u2500 someOtherStep.groovy","title":"Library File Structure"},{"location":"how-to/upgrade-2.0/#override-merge-annotations","text":"Previously, the ability to govern pipeline configuration changes were confined to the block-level - with no way to govern individual fields. To address this, JTE has pivoted from the flags merge=true & override=true to the annotations @merge & @override . These annotations can be placed on individual fields within a block, enabling field-level governance. Pre-2.0 Post-2.0 someBlock { merge = true // future configs can add fields to this block my_governed_field = \"some value\" // cannot be modified } anotherBlock { override = true // entire block can be overridden. no way to only override a field in a block. may_not_be_changed = true default_value_may_be_changed = true } @merge someBlock { // future configs can add fields to this block my_governed_field = \"some value\" } anotherBlock { // future configs can't add fields to this block may_not_be_changed = true // not modifiable @override default_value = true // may be overridden }","title":"@override &amp; @merge annotations"},{"location":"how-to/upgrade-2.0/#top-level-pipeline-configuration-values-and-the-jte-block","text":"Previously, there were top level configuration values like allow_scm_jenkinsfile and pipeline_template . These values are now in the jte block in the pipeline_config Pre-2.0 Post-2.0 pipeline_config.groovy allow_scm_jenkinsfile = false pipeline_template = \"my_template\" libraries {} // just here to show the relation to the root pipeline_config.groovy jte { allow_scm_jenkinsfile = false pipeline_template = \"my_template\" } libraries {} // just here to show relation to the root","title":"Top level pipeline configuration values and the jte{} block"},{"location":"how-to/upgrade-2.0/#lifecycle-hook-hookcontext","text":"JTE provides some syntactic sugar by means of autowiring variables to library steps to simplify library development. Previously, library steps that implemented lifecycle hooks were required to accept a method parameter to accept the hook context. This parameter was typically called context but could be called anything. Pre-2.0 Post-2.0 @AfterStep ({ context . step == \"build\" }) // variable called context void call ( context ){ // hooks required to accept a method parameter println \"running after the ${context.step} step\" } @AfterStep ({ hookContext . step == \"build\" }) // variable called hookContext void call (){ // no method parameter required println \"running after the ${hookContext.step} step\" // hookContext variable autowired }","title":"Lifecycle Hook: hookContext"},{"location":"how-to/upgrade-2.0/#configuration-changes","text":"JTE 2.0 resulted in significant refactoring of the codebase and underlying package structure.","title":"Configuration Changes"},{"location":"how-to/upgrade-2.0/#global-configurations","text":"There have been updates to the underlying class structure of the Global Governance Tier configured in Manage Jenkins > Configure System > Jenkins Templating Engine . This will impact the Jenkins Configuration as Code (JCasC) YAML schema used to configure JTE. Tip It's recommended to configure the Global Governance Tier manually the way you require and exporting the JCasC YAML to see the schema required to automate configuring JTE.","title":"Global Configurations"},{"location":"how-to/upgrade-2.0/#job-configurations","text":"There have been updates to the underlying package and class structure for JTE as a whole as well as feature development for ad hoc pipeline jobs. This impacts Job DSL scripts used to configure jobs utilizing JTE. JTE also now supports fetching the Pipeline Configuration and Pipeline Template for a one-off pipeline job, which results in some changes to the structure of Job DSL for ad hoc pipeline jobs. Tip Job DSL supports Dynamic DSL which means that Job DSL supports the Jenkins Templating Engine settings. It's recommended to utilize the Job DSL API Viewer on your Jenkins Instance once JTE 2.0 has been installed to see how to configure JTE settings.","title":"Job Configurations"},{"location":"reference/autowired-variables/","text":"Autowired Variables \u00b6 The JTE framework often makes use of autowired variables to share both configuration data and contextual information. This page outlines the various autowired variables, their scope, and what data they provide. Overview \u00b6 Variable Description Scope pipelineConfig Represents the aggregated Pipeline Configuration Accessible everywhere jte The Primitive Namespace object Accessible everywhere config Represents a library's configuration provided by the aggregated Pipeline Configuration Within Library Steps stepContext Enables step introspection. Especially helpful when using Step Aliasing Within Library Steps hookContext Represents contextual information for Lifecycle Hooks Within Library Steps Autowired Global Variables \u00b6 pipelineConfig \u00b6 The pipelineConfig is accessible from everywhere and allows access to the aggregated Pipeline Configuration as a Map . Example Usage of pipelineConfig An example of accessing the Pipeline Configuration via pipelineConfig : Pipeline Configuration Pipeline Template pipeline_config.groovy keywords { foo = \"bar\" } random_field = 11 Jenkinsfile println pipelineConfig . keywords . foo println pipelineConfig . random_field jte \u00b6 The jte variable represents the Primitive Namespace . All loaded Pipeline Primitives for a Run can be accessed via the jte variable This is different from the pipelineConfig variable. The pipelineConfig variable gives a Map representation of the aggregated Pipeline Configuration whereas the jte variable allows access to the actual Pipeline Primitive objects . Example Usage of jte Assume there's a gradle and an npm library that both contribute a build() step. By default, loading would result in the pipeline failing. However, you can perform Step Overloading by setting jte.permissive_initialization to True . The jte would be used in this scenario to invoke the build() step from the gradle and npm libraries. Pipeline Configuration Pipeline Template pipeline_config.groovy jte { permissive_initialization = true } libraries { gradle npm } Jenkinsfile // invoke the gradle build step jte . libraries . gradle . build () // invoke the npm build step jte . libraries . npm . build () jte block vs jte variable You may have noticed in the example above that a jte{} block is used in the Pipeline Configuration and a jte variable is used in the Pipeline Template. These are different things. The jte{} block refers to framework-level feature flags as explained on the Pipeline Configuration schema page. The jte variable refers to the Pipeline Primitive Namespace variable. steps \u00b6 The steps variable doesn't technically come from JTE. It's a feature of all Jenkins Pipelines. The steps variable allows direct access to invoke Jenkins Pipeline DSL Steps. This variable is most commonly used when invoking Jenkins Pipeline DSL Steps from a Library Class or when Overloading Steps . Autowired Library Step Variables \u00b6 The following variables are only accessible within Library Steps . config \u00b6 The config variable represents the library configuration for the library that contributed the step as a Map . Example Usage of config Assume there's a gradle library that contributes a build() step. Pipeline Configuration Library build() Step pipeline_config.groovy libraries { gradle { version = \"6.3\" } } build.groovy void call (){ String gradleVersion = config . version } hookContext \u00b6 The hookContext variable provides information about the current step to Lifecycle Hooks . Property Type Description library String The library that contributed the step that triggered the Lifecycle Hook. Is null when the Lifecycle Hook wasn't triggered by a step. step String The name of the Library Step that triggered the Lifecycle Hook. Is null when the Lifecycle Hook wasn't triggered by a step. methodName String The name of the method within the step that was invoked to trigger the Lifecycle Hook. Is null when the Lifecycle Hook wasn't triggered by a step. exceptionThrown Boolean When the hook is triggered by a step, this refers to if the step triggering step threw an exception. When the hook is triggered by template completion, refers to if there is an uncaught exception that will fail the pipeline. Example hookContext usage The following example shows how to use the hookContext variable so that a Lifecycle Hook only triggers after the build() step from the gradle library. Lifecycle Hook Step @AfterStep ({ hookContext . library == \"gradle\" && hookContext . step == \"build\" }) void call (){ println \"running after the ${hookContext.library}'s ${hookContext.step} step\" } stageContext \u00b6 The stageContext variable provides information about the current Stage . Property Type Description name String The name of the current Stage being executed. null if step isn't being executed as part of a Stage. args Map The named parameters provided to the Stage. An empty Map if no parameters provided. Example usage of stageContext The following example shows how to modify step behavior based upon Stage context. Pipeline Configuration NPM unit_test() Step pipeline_config.groovy libraries { npm // contributes unit_test() sonarqube // contributes static_code_analysis() } stages { ci { unit_test static_code_analysis } } unit_test.groovy void call (){ if ( stageContext . name == \"ci\" ){ println \"running as part of the ci Stage\" } } stepContext \u00b6 The stepContext allows step introspection, such as querying the name of the library providing the step or the current name of the step. Property Type Description library String The name of the library that contributed the step name String The current name of the step. May differ from the basename of the step's groovy file if using Step Aliasing isAlias Boolean Is true when stepContext.name refers to an alias Example usage of stepContext Aliased Step Pipeline Template generic.groovy @StepAlias ([ \"build\" , \"unit_test\" ]) void call (){ println \"currently running as ${stepContext.name}\" } Jenkinsfile build () // prints \"currently running as build\" unit_test () // prints \"currently running as unit_test\"","title":"Autowired Variables"},{"location":"reference/autowired-variables/#autowired-variables","text":"The JTE framework often makes use of autowired variables to share both configuration data and contextual information. This page outlines the various autowired variables, their scope, and what data they provide.","title":"Autowired Variables"},{"location":"reference/autowired-variables/#overview","text":"Variable Description Scope pipelineConfig Represents the aggregated Pipeline Configuration Accessible everywhere jte The Primitive Namespace object Accessible everywhere config Represents a library's configuration provided by the aggregated Pipeline Configuration Within Library Steps stepContext Enables step introspection. Especially helpful when using Step Aliasing Within Library Steps hookContext Represents contextual information for Lifecycle Hooks Within Library Steps","title":"Overview"},{"location":"reference/autowired-variables/#autowired-global-variables","text":"","title":"Autowired Global Variables"},{"location":"reference/autowired-variables/#pipelineconfig","text":"The pipelineConfig is accessible from everywhere and allows access to the aggregated Pipeline Configuration as a Map . Example Usage of pipelineConfig An example of accessing the Pipeline Configuration via pipelineConfig : Pipeline Configuration Pipeline Template pipeline_config.groovy keywords { foo = \"bar\" } random_field = 11 Jenkinsfile println pipelineConfig . keywords . foo println pipelineConfig . random_field","title":"pipelineConfig"},{"location":"reference/autowired-variables/#jte","text":"The jte variable represents the Primitive Namespace . All loaded Pipeline Primitives for a Run can be accessed via the jte variable This is different from the pipelineConfig variable. The pipelineConfig variable gives a Map representation of the aggregated Pipeline Configuration whereas the jte variable allows access to the actual Pipeline Primitive objects . Example Usage of jte Assume there's a gradle and an npm library that both contribute a build() step. By default, loading would result in the pipeline failing. However, you can perform Step Overloading by setting jte.permissive_initialization to True . The jte would be used in this scenario to invoke the build() step from the gradle and npm libraries. Pipeline Configuration Pipeline Template pipeline_config.groovy jte { permissive_initialization = true } libraries { gradle npm } Jenkinsfile // invoke the gradle build step jte . libraries . gradle . build () // invoke the npm build step jte . libraries . npm . build () jte block vs jte variable You may have noticed in the example above that a jte{} block is used in the Pipeline Configuration and a jte variable is used in the Pipeline Template. These are different things. The jte{} block refers to framework-level feature flags as explained on the Pipeline Configuration schema page. The jte variable refers to the Pipeline Primitive Namespace variable.","title":"jte"},{"location":"reference/autowired-variables/#steps","text":"The steps variable doesn't technically come from JTE. It's a feature of all Jenkins Pipelines. The steps variable allows direct access to invoke Jenkins Pipeline DSL Steps. This variable is most commonly used when invoking Jenkins Pipeline DSL Steps from a Library Class or when Overloading Steps .","title":"steps"},{"location":"reference/autowired-variables/#autowired-library-step-variables","text":"The following variables are only accessible within Library Steps .","title":"Autowired Library Step Variables"},{"location":"reference/autowired-variables/#config","text":"The config variable represents the library configuration for the library that contributed the step as a Map . Example Usage of config Assume there's a gradle library that contributes a build() step. Pipeline Configuration Library build() Step pipeline_config.groovy libraries { gradle { version = \"6.3\" } } build.groovy void call (){ String gradleVersion = config . version }","title":"config"},{"location":"reference/autowired-variables/#hookcontext","text":"The hookContext variable provides information about the current step to Lifecycle Hooks . Property Type Description library String The library that contributed the step that triggered the Lifecycle Hook. Is null when the Lifecycle Hook wasn't triggered by a step. step String The name of the Library Step that triggered the Lifecycle Hook. Is null when the Lifecycle Hook wasn't triggered by a step. methodName String The name of the method within the step that was invoked to trigger the Lifecycle Hook. Is null when the Lifecycle Hook wasn't triggered by a step. exceptionThrown Boolean When the hook is triggered by a step, this refers to if the step triggering step threw an exception. When the hook is triggered by template completion, refers to if there is an uncaught exception that will fail the pipeline. Example hookContext usage The following example shows how to use the hookContext variable so that a Lifecycle Hook only triggers after the build() step from the gradle library. Lifecycle Hook Step @AfterStep ({ hookContext . library == \"gradle\" && hookContext . step == \"build\" }) void call (){ println \"running after the ${hookContext.library}'s ${hookContext.step} step\" }","title":"hookContext"},{"location":"reference/autowired-variables/#stagecontext","text":"The stageContext variable provides information about the current Stage . Property Type Description name String The name of the current Stage being executed. null if step isn't being executed as part of a Stage. args Map The named parameters provided to the Stage. An empty Map if no parameters provided. Example usage of stageContext The following example shows how to modify step behavior based upon Stage context. Pipeline Configuration NPM unit_test() Step pipeline_config.groovy libraries { npm // contributes unit_test() sonarqube // contributes static_code_analysis() } stages { ci { unit_test static_code_analysis } } unit_test.groovy void call (){ if ( stageContext . name == \"ci\" ){ println \"running as part of the ci Stage\" } }","title":"stageContext"},{"location":"reference/autowired-variables/#stepcontext","text":"The stepContext allows step introspection, such as querying the name of the library providing the step or the current name of the step. Property Type Description library String The name of the library that contributed the step name String The current name of the step. May differ from the basename of the step's groovy file if using Step Aliasing isAlias Boolean Is true when stepContext.name refers to an alias Example usage of stepContext Aliased Step Pipeline Template generic.groovy @StepAlias ([ \"build\" , \"unit_test\" ]) void call (){ println \"currently running as ${stepContext.name}\" } Jenkinsfile build () // prints \"currently running as build\" unit_test () // prints \"currently running as unit_test\"","title":"stepContext"},{"location":"reference/governance-tier/","text":"Governance Tier \u00b6 Governance Tiers are nodes in the Configuration Hierarchy . This page explain the options to configure a Governance Tier. Overview \u00b6 Governance Tiers can store three important things: a Pipeline Catalog , a Pipeline Configuration , and a list of Library Sources . The configuration for Library Sources stands alone. The configuration for the Pipeline Catalog and Pipeline Configuration are grouped together. Library Sources \u00b6 Governance Tiers can configure a list of Library Sources . When adding a Library Source, there will be a dropdown to determine the type of Library Provider. A Library Provider is a retrieval mechanism for libraries. JTE packages two types of Library Providers as part of the plugin: \"From SCM\" and \"From Plugin.\" Note Users will only see the \"From Plugin\" option available in the dropdown if a plugin has been installed that's capable of providing libraries. The ordering of Library Sources in the list impacts Library Resolution . From Remote Repository \u00b6 When configuring a Library Source that fetches from a remote repository, users can configure the type of source code repository as well as the configuration base directory. The configuration base directory is the path within the remote repository where the libraries can be found. Each subdirectory within the configuration base directory will be treated as a library. Info Refer to the Library Structure for how to organize files within a library directory. Pipeline Catalog \u00b6 Default Pipeline Template \u00b6 Governance Tier Type Location of Default Pipeline Template From a Remote Repository a Jenkinsfile at the root of the configuration base directory From the Jenkins Console a dedicated text box labeled 'Default Template' Named Pipeline Templates \u00b6 Governance Tier Type Location of Named Pipeline Templates From a Remote Repository groovy files within a pipeline_templates directory located at the root of the configuration base directory From the Jenkins Console a list of named templates can be added directly in the Jenkins Console Pipeline Configuration \u00b6 Governance Tier Type Location of the Pipeline Configuration From a Remote Repository a pipeline_config.groovy at the root of the configuration base directory From the Jenkins Console a text field labeled Pipeline Configuration","title":"Governance Tier"},{"location":"reference/governance-tier/#governance-tier","text":"Governance Tiers are nodes in the Configuration Hierarchy . This page explain the options to configure a Governance Tier.","title":"Governance Tier"},{"location":"reference/governance-tier/#overview","text":"Governance Tiers can store three important things: a Pipeline Catalog , a Pipeline Configuration , and a list of Library Sources . The configuration for Library Sources stands alone. The configuration for the Pipeline Catalog and Pipeline Configuration are grouped together.","title":"Overview"},{"location":"reference/governance-tier/#library-sources","text":"Governance Tiers can configure a list of Library Sources . When adding a Library Source, there will be a dropdown to determine the type of Library Provider. A Library Provider is a retrieval mechanism for libraries. JTE packages two types of Library Providers as part of the plugin: \"From SCM\" and \"From Plugin.\" Note Users will only see the \"From Plugin\" option available in the dropdown if a plugin has been installed that's capable of providing libraries. The ordering of Library Sources in the list impacts Library Resolution .","title":"Library Sources"},{"location":"reference/governance-tier/#from-remote-repository","text":"When configuring a Library Source that fetches from a remote repository, users can configure the type of source code repository as well as the configuration base directory. The configuration base directory is the path within the remote repository where the libraries can be found. Each subdirectory within the configuration base directory will be treated as a library. Info Refer to the Library Structure for how to organize files within a library directory.","title":"From Remote Repository"},{"location":"reference/governance-tier/#pipeline-catalog","text":"","title":"Pipeline Catalog"},{"location":"reference/governance-tier/#default-pipeline-template","text":"Governance Tier Type Location of Default Pipeline Template From a Remote Repository a Jenkinsfile at the root of the configuration base directory From the Jenkins Console a dedicated text box labeled 'Default Template'","title":"Default Pipeline Template"},{"location":"reference/governance-tier/#named-pipeline-templates","text":"Governance Tier Type Location of Named Pipeline Templates From a Remote Repository groovy files within a pipeline_templates directory located at the root of the configuration base directory From the Jenkins Console a list of named templates can be added directly in the Jenkins Console","title":"Named Pipeline Templates"},{"location":"reference/governance-tier/#pipeline-configuration","text":"Governance Tier Type Location of the Pipeline Configuration From a Remote Repository a pipeline_config.groovy at the root of the configuration base directory From the Jenkins Console a text field labeled Pipeline Configuration","title":"Pipeline Configuration"},{"location":"reference/library-configuration-schema/","text":"Library Configuration Schema \u00b6 This page outlines the schema for Library Configuration Files . Library Configuration Validation \u00b6 Schema \u00b6 library_config.groovy fields { // (1) required {} // (2) optional {} // (3) } The fields{} block is used to declare what properties are expected in a library configuration. The required{} block is used to declare required fields. The optional{} block is used to declare optional fields. Within the required and optional blocks, list the parameters the library supports in a parameterName = <Validation Type> format. Note If a library doesn't include a library configuration file, then users can supply arbitrary parameters to the library from the Pipeline Configuration. If a library does include a library configuration file, then users will only be able to supply parameters that are listed within the required and optional blocks. The presence of extraneous parameters will fail the build. Supported Validations \u00b6 The library configuration supports several different validation types for library parameters. Type Validation \u00b6 Type validation confirms that a library parameter is an instance of a particular type. The current options for data types to test for are: Boolean / boolean String Integer / int Double BigDecimal Float Number List ArrayList Type Validation Example Library Configuration File library_config.groovy fields { required { parameterA = String // (1) parameterB = Number // (2) parameterC = Boolean // (3) } optional { parameterD = String // (4) parameterE = Boolean // (5) parameterF = List // (6) parameterG = ArrayList // (7) } } ensures that parameterA was configured and is an instance of a String ensures that parameterB was configured and is an instance of a Number ensures that parameterC was configured and is an instance of a Boolean if parameterD was configured, ensures it's a String if parameterE was configured, ensures it's a Boolean if parameterF was configured, ensures it's a List if parameterG was configured, ensures it's an ArrayList Enum Validation \u00b6 Enum validation ensures that a library parameter value is one of the options defined by a list in the library configuration. Enum Validation Example Library Configuration File library_config.groovy fields { required { parameterA = [ \"a\" , \"b\" , 11 ] // (1) } } ensures that parameterA was configured and is set to either 'a', 'b', or 11 Regular Expression Validation \u00b6 Regular expression validation uses Groovy's match operator to determine if the parameter value is matched by the regular expression. Regular Expression Example Library Configuration File library_config.groovy fields { required { parameterA = ~ /^s.*/ // (1) } } ensures that parameterA starts with s Nested Parameters \u00b6 Library parameters can be arbitrarily nested within the Pipeline Configuration. For example, the following Pipeline Configuration would be valid to pass the example.nestedParameter parameter to a library named testing . Pipeline Configuration Library Configuration pipeline_config.groovy libraries { testing { example { nestedParameter = 11 } } } library_config.groovy fields { required { example { nestedParameter = Number } } } Tip To validate nested library parameters in the library configuration, nest their validation in the same structure within the required or optional blocks.","title":"Library Configuration Schema"},{"location":"reference/library-configuration-schema/#library-configuration-schema","text":"This page outlines the schema for Library Configuration Files .","title":"Library Configuration Schema"},{"location":"reference/library-configuration-schema/#library-configuration-validation","text":"","title":"Library Configuration Validation"},{"location":"reference/library-configuration-schema/#schema","text":"library_config.groovy fields { // (1) required {} // (2) optional {} // (3) } The fields{} block is used to declare what properties are expected in a library configuration. The required{} block is used to declare required fields. The optional{} block is used to declare optional fields. Within the required and optional blocks, list the parameters the library supports in a parameterName = <Validation Type> format. Note If a library doesn't include a library configuration file, then users can supply arbitrary parameters to the library from the Pipeline Configuration. If a library does include a library configuration file, then users will only be able to supply parameters that are listed within the required and optional blocks. The presence of extraneous parameters will fail the build.","title":"Schema"},{"location":"reference/library-configuration-schema/#supported-validations","text":"The library configuration supports several different validation types for library parameters.","title":"Supported Validations"},{"location":"reference/library-configuration-schema/#type-validation","text":"Type validation confirms that a library parameter is an instance of a particular type. The current options for data types to test for are: Boolean / boolean String Integer / int Double BigDecimal Float Number List ArrayList Type Validation Example Library Configuration File library_config.groovy fields { required { parameterA = String // (1) parameterB = Number // (2) parameterC = Boolean // (3) } optional { parameterD = String // (4) parameterE = Boolean // (5) parameterF = List // (6) parameterG = ArrayList // (7) } } ensures that parameterA was configured and is an instance of a String ensures that parameterB was configured and is an instance of a Number ensures that parameterC was configured and is an instance of a Boolean if parameterD was configured, ensures it's a String if parameterE was configured, ensures it's a Boolean if parameterF was configured, ensures it's a List if parameterG was configured, ensures it's an ArrayList","title":"Type Validation"},{"location":"reference/library-configuration-schema/#enum-validation","text":"Enum validation ensures that a library parameter value is one of the options defined by a list in the library configuration. Enum Validation Example Library Configuration File library_config.groovy fields { required { parameterA = [ \"a\" , \"b\" , 11 ] // (1) } } ensures that parameterA was configured and is set to either 'a', 'b', or 11","title":"Enum Validation"},{"location":"reference/library-configuration-schema/#regular-expression-validation","text":"Regular expression validation uses Groovy's match operator to determine if the parameter value is matched by the regular expression. Regular Expression Example Library Configuration File library_config.groovy fields { required { parameterA = ~ /^s.*/ // (1) } } ensures that parameterA starts with s","title":"Regular Expression Validation"},{"location":"reference/library-configuration-schema/#nested-parameters","text":"Library parameters can be arbitrarily nested within the Pipeline Configuration. For example, the following Pipeline Configuration would be valid to pass the example.nestedParameter parameter to a library named testing . Pipeline Configuration Library Configuration pipeline_config.groovy libraries { testing { example { nestedParameter = 11 } } } library_config.groovy fields { required { example { nestedParameter = Number } } } Tip To validate nested library parameters in the library configuration, nest their validation in the same structure within the required or optional blocks.","title":"Nested Parameters"},{"location":"reference/overview/","text":"Overview \u00b6 Reference pages are information oriented descriptions of JTE mechanics. Learn More \u00b6 Page Description Pipeline Configuration Schema Learn what can be configured in the Pipeline Configuration Autowired Variables Learn what variables JTE injects into various scopes to share context and data Library Configuration Schema Learn how to create a Library Configuration File","title":"Overview"},{"location":"reference/overview/#overview","text":"Reference pages are information oriented descriptions of JTE mechanics.","title":"Overview"},{"location":"reference/overview/#learn-more","text":"Page Description Pipeline Configuration Schema Learn what can be configured in the Pipeline Configuration Autowired Variables Learn what variables JTE injects into various scopes to share context and data Library Configuration Schema Learn how to create a Library Configuration File","title":"Learn More"},{"location":"reference/pipeline-configuration-schema/","text":"Pipeline Configuration Schema \u00b6 Overview \u00b6 Check out the Pipeline Configuration page for an explanation of the Pipeline Configuration's purpose and syntax. Root-Level Blocks \u00b6 jte \u00b6 The jte{} block of the Pipeline Configuration is reserved for fields that change framework behavior. key description type default allow_scm_jenkinsfile Determines whether a Jenkinsfile in a source code repository will be used when determining the Pipeline Template. Refer to Pipeline Template Selection for more information. Boolean True permissive_initialization Determine whether to fail the build during pipeline initialization if multiple Pipeline Primitives with conflicting names are loaded. Setting to True will allow multiple Pipeline Primitives with the same name to be loaded. Boolean False pipeline_template Specify a named template from the Pipeline Catalog to use. String null reverse_library_resolution Determine whether to reverse the order that Library Sources are queried for a library. Refer to Library Resolution for more information. Boolean False Example JTE Block pipeline_config.groovy jte { allow_scm_jenkinsfile = False permissive_initialization = True pipeline_template = \"my-named-template.groovy\" reverse_library_resolution = True } template_methods \u00b6 The template_methods{} block is used to define the names of steps to create a no-op placeholder for if a library doesn't provide the step's implementation. Refer to Placeholder Steps for more information. Example template_methods block pipeline_config.groovy template_methods { unit_test build deploy_to } libraries \u00b6 The libraries{} block determines which libraries to load. The block names within libraries must reference a library within a configured Library Source available to the job. Refer to the Library Development Overview for more information. Example libraries block pipeline_config.groovy libraries { library_A { param1 = \"foo\" param2 = \"bar\" ... } ... } stages \u00b6 The stages{} block is used to define Stages . Example stages block pipeline_config.groovy stages { stage_name { step1 step2 ... } ... } application_environments \u00b6 The application_environments{} block is used to define Application Environments . Example application_environments block pipeline_config.groovy application_environments { dev { long_name = \"Development\" } test prod } keywords \u00b6 The keywords{} block is used to define Keywords . Example keywords block pipeline_config.groovy keywords { main = ~ /^[Mm](ain|aster)$/ globals { foo = \"bar\" } }","title":"Pipeline Configuration Schema"},{"location":"reference/pipeline-configuration-schema/#pipeline-configuration-schema","text":"","title":"Pipeline Configuration Schema"},{"location":"reference/pipeline-configuration-schema/#overview","text":"Check out the Pipeline Configuration page for an explanation of the Pipeline Configuration's purpose and syntax.","title":"Overview"},{"location":"reference/pipeline-configuration-schema/#root-level-blocks","text":"","title":"Root-Level Blocks"},{"location":"reference/pipeline-configuration-schema/#jte","text":"The jte{} block of the Pipeline Configuration is reserved for fields that change framework behavior. key description type default allow_scm_jenkinsfile Determines whether a Jenkinsfile in a source code repository will be used when determining the Pipeline Template. Refer to Pipeline Template Selection for more information. Boolean True permissive_initialization Determine whether to fail the build during pipeline initialization if multiple Pipeline Primitives with conflicting names are loaded. Setting to True will allow multiple Pipeline Primitives with the same name to be loaded. Boolean False pipeline_template Specify a named template from the Pipeline Catalog to use. String null reverse_library_resolution Determine whether to reverse the order that Library Sources are queried for a library. Refer to Library Resolution for more information. Boolean False Example JTE Block pipeline_config.groovy jte { allow_scm_jenkinsfile = False permissive_initialization = True pipeline_template = \"my-named-template.groovy\" reverse_library_resolution = True }","title":"jte"},{"location":"reference/pipeline-configuration-schema/#template_methods","text":"The template_methods{} block is used to define the names of steps to create a no-op placeholder for if a library doesn't provide the step's implementation. Refer to Placeholder Steps for more information. Example template_methods block pipeline_config.groovy template_methods { unit_test build deploy_to }","title":"template_methods"},{"location":"reference/pipeline-configuration-schema/#libraries","text":"The libraries{} block determines which libraries to load. The block names within libraries must reference a library within a configured Library Source available to the job. Refer to the Library Development Overview for more information. Example libraries block pipeline_config.groovy libraries { library_A { param1 = \"foo\" param2 = \"bar\" ... } ... }","title":"libraries"},{"location":"reference/pipeline-configuration-schema/#stages","text":"The stages{} block is used to define Stages . Example stages block pipeline_config.groovy stages { stage_name { step1 step2 ... } ... }","title":"stages"},{"location":"reference/pipeline-configuration-schema/#application_environments","text":"The application_environments{} block is used to define Application Environments . Example application_environments block pipeline_config.groovy application_environments { dev { long_name = \"Development\" } test prod }","title":"application_environments"},{"location":"reference/pipeline-configuration-schema/#keywords","text":"The keywords{} block is used to define Keywords . Example keywords block pipeline_config.groovy keywords { main = ~ /^[Mm](ain|aster)$/ globals { foo = \"bar\" } }","title":"keywords"},{"location":"snippets/hookContext/","text":"Property Type Description library String The library that contributed the step that triggered the Lifecycle Hook. Is null when the Lifecycle Hook wasn't triggered by a step. step String The name of the Library Step that triggered the Lifecycle Hook. Is null when the Lifecycle Hook wasn't triggered by a step. methodName String The name of the method within the step that was invoked to trigger the Lifecycle Hook. Is null when the Lifecycle Hook wasn't triggered by a step. exceptionThrown Boolean When the hook is triggered by a step, this refers to if the step triggering step threw an exception. When the hook is triggered by template completion, refers to if there is an uncaught exception that will fail the pipeline.","title":"hookContext"},{"location":"tutorials/","text":"Overview \u00b6 Tutorials are learning oriented lessons to teach users about JTE. The best way to learn a technology is to use it. These Learning Labs have been designed to give you hands-on experience with the Jenkins Templating Engine. Learning Labs Overview \u00b6 Lab Description Local Development In this lab, we\u2019ll be deploying a local Jenkins instance using Docker and installing the Jenkins Templating Engine. JTE: The Basics Explore the basic concepts of the Jenkins Templating Engine and different ways to use it. JTE: Learn the Primitives Explore the different Pipeline Primitives JTE exposes, called primitives, to aid in pipeline template authoring. JTE: Advanced Features Level up your JTE skills by learning its advanced features like the default step implementation and how to leverage step lifecycle hooks.","title":"Overview"},{"location":"tutorials/#overview","text":"Tutorials are learning oriented lessons to teach users about JTE. The best way to learn a technology is to use it. These Learning Labs have been designed to give you hands-on experience with the Jenkins Templating Engine.","title":"Overview"},{"location":"tutorials/#learning-labs-overview","text":"Lab Description Local Development In this lab, we\u2019ll be deploying a local Jenkins instance using Docker and installing the Jenkins Templating Engine. JTE: The Basics Explore the basic concepts of the Jenkins Templating Engine and different ways to use it. JTE: Learn the Primitives Explore the different Pipeline Primitives JTE exposes, called primitives, to aid in pipeline template authoring. JTE: Advanced Features Level up your JTE skills by learning its advanced features like the default step implementation and how to leverage step lifecycle hooks.","title":"Learning Labs Overview"},{"location":"tutorials/jte-advanced-features/","text":"JTE: Advanced Features \u00b6 In this lab we're going to highlight some of JTE's more advanced features. What You'll Learn \u00b6 The Default Step Implementation : How to create steps on the fly in the Pipeline Configuration. Pipeline Lifecycle Hooks : How to register steps to be dynamically invoked in response to pipeline events. Multi-Method Steps : How to create utility wrappers by defining multiple methods in a step.","title":"JTE: Advanced Features"},{"location":"tutorials/jte-advanced-features/#jte-advanced-features","text":"In this lab we're going to highlight some of JTE's more advanced features.","title":"JTE: Advanced Features"},{"location":"tutorials/jte-advanced-features/#what-youll-learn","text":"The Default Step Implementation : How to create steps on the fly in the Pipeline Configuration. Pipeline Lifecycle Hooks : How to register steps to be dynamically invoked in response to pipeline events. Multi-Method Steps : How to create utility wrappers by defining multiple methods in a step.","title":"What You'll Learn"},{"location":"tutorials/jte-advanced-features/1-prerequisites/","text":"Prerequisites \u00b6 Jenkins Instance \u00b6 A Jenkins instance will be required for this lab. If you don't have one available to you, we would recommend going through Local Development Learning Lab to deploy a local Jenkins instance through Docker. JTE: Pipeline Primitives \u00b6 This lab is the third and final lab specifically dedicated to the Jenkins Templating Engine's functionality. Please be sure to have completed the Pipeline Primitives Learning Lab before attempting this one, as we will be picking up where that lab leaves off.","title":"Prerequisites"},{"location":"tutorials/jte-advanced-features/1-prerequisites/#prerequisites","text":"","title":"Prerequisites"},{"location":"tutorials/jte-advanced-features/1-prerequisites/#jenkins-instance","text":"A Jenkins instance will be required for this lab. If you don't have one available to you, we would recommend going through Local Development Learning Lab to deploy a local Jenkins instance through Docker.","title":"Jenkins Instance"},{"location":"tutorials/jte-advanced-features/1-prerequisites/#jte-pipeline-primitives","text":"This lab is the third and final lab specifically dedicated to the Jenkins Templating Engine's functionality. Please be sure to have completed the Pipeline Primitives Learning Lab before attempting this one, as we will be picking up where that lab leaves off.","title":"JTE: Pipeline Primitives"},{"location":"tutorials/jte-advanced-features/2-default-step-implementation/","text":"Default Step Implementation \u00b6 There are a large number of use cases where a step contributed by a library is going to do the same thing: Check out the source code Run some CLI/shell command Archive some files that were generated by the command To support fast development of steps that follow this pattern, we can use a default step implementation. What is the Default Step Implementation? \u00b6 The default step implementation allows you to define a step from the Pipeline Configuration that specifies a container image as the runtime environment for the step, a shell command or script to execute, and then a stash to be created in order to store files generated. Benefits \u00b6 The default step implementation allows a quick-and-dirty way to prototype a step on the fly without needing to create a library. Using container images for pipeline runtime dependencies is an excellent way to build a DevSecOps pipeline that is loosely coupled to the underlying infrastructure. Setbacks \u00b6 The default step implementation can be a little too powerful if you're striving for a tightly governed DevSecOps pipeline. This feature should be exposed to end users with care, as it exposes a lot of the functionality to the teams that would be using the template. This feature hard-codes a particular functionality from the Pipeline Configuration and doesn't have the same configuration flexibility that a library would have. Create a Step \u00b6 In the same Pipeline Job we were using during the JTE: Pipeline Primitives lab, let's add a new step called unit_test() . Update the Pipeline Configuration \u00b6 In your single-job , append to your Pipeline Configuration from the keywords lab: Pipeline Configuration steps { unit_test { stage = \"Unit Test\" image = \"maven\" command = \"mvn -v\" } } Important Steps implemented through the default step implementation are defined in the steps block of the Pipeline Configuration. Root level keys within the steps block become the name of the step that can be invoked from the Pipeline Template. The step above creates a unit_test step that can be invoked from the Pipeline Template that executes the command mvn -v inside the maven:latest container image from Docker Hub. It would also make sense to update the continuous_integration Stage that was created to include the unit_test step, so the full Pipeline Configuration in your single-job should look like (note we changed requiresApproval = false to run without manual intervention): Pipeline Configuration libraries { maven sonarqube ansible } stages { continuous_integration { unit_test build static_code_analysis } } application_environments { dev { ip_addresses = [ \"0.0.0.1\" , \"0.0.0.2\" ] } prod { long_name = \"Production\" ip_addresses = [ \"0.0.1.1\" , \"0.0.1.2\" , \"0.0.1.3\" , \"0.0.1.4\" ] } } keywords { requiresApproval = false } steps { unit_test { stage = \"Unit Test\" image = \"maven\" command = \"mvn -v\" } } Run the Pipeline \u00b6 Run the pipeline. From the job's main page, click Build Now in the left-hand navigation menu. View the build logs and you should see output similar to: Started by user admin [JTE] Pipeline Configuration Modifications (show) [JTE] Obtained Pipeline Template from job configuration [JTE] Loading Library maven (show) [JTE] Loading Library sonarqube (show) [JTE] Loading Library ansible (show) [JTE] Creating step unit_test from the default step implementation. [JTE] Template Primitives are overwriting Jenkins steps with the following names: (show) [Pipeline] Start of Pipeline [JTE][Stage - continuous_integration] [JTE][Step - null/unit_test.call()] [Pipeline] stage [Pipeline] { (Unit Test) [Pipeline] node Running on Jenkins in /var/jenkins_home/workspace/single-job [Pipeline] { [Pipeline] isUnix [Pipeline] withEnv [Pipeline] { [Pipeline] sh + docker inspect -f . maven . [Pipeline] } [Pipeline] // withEnv [Pipeline] withDockerContainer Jenkins seems to be running inside container f8a61ccd04d1fd2e436dc0ccbc3f5ad59cd95b6a736420fb3ef808b9da5b7dec $ docker run -t -d -u 0:0 -w /var/jenkins_home/workspace/single-job --volumes-from f8a61ccd04d1fd2e436dc0ccbc3f5ad59cd95b6a736420fb3ef808b9da5b7dec -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** maven cat $ docker top e18bdb071ce2403ffa413da3c58bd1b8cb4711ba9a861b080e0727364aa62e62 -eo pid,comm [Pipeline] { [Pipeline] unstash [Pipeline] sh + mvn -v Apache Maven 3.8.7 (b89d5959fcde851dcb1c8946a785a163f14e1e29) Maven home: /usr/share/maven Java version: 17.0.5, vendor: Eclipse Adoptium, runtime: /opt/java/openjdk Default locale: en_US, platform encoding: UTF-8 OS name: \"linux\", version: \"5.10.104-linuxkit\", arch: \"amd64\", family: \"unix\" [Pipeline] } $ docker stop --time=1 e18bdb071ce2403ffa413da3c58bd1b8cb4711ba9a861b080e0727364aa62e62 $ docker rm -f --volumes e18bdb071ce2403ffa413da3c58bd1b8cb4711ba9a861b080e0727364aa62e62 [Pipeline] // withDockerContainer [Pipeline] } [Pipeline] // node [Pipeline] } [Pipeline] // stage [JTE][Step - maven/build.call()] [Pipeline] stage [Pipeline] { (Maven: Build) [Pipeline] echo build from the maven library [Pipeline] } [Pipeline] // stage [JTE][Step - sonarqube/static_code_analysis.call()] [Pipeline] stage [Pipeline] { (SonarQube: Static Code Analysis) [Pipeline] echo static code analysis from the sonarqube library [Pipeline] } [Pipeline] // stage [JTE][Step - ansible/deploy_to.call(ApplicationEnvironment)] [Pipeline] stage [Pipeline] { (Deploy to: dev) [Pipeline] echo Performing a deployment through Ansible.. [Pipeline] echo Deploying to 0.0.0.1 [Pipeline] echo Deploying to 0.0.0.2 [Pipeline] } [Pipeline] // stage [JTE][Step - ansible/deploy_to.call(ApplicationEnvironment)] [Pipeline] stage [Pipeline] { (Deploy to: Production) [Pipeline] echo Performing a deployment through Ansible.. [Pipeline] echo Deploying to 0.0.1.1 [Pipeline] echo Deploying to 0.0.1.2 [Pipeline] echo Deploying to 0.0.1.3 [Pipeline] echo Deploying to 0.0.1.4 [Pipeline] } [Pipeline] // stage [Pipeline] End of Pipeline Finished: SUCCESS When reading the lines, notice: [JTE] Creating step unit_test from the default step implementation. at the beginning of the build. JTE saw a step was defined in the Pipeline Configuration and constructed the unit_test step on the fly for use in the Pipeline Template. The logs pertaining to the unit_test step were: [JTE][Step - null/unit_test.call()] [Pipeline] stage [Pipeline] { (Unit Test) [Pipeline] node Running on Jenkins in /var/jenkins_home/workspace/single-job [Pipeline] { [Pipeline] isUnix [Pipeline] withEnv [Pipeline] { [Pipeline] sh + docker inspect -f . maven . [Pipeline] } [Pipeline] // withEnv [Pipeline] withDockerContainer Jenkins seems to be running inside container f8a61ccd04d1fd2e436dc0ccbc3f5ad59cd95b6a736420fb3ef808b9da5b7dec $ docker run -t -d -u 0:0 -w /var/jenkins_home/workspace/single-job --volumes-from f8a61ccd04d1fd2e436dc0ccbc3f5ad59cd95b6a736420fb3ef808b9da5b7dec -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** maven cat $ docker top e18bdb071ce2403ffa413da3c58bd1b8cb4711ba9a861b080e0727364aa62e62 -eo pid,comm [Pipeline] { [Pipeline] unstash [Pipeline] sh + mvn -v Apache Maven 3.8.7 (b89d5959fcde851dcb1c8946a785a163f14e1e29) Maven home: /usr/share/maven Java version: 17.0.5, vendor: Eclipse Adoptium, runtime: /opt/java/openjdk Default locale: en_US, platform encoding: UTF-8 OS name: \"linux\", version: \"5.10.104-linuxkit\", arch: \"amd64\", family: \"unix\" [Pipeline] } $ docker stop --time=1 e18bdb071ce2403ffa413da3c58bd1b8cb4711ba9a861b080e0727364aa62e62 $ docker rm -f --volumes e18bdb071ce2403ffa413da3c58bd1b8cb4711ba9a861b080e0727364aa62e62 [Pipeline] // withDockerContainer [Pipeline] } [Pipeline] // node [Pipeline] } [Pipeline] // stage You can see JTE announcing it's about to execute a step called unit_test that was constructed via the default step implementation here: [JTE][Step - null/unit_test.call()] . When the step was executed, it checked if the maven step was available locally and pulled the image if not. Within the container image, it then ran mvn -v and the Maven version was printed to the build log.","title":"Default Step Implementation"},{"location":"tutorials/jte-advanced-features/2-default-step-implementation/#default-step-implementation","text":"There are a large number of use cases where a step contributed by a library is going to do the same thing: Check out the source code Run some CLI/shell command Archive some files that were generated by the command To support fast development of steps that follow this pattern, we can use a default step implementation.","title":"Default Step Implementation"},{"location":"tutorials/jte-advanced-features/2-default-step-implementation/#what-is-the-default-step-implementation","text":"The default step implementation allows you to define a step from the Pipeline Configuration that specifies a container image as the runtime environment for the step, a shell command or script to execute, and then a stash to be created in order to store files generated.","title":"What is the Default Step Implementation?"},{"location":"tutorials/jte-advanced-features/2-default-step-implementation/#benefits","text":"The default step implementation allows a quick-and-dirty way to prototype a step on the fly without needing to create a library. Using container images for pipeline runtime dependencies is an excellent way to build a DevSecOps pipeline that is loosely coupled to the underlying infrastructure.","title":"Benefits"},{"location":"tutorials/jte-advanced-features/2-default-step-implementation/#setbacks","text":"The default step implementation can be a little too powerful if you're striving for a tightly governed DevSecOps pipeline. This feature should be exposed to end users with care, as it exposes a lot of the functionality to the teams that would be using the template. This feature hard-codes a particular functionality from the Pipeline Configuration and doesn't have the same configuration flexibility that a library would have.","title":"Setbacks"},{"location":"tutorials/jte-advanced-features/2-default-step-implementation/#create-a-step","text":"In the same Pipeline Job we were using during the JTE: Pipeline Primitives lab, let's add a new step called unit_test() .","title":"Create a Step"},{"location":"tutorials/jte-advanced-features/2-default-step-implementation/#update-the-pipeline-configuration","text":"In your single-job , append to your Pipeline Configuration from the keywords lab: Pipeline Configuration steps { unit_test { stage = \"Unit Test\" image = \"maven\" command = \"mvn -v\" } } Important Steps implemented through the default step implementation are defined in the steps block of the Pipeline Configuration. Root level keys within the steps block become the name of the step that can be invoked from the Pipeline Template. The step above creates a unit_test step that can be invoked from the Pipeline Template that executes the command mvn -v inside the maven:latest container image from Docker Hub. It would also make sense to update the continuous_integration Stage that was created to include the unit_test step, so the full Pipeline Configuration in your single-job should look like (note we changed requiresApproval = false to run without manual intervention): Pipeline Configuration libraries { maven sonarqube ansible } stages { continuous_integration { unit_test build static_code_analysis } } application_environments { dev { ip_addresses = [ \"0.0.0.1\" , \"0.0.0.2\" ] } prod { long_name = \"Production\" ip_addresses = [ \"0.0.1.1\" , \"0.0.1.2\" , \"0.0.1.3\" , \"0.0.1.4\" ] } } keywords { requiresApproval = false } steps { unit_test { stage = \"Unit Test\" image = \"maven\" command = \"mvn -v\" } }","title":"Update the Pipeline Configuration"},{"location":"tutorials/jte-advanced-features/2-default-step-implementation/#run-the-pipeline","text":"Run the pipeline. From the job's main page, click Build Now in the left-hand navigation menu. View the build logs and you should see output similar to: Started by user admin [JTE] Pipeline Configuration Modifications (show) [JTE] Obtained Pipeline Template from job configuration [JTE] Loading Library maven (show) [JTE] Loading Library sonarqube (show) [JTE] Loading Library ansible (show) [JTE] Creating step unit_test from the default step implementation. [JTE] Template Primitives are overwriting Jenkins steps with the following names: (show) [Pipeline] Start of Pipeline [JTE][Stage - continuous_integration] [JTE][Step - null/unit_test.call()] [Pipeline] stage [Pipeline] { (Unit Test) [Pipeline] node Running on Jenkins in /var/jenkins_home/workspace/single-job [Pipeline] { [Pipeline] isUnix [Pipeline] withEnv [Pipeline] { [Pipeline] sh + docker inspect -f . maven . [Pipeline] } [Pipeline] // withEnv [Pipeline] withDockerContainer Jenkins seems to be running inside container f8a61ccd04d1fd2e436dc0ccbc3f5ad59cd95b6a736420fb3ef808b9da5b7dec $ docker run -t -d -u 0:0 -w /var/jenkins_home/workspace/single-job --volumes-from f8a61ccd04d1fd2e436dc0ccbc3f5ad59cd95b6a736420fb3ef808b9da5b7dec -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** maven cat $ docker top e18bdb071ce2403ffa413da3c58bd1b8cb4711ba9a861b080e0727364aa62e62 -eo pid,comm [Pipeline] { [Pipeline] unstash [Pipeline] sh + mvn -v Apache Maven 3.8.7 (b89d5959fcde851dcb1c8946a785a163f14e1e29) Maven home: /usr/share/maven Java version: 17.0.5, vendor: Eclipse Adoptium, runtime: /opt/java/openjdk Default locale: en_US, platform encoding: UTF-8 OS name: \"linux\", version: \"5.10.104-linuxkit\", arch: \"amd64\", family: \"unix\" [Pipeline] } $ docker stop --time=1 e18bdb071ce2403ffa413da3c58bd1b8cb4711ba9a861b080e0727364aa62e62 $ docker rm -f --volumes e18bdb071ce2403ffa413da3c58bd1b8cb4711ba9a861b080e0727364aa62e62 [Pipeline] // withDockerContainer [Pipeline] } [Pipeline] // node [Pipeline] } [Pipeline] // stage [JTE][Step - maven/build.call()] [Pipeline] stage [Pipeline] { (Maven: Build) [Pipeline] echo build from the maven library [Pipeline] } [Pipeline] // stage [JTE][Step - sonarqube/static_code_analysis.call()] [Pipeline] stage [Pipeline] { (SonarQube: Static Code Analysis) [Pipeline] echo static code analysis from the sonarqube library [Pipeline] } [Pipeline] // stage [JTE][Step - ansible/deploy_to.call(ApplicationEnvironment)] [Pipeline] stage [Pipeline] { (Deploy to: dev) [Pipeline] echo Performing a deployment through Ansible.. [Pipeline] echo Deploying to 0.0.0.1 [Pipeline] echo Deploying to 0.0.0.2 [Pipeline] } [Pipeline] // stage [JTE][Step - ansible/deploy_to.call(ApplicationEnvironment)] [Pipeline] stage [Pipeline] { (Deploy to: Production) [Pipeline] echo Performing a deployment through Ansible.. [Pipeline] echo Deploying to 0.0.1.1 [Pipeline] echo Deploying to 0.0.1.2 [Pipeline] echo Deploying to 0.0.1.3 [Pipeline] echo Deploying to 0.0.1.4 [Pipeline] } [Pipeline] // stage [Pipeline] End of Pipeline Finished: SUCCESS When reading the lines, notice: [JTE] Creating step unit_test from the default step implementation. at the beginning of the build. JTE saw a step was defined in the Pipeline Configuration and constructed the unit_test step on the fly for use in the Pipeline Template. The logs pertaining to the unit_test step were: [JTE][Step - null/unit_test.call()] [Pipeline] stage [Pipeline] { (Unit Test) [Pipeline] node Running on Jenkins in /var/jenkins_home/workspace/single-job [Pipeline] { [Pipeline] isUnix [Pipeline] withEnv [Pipeline] { [Pipeline] sh + docker inspect -f . maven . [Pipeline] } [Pipeline] // withEnv [Pipeline] withDockerContainer Jenkins seems to be running inside container f8a61ccd04d1fd2e436dc0ccbc3f5ad59cd95b6a736420fb3ef808b9da5b7dec $ docker run -t -d -u 0:0 -w /var/jenkins_home/workspace/single-job --volumes-from f8a61ccd04d1fd2e436dc0ccbc3f5ad59cd95b6a736420fb3ef808b9da5b7dec -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** maven cat $ docker top e18bdb071ce2403ffa413da3c58bd1b8cb4711ba9a861b080e0727364aa62e62 -eo pid,comm [Pipeline] { [Pipeline] unstash [Pipeline] sh + mvn -v Apache Maven 3.8.7 (b89d5959fcde851dcb1c8946a785a163f14e1e29) Maven home: /usr/share/maven Java version: 17.0.5, vendor: Eclipse Adoptium, runtime: /opt/java/openjdk Default locale: en_US, platform encoding: UTF-8 OS name: \"linux\", version: \"5.10.104-linuxkit\", arch: \"amd64\", family: \"unix\" [Pipeline] } $ docker stop --time=1 e18bdb071ce2403ffa413da3c58bd1b8cb4711ba9a861b080e0727364aa62e62 $ docker rm -f --volumes e18bdb071ce2403ffa413da3c58bd1b8cb4711ba9a861b080e0727364aa62e62 [Pipeline] // withDockerContainer [Pipeline] } [Pipeline] // node [Pipeline] } [Pipeline] // stage You can see JTE announcing it's about to execute a step called unit_test that was constructed via the default step implementation here: [JTE][Step - null/unit_test.call()] . When the step was executed, it checked if the maven step was available locally and pulled the image if not. Within the container image, it then ran mvn -v and the Maven version was printed to the build log.","title":"Run the Pipeline"},{"location":"tutorials/jte-advanced-features/3-pipeline-lifecycle-hooks/","text":"Pipeline Lifecycle Hooks \u00b6 There can be interdependent functionalities that present a challenge to the Jenkins Templating Engine. For example, let's say we wanted to introduce Splunk(link) monitoring by sending events as part of the pipeline. How do you: Maintain a clean, easy-to-read Pipeline Template? Maintain a separation of duties between libraries as to not hard-code a Splunk integration into every Library Step? It would be great if there was a seamless way to inject functionality in response to different phases of the pipeline without having to tightly couple that functionality to existing Library Steps or Pipeline Templates... There is! The Jenkins Templating Engine has a neat feature we call Pipeline Lifecycle Hooks that were made for just these situations. We'll walk through the Splunk use case to demonstrate this functionality. Note Read the entire Pipeline Lifecycle Hook documentation . Create a Splunk Library \u00b6 Methods defined within steps are able to register themselves to correspond to specific lifecycle events via annotations. As such, these steps are typically not invoked directly by other steps or from the Pipeline Template. Because of this, the name of the step is inconsequential but can't conflict with other step names that are loaded. Therefore, we typically recommend following a naming convention of prepending the step name with the library name. Important It doesn't matter what you call steps that only contain Pipeline Lifecycle Hook-annotated methods. But to avoid collisions of everyone naming their hook steps beforeStep.groovy - we recommend <libraryName>_<action> as we'll demonstrate in this lab. Notify of Pipeline Start \u00b6 Within the same Library Source repo you created during JTE: The Basics, create a step called splunk_pipeline_start.groovy within libraries/splunk : ./libraries/splunk/steps/splunk_pipeline_start.groovy @Init void call () { println \"Splunk: beginning of the pipeline!\" } Breaking down this step, the @Init registers the call method defined in this step to be invoked at the beginning of the pipeline. Update the Pipeline Configuration \u00b6 In the single-job again, update the Pipeline Configuration to load the splunk library we just created. Pipeline Configuration libraries { maven sonarqube ansible splunk } That's it! Just by loading the library, JTE will be able to find the methods within steps annotated with a Pipeline Lifecycle Hook. Run the job and you should see output in the logs similar to: [JTE][@Init - splunk/splunk_pipeline_start.call] [Pipeline] echo Splunk: beginning of the pipeline! Add Before and After Step Execution Hooks \u00b6 Let's add some hooks that inject themselves both before and after each step is executed in the pipeline. Add an additional step file to your Splunk library: ./libraries/splunk/steps/splunk_step_watcher.groovy @BeforeStep void before () { println \"Splunk: running before the ${hookContext.library} library's ${hookContext.step} step\" } @AfterStep void after () { println \"Splunk: running after the ${hookContext.library} library's ${hookContext.step} step\" } Take notice of the JTE-native hookContext variable. This variable provides runtime context for the hook based on the \"event\" that is triggering the hook to run. Note Make sure you push your code to the main/master branch before running. Here, we're defining two different methods in a single step. In the next section we'll talk about this in more detail. For right now, the important piece is that the method's have the @BeforeStep and @AfterStep annotations. Rerunning the pipeline, we can now see these hooks get executed before and after (Maven in this snippet): [JTE][@BeforeStep - splunk/splunk_step_watcher.before] [Pipeline] echo Splunk: running before the maven library's build step [JTE][Step - maven/build.call()] [Pipeline] stage [Pipeline] { (Maven: Build) [Pipeline] echo build from the maven library [Pipeline] } [Pipeline] // stage [JTE][@AfterStep - splunk/splunk_step_watcher.after] [Pipeline] echo Splunk: running after the maven library's build step Notify of End of Pipeline Execution \u00b6 Let's try out one more hook to get executed when the pipeline has finished, create a third step file: ./libraries/splunk/splunk_pipeline_end.groovy @CleanUp void call ( context ) { println \"Splunk: end of the pipeline!\" } ```` Push your code , then run the pipeline again and you should see logs at the end similar to: ``` text [ JTE ][ @CleanUp - splunk / splunk_pipeline_end . call ] [ Pipeline ] echo Splunk: end of the pipeline ! [ Pipeline ] End of Pipeline Finished: SUCCESS Restricting Hook Execution \u00b6 What if we only wanted the @AfterStep hook to be executed after the static_code_analysis step? Pipeline Lifecycle Hook annotations accept a Closure parameter. This Closure will be executed, and if the return of the Closure is non-false the step will be executed. Important Remember: Groovy has implicit return statements. The last statement made becomes the return object by default. We call this functionality Conditional Hook Execution . Update the @AfterStep Annotation \u00b6 Let's see it in action. Update the line with @AfterStep to: ./libraries/splunk/steps/splunk_step_watcher.groovy @BeforeStep void before () { println \"Splunk: running before the ${hookContext.library} library's ${hookContext.step} step\" } @AfterStep ({ hookContext . step . equals ( \"static_code_analysis\" ) }) void after () { println \"Splunk: running after the ${hookContext.library} library's ${hookContext.step} step\" } Push your code, re-run the pipeline and notice that now, the hook has been restricted to only run after the desired step. Important When the Closure parameter is invoked, it will have access to the hookContext variable as well as the library configuration that is stored via the config variable. Taking It A Step Further \u00b6 It would be even better if we could externalize the configuration of exactly which steps the @AfterStep hook should be triggered. To do this, update the @AfterStep annotation again to be: ./libraries/splunk/steps/splunk_step_watcher.groovy @BeforeStep void before () { println \"Splunk: running before the ${hookContext.library} library's ${hookContext.step} step\" } @AfterStep ({ hookContext . step in config . afterSteps }) void after () { println \"Splunk: running after the ${hookContext.library} library's ${hookContext.step} step\" } Now, we can conditionally execute the hook by checking if the name of the step that was just executed is in an array called afterSteps defined as part of the splunk library in the Pipeline Configuration! Update the splunk portion of the single-job Pipeline Configuration to: Pipeline Configuration libraries { maven sonarqube ansible splunk { afterSteps = [ \"static_code_analysis\" , \"unit_test\" ] } } Run the pipeline again and notice that the hook was only executed after the steps defined in the Pipeline Configuration. Note Conditional Execution Closure Parameters can be passed to any Pipeline Lifecycle Hook annotation. As long as the Closure returns a non-false value, the hook will be invoked. Important Remember to read through the Pipeline Lifecycle Hook documentation to see all the annotations available.","title":"Pipeline Lifecycle Hooks"},{"location":"tutorials/jte-advanced-features/3-pipeline-lifecycle-hooks/#pipeline-lifecycle-hooks","text":"There can be interdependent functionalities that present a challenge to the Jenkins Templating Engine. For example, let's say we wanted to introduce Splunk(link) monitoring by sending events as part of the pipeline. How do you: Maintain a clean, easy-to-read Pipeline Template? Maintain a separation of duties between libraries as to not hard-code a Splunk integration into every Library Step? It would be great if there was a seamless way to inject functionality in response to different phases of the pipeline without having to tightly couple that functionality to existing Library Steps or Pipeline Templates... There is! The Jenkins Templating Engine has a neat feature we call Pipeline Lifecycle Hooks that were made for just these situations. We'll walk through the Splunk use case to demonstrate this functionality. Note Read the entire Pipeline Lifecycle Hook documentation .","title":"Pipeline Lifecycle Hooks"},{"location":"tutorials/jte-advanced-features/3-pipeline-lifecycle-hooks/#create-a-splunk-library","text":"Methods defined within steps are able to register themselves to correspond to specific lifecycle events via annotations. As such, these steps are typically not invoked directly by other steps or from the Pipeline Template. Because of this, the name of the step is inconsequential but can't conflict with other step names that are loaded. Therefore, we typically recommend following a naming convention of prepending the step name with the library name. Important It doesn't matter what you call steps that only contain Pipeline Lifecycle Hook-annotated methods. But to avoid collisions of everyone naming their hook steps beforeStep.groovy - we recommend <libraryName>_<action> as we'll demonstrate in this lab.","title":"Create a Splunk Library"},{"location":"tutorials/jte-advanced-features/3-pipeline-lifecycle-hooks/#notify-of-pipeline-start","text":"Within the same Library Source repo you created during JTE: The Basics, create a step called splunk_pipeline_start.groovy within libraries/splunk : ./libraries/splunk/steps/splunk_pipeline_start.groovy @Init void call () { println \"Splunk: beginning of the pipeline!\" } Breaking down this step, the @Init registers the call method defined in this step to be invoked at the beginning of the pipeline.","title":"Notify of Pipeline Start"},{"location":"tutorials/jte-advanced-features/3-pipeline-lifecycle-hooks/#update-the-pipeline-configuration","text":"In the single-job again, update the Pipeline Configuration to load the splunk library we just created. Pipeline Configuration libraries { maven sonarqube ansible splunk } That's it! Just by loading the library, JTE will be able to find the methods within steps annotated with a Pipeline Lifecycle Hook. Run the job and you should see output in the logs similar to: [JTE][@Init - splunk/splunk_pipeline_start.call] [Pipeline] echo Splunk: beginning of the pipeline!","title":"Update the Pipeline Configuration"},{"location":"tutorials/jte-advanced-features/3-pipeline-lifecycle-hooks/#add-before-and-after-step-execution-hooks","text":"Let's add some hooks that inject themselves both before and after each step is executed in the pipeline. Add an additional step file to your Splunk library: ./libraries/splunk/steps/splunk_step_watcher.groovy @BeforeStep void before () { println \"Splunk: running before the ${hookContext.library} library's ${hookContext.step} step\" } @AfterStep void after () { println \"Splunk: running after the ${hookContext.library} library's ${hookContext.step} step\" } Take notice of the JTE-native hookContext variable. This variable provides runtime context for the hook based on the \"event\" that is triggering the hook to run. Note Make sure you push your code to the main/master branch before running. Here, we're defining two different methods in a single step. In the next section we'll talk about this in more detail. For right now, the important piece is that the method's have the @BeforeStep and @AfterStep annotations. Rerunning the pipeline, we can now see these hooks get executed before and after (Maven in this snippet): [JTE][@BeforeStep - splunk/splunk_step_watcher.before] [Pipeline] echo Splunk: running before the maven library's build step [JTE][Step - maven/build.call()] [Pipeline] stage [Pipeline] { (Maven: Build) [Pipeline] echo build from the maven library [Pipeline] } [Pipeline] // stage [JTE][@AfterStep - splunk/splunk_step_watcher.after] [Pipeline] echo Splunk: running after the maven library's build step","title":"Add Before and After Step Execution Hooks"},{"location":"tutorials/jte-advanced-features/3-pipeline-lifecycle-hooks/#notify-of-end-of-pipeline-execution","text":"Let's try out one more hook to get executed when the pipeline has finished, create a third step file: ./libraries/splunk/splunk_pipeline_end.groovy @CleanUp void call ( context ) { println \"Splunk: end of the pipeline!\" } ```` Push your code , then run the pipeline again and you should see logs at the end similar to: ``` text [ JTE ][ @CleanUp - splunk / splunk_pipeline_end . call ] [ Pipeline ] echo Splunk: end of the pipeline ! [ Pipeline ] End of Pipeline Finished: SUCCESS","title":"Notify of End of Pipeline Execution"},{"location":"tutorials/jte-advanced-features/3-pipeline-lifecycle-hooks/#restricting-hook-execution","text":"What if we only wanted the @AfterStep hook to be executed after the static_code_analysis step? Pipeline Lifecycle Hook annotations accept a Closure parameter. This Closure will be executed, and if the return of the Closure is non-false the step will be executed. Important Remember: Groovy has implicit return statements. The last statement made becomes the return object by default. We call this functionality Conditional Hook Execution .","title":"Restricting Hook Execution"},{"location":"tutorials/jte-advanced-features/3-pipeline-lifecycle-hooks/#update-the-afterstep-annotation","text":"Let's see it in action. Update the line with @AfterStep to: ./libraries/splunk/steps/splunk_step_watcher.groovy @BeforeStep void before () { println \"Splunk: running before the ${hookContext.library} library's ${hookContext.step} step\" } @AfterStep ({ hookContext . step . equals ( \"static_code_analysis\" ) }) void after () { println \"Splunk: running after the ${hookContext.library} library's ${hookContext.step} step\" } Push your code, re-run the pipeline and notice that now, the hook has been restricted to only run after the desired step. Important When the Closure parameter is invoked, it will have access to the hookContext variable as well as the library configuration that is stored via the config variable.","title":"Update the @AfterStep Annotation"},{"location":"tutorials/jte-advanced-features/3-pipeline-lifecycle-hooks/#taking-it-a-step-further","text":"It would be even better if we could externalize the configuration of exactly which steps the @AfterStep hook should be triggered. To do this, update the @AfterStep annotation again to be: ./libraries/splunk/steps/splunk_step_watcher.groovy @BeforeStep void before () { println \"Splunk: running before the ${hookContext.library} library's ${hookContext.step} step\" } @AfterStep ({ hookContext . step in config . afterSteps }) void after () { println \"Splunk: running after the ${hookContext.library} library's ${hookContext.step} step\" } Now, we can conditionally execute the hook by checking if the name of the step that was just executed is in an array called afterSteps defined as part of the splunk library in the Pipeline Configuration! Update the splunk portion of the single-job Pipeline Configuration to: Pipeline Configuration libraries { maven sonarqube ansible splunk { afterSteps = [ \"static_code_analysis\" , \"unit_test\" ] } } Run the pipeline again and notice that the hook was only executed after the steps defined in the Pipeline Configuration. Note Conditional Execution Closure Parameters can be passed to any Pipeline Lifecycle Hook annotation. As long as the Closure returns a non-false value, the hook will be invoked. Important Remember to read through the Pipeline Lifecycle Hook documentation to see all the annotations available.","title":"Taking It A Step Further"},{"location":"tutorials/jte-advanced-features/4-multimethod-steps/","text":"Multi-Method Steps \u00b6 While learning about Pipeline Lifecycle Hooks, we created a step that: Implemented multiple methods. Implemented a step without a call method. In this section, we're going to dive into multi-method steps in a little more detail. Important Have you ever wondered why Library Steps create a method named call ? This is because, in the Groovy scripting language, something() gets translated to something.call() . If we understand this concept, then it would make sense that we could define other methods within our steps and invoke them by their full name. When to use Multi-Method Steps \u00b6 The most common use case for defining multiple methods inside one step file is when you're creating some utility functionality. To demonstrate this, let's create a mock git utility that can add , commit , and push . Create the Git Library \u00b6 In the same Pipeline Configuration Repository we used for JTE: The Basics, create a git library. Because we're creating a git utility, add a file called git.groovy in libraries/git in your Library Sources repo with the contents: ./libraries/git/git.groovy /* takes an arraylist of files to pass to git add */ void add ( ArrayList files ) { println \"git add ${files.join(\" \")}\" } /* takes a string commit message to pass to git commit */ void commit ( String message ) { println \"git commit -m ${message}\" } /* performs the git push */ void push () { println \"git push\" } In this example, we're creating a step that serves as a utility wrapper. These are typically not invoked directly by Pipeline Templates but rather consumed by other steps. That is why it is okay, in this case, to accept input parameters for these methods. We will be invoking this functionality directly from the Pipeline Template to demonstrate its usefulness. Important The file structure for your Pipeline Configuration libraries directory should now be: . \u251c\u2500\u2500 libraries \u251c\u2500\u2500 ansible \u2502 \u2514\u2500\u2500 steps \u2502 \u2514\u2500\u2500 deploy_to.groovy \u251c\u2500\u2500 git \u2502 \u2514\u2500\u2500 steps \u2502 \u2514\u2500\u2500 git.groovy \u251c\u2500\u2500 gradle \u2502 \u2514\u2500\u2500 steps \u2502 \u2514\u2500\u2500 build.groovy \u251c\u2500\u2500 maven \u2502 \u2514\u2500\u2500 steps \u2502 \u2514\u2500\u2500 build.groovy \u251c\u2500\u2500 sonarqube \u2502 \u2514\u2500\u2500 steps \u2502 \u2514\u2500\u2500 static_code_analysis.groovy \u2514\u2500\u2500 splunk \u2514\u2500\u2500 steps \u251c\u2500\u2500 splunk_pipeline_end.groovy \u251c\u2500\u2500 splunk_pipeline_start.groovy \u2514\u2500\u2500 splunk_step_watcher.groovy Update the Pipeline Configuration \u00b6 Update the Pipeline Configuration in your single-job to load the git library. The libraries portion of the Pipeline Configuration should now be: Pipeline Configuration libraries { maven sonarqube ansible splunk { afterSteps = [ \"static_code_analysis\" , \"unit_test\" ] } git } Use the New Git Utility \u00b6 Prepend to the existing Jenkinsfile/Template for single-job (before continuous_integration() ): Jenkinsfile git . add ([ \"a\" , \"b\" , \"c\" ]) git . commit \"my commit message\" git . push () Important When invoking a non-call method defined within a step, you do so by <step_name>.<method_name>(<arguments>) . Run the Pipeline \u00b6 Run the single-job again and you will see logs similar to: Started by user admin [JTE] Pipeline Configuration Modifications (show) [JTE] Obtained Pipeline Template from job configuration [JTE] Loading Library maven (show) [JTE] Loading Library sonarqube (show) [JTE] Loading Library ansible (show) [JTE] Loading Library splunk (show) [JTE] Loading Library git (show) [JTE] Creating step unit_test from the default step implementation. [JTE] Template Primitives are overwriting Jenkins steps with the following names: (show) [Pipeline] Start of Pipeline [JTE][@Init - splunk/splunk_pipeline_start.call] [Pipeline] echo Splunk: beginning of the pipeline! [JTE][@BeforeStep - splunk/splunk_step_watcher.before] [Pipeline] echo Splunk: running before the git library's git step [JTE][Step - git/git.add(ArrayList)] [Pipeline] echo git add a b c [JTE][@BeforeStep - splunk/splunk_step_watcher.before] [Pipeline] echo Splunk: running before the git library's git step [JTE][Step - git/git.commit(String)] [Pipeline] echo git commit -m my commit message [JTE][@BeforeStep - splunk/splunk_step_watcher.before] [Pipeline] echo Splunk: running before the git library's git step [JTE][Step - git/git.push()] [Pipeline] echo git push [JTE][Stage - continuous_integration] [JTE][@BeforeStep - splunk/splunk_step_watcher.before] [Pipeline] echo Splunk: running before the null library's unit_test step [JTE][Step - null/unit_test.call()] [Pipeline] stage [Pipeline] { (Unit Test) [Pipeline] node Running on Jenkins in /var/jenkins_home/workspace/single-job [Pipeline] { [Pipeline] isUnix [Pipeline] withEnv [Pipeline] { [Pipeline] sh + docker inspect -f . maven . [Pipeline] } [Pipeline] // withEnv [Pipeline] withDockerContainer Jenkins seems to be running inside container f8a61ccd04d1fd2e436dc0ccbc3f5ad59cd95b6a736420fb3ef808b9da5b7dec $ docker run -t -d -u 0:0 -w /var/jenkins_home/workspace/single-job --volumes-from f8a61ccd04d1fd2e436dc0ccbc3f5ad59cd95b6a736420fb3ef808b9da5b7dec -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** maven cat $ docker top 8a28e4d78d0bb343f652e5420c00767b50a4e87f12f075f420ecfd5ce73a32d3 -eo pid,comm [Pipeline] { [Pipeline] unstash [Pipeline] sh + mvn -v Apache Maven 3.8.7 (b89d5959fcde851dcb1c8946a785a163f14e1e29) Maven home: /usr/share/maven Java version: 17.0.5, vendor: Eclipse Adoptium, runtime: /opt/java/openjdk Default locale: en_US, platform encoding: UTF-8 OS name: \"linux\", version: \"5.10.104-linuxkit\", arch: \"amd64\", family: \"unix\" [Pipeline] } $ docker stop --time=1 8a28e4d78d0bb343f652e5420c00767b50a4e87f12f075f420ecfd5ce73a32d3 $ docker rm -f --volumes 8a28e4d78d0bb343f652e5420c00767b50a4e87f12f075f420ecfd5ce73a32d3 [Pipeline] // withDockerContainer [Pipeline] } [Pipeline] // node [Pipeline] } [Pipeline] // stage [JTE][@AfterStep - splunk/splunk_step_watcher.after] [Pipeline] echo Splunk: running after the null library's unit_test step [JTE][@BeforeStep - splunk/splunk_step_watcher.before] [Pipeline] echo Splunk: running before the maven library's build step [JTE][Step - maven/build.call()] [Pipeline] stage [Pipeline] { (Maven: Build) [Pipeline] echo build from the maven library [Pipeline] } [Pipeline] // stage [JTE][@BeforeStep - splunk/splunk_step_watcher.before] [Pipeline] echo Splunk: running before the sonarqube library's static_code_analysis step [JTE][Step - sonarqube/static_code_analysis.call()] [Pipeline] stage [Pipeline] { (SonarQube: Static Code Analysis) [Pipeline] echo static code analysis from the sonarqube library [Pipeline] } [Pipeline] // stage [JTE][@AfterStep - splunk/splunk_step_watcher.after] [Pipeline] echo Splunk: running after the sonarqube library's static_code_analysis step [JTE][@BeforeStep - splunk/splunk_step_watcher.before] [Pipeline] echo Splunk: running before the ansible library's deploy_to step [JTE][Step - ansible/deploy_to.call(ApplicationEnvironment)] [Pipeline] stage [Pipeline] { (Deploy to: dev) [Pipeline] echo Performing a deployment through Ansible.. [Pipeline] echo Deploying to 0.0.0.1 [Pipeline] echo Deploying to 0.0.0.2 [Pipeline] } [Pipeline] // stage [JTE][@BeforeStep - splunk/splunk_step_watcher.before] [Pipeline] echo Splunk: running before the ansible library's deploy_to step [JTE][Step - ansible/deploy_to.call(ApplicationEnvironment)] [Pipeline] stage [Pipeline] { (Deploy to: Production) [Pipeline] echo Performing a deployment through Ansible.. [Pipeline] echo Deploying to 0.0.1.1 [Pipeline] echo Deploying to 0.0.1.2 [Pipeline] echo Deploying to 0.0.1.3 [Pipeline] echo Deploying to 0.0.1.4 [Pipeline] } [Pipeline] // stage [JTE][@CleanUp - splunk/splunk_pipeline_end.call] [Pipeline] echo Splunk: end of the pipeline! [Pipeline] End of Pipeline Finished: SUCCESS You learned in this lesson that we can call steps in a very programmatic way from our template, this opens the door to new and creative ways to create a governed pipeline that allows flexibility for different step implementations.","title":"Multi-Method Steps"},{"location":"tutorials/jte-advanced-features/4-multimethod-steps/#multi-method-steps","text":"While learning about Pipeline Lifecycle Hooks, we created a step that: Implemented multiple methods. Implemented a step without a call method. In this section, we're going to dive into multi-method steps in a little more detail. Important Have you ever wondered why Library Steps create a method named call ? This is because, in the Groovy scripting language, something() gets translated to something.call() . If we understand this concept, then it would make sense that we could define other methods within our steps and invoke them by their full name.","title":"Multi-Method Steps"},{"location":"tutorials/jte-advanced-features/4-multimethod-steps/#when-to-use-multi-method-steps","text":"The most common use case for defining multiple methods inside one step file is when you're creating some utility functionality. To demonstrate this, let's create a mock git utility that can add , commit , and push .","title":"When to use Multi-Method Steps"},{"location":"tutorials/jte-advanced-features/4-multimethod-steps/#create-the-git-library","text":"In the same Pipeline Configuration Repository we used for JTE: The Basics, create a git library. Because we're creating a git utility, add a file called git.groovy in libraries/git in your Library Sources repo with the contents: ./libraries/git/git.groovy /* takes an arraylist of files to pass to git add */ void add ( ArrayList files ) { println \"git add ${files.join(\" \")}\" } /* takes a string commit message to pass to git commit */ void commit ( String message ) { println \"git commit -m ${message}\" } /* performs the git push */ void push () { println \"git push\" } In this example, we're creating a step that serves as a utility wrapper. These are typically not invoked directly by Pipeline Templates but rather consumed by other steps. That is why it is okay, in this case, to accept input parameters for these methods. We will be invoking this functionality directly from the Pipeline Template to demonstrate its usefulness. Important The file structure for your Pipeline Configuration libraries directory should now be: . \u251c\u2500\u2500 libraries \u251c\u2500\u2500 ansible \u2502 \u2514\u2500\u2500 steps \u2502 \u2514\u2500\u2500 deploy_to.groovy \u251c\u2500\u2500 git \u2502 \u2514\u2500\u2500 steps \u2502 \u2514\u2500\u2500 git.groovy \u251c\u2500\u2500 gradle \u2502 \u2514\u2500\u2500 steps \u2502 \u2514\u2500\u2500 build.groovy \u251c\u2500\u2500 maven \u2502 \u2514\u2500\u2500 steps \u2502 \u2514\u2500\u2500 build.groovy \u251c\u2500\u2500 sonarqube \u2502 \u2514\u2500\u2500 steps \u2502 \u2514\u2500\u2500 static_code_analysis.groovy \u2514\u2500\u2500 splunk \u2514\u2500\u2500 steps \u251c\u2500\u2500 splunk_pipeline_end.groovy \u251c\u2500\u2500 splunk_pipeline_start.groovy \u2514\u2500\u2500 splunk_step_watcher.groovy","title":"Create the Git Library"},{"location":"tutorials/jte-advanced-features/4-multimethod-steps/#update-the-pipeline-configuration","text":"Update the Pipeline Configuration in your single-job to load the git library. The libraries portion of the Pipeline Configuration should now be: Pipeline Configuration libraries { maven sonarqube ansible splunk { afterSteps = [ \"static_code_analysis\" , \"unit_test\" ] } git }","title":"Update the Pipeline Configuration"},{"location":"tutorials/jte-advanced-features/4-multimethod-steps/#use-the-new-git-utility","text":"Prepend to the existing Jenkinsfile/Template for single-job (before continuous_integration() ): Jenkinsfile git . add ([ \"a\" , \"b\" , \"c\" ]) git . commit \"my commit message\" git . push () Important When invoking a non-call method defined within a step, you do so by <step_name>.<method_name>(<arguments>) .","title":"Use the New Git Utility"},{"location":"tutorials/jte-advanced-features/4-multimethod-steps/#run-the-pipeline","text":"Run the single-job again and you will see logs similar to: Started by user admin [JTE] Pipeline Configuration Modifications (show) [JTE] Obtained Pipeline Template from job configuration [JTE] Loading Library maven (show) [JTE] Loading Library sonarqube (show) [JTE] Loading Library ansible (show) [JTE] Loading Library splunk (show) [JTE] Loading Library git (show) [JTE] Creating step unit_test from the default step implementation. [JTE] Template Primitives are overwriting Jenkins steps with the following names: (show) [Pipeline] Start of Pipeline [JTE][@Init - splunk/splunk_pipeline_start.call] [Pipeline] echo Splunk: beginning of the pipeline! [JTE][@BeforeStep - splunk/splunk_step_watcher.before] [Pipeline] echo Splunk: running before the git library's git step [JTE][Step - git/git.add(ArrayList)] [Pipeline] echo git add a b c [JTE][@BeforeStep - splunk/splunk_step_watcher.before] [Pipeline] echo Splunk: running before the git library's git step [JTE][Step - git/git.commit(String)] [Pipeline] echo git commit -m my commit message [JTE][@BeforeStep - splunk/splunk_step_watcher.before] [Pipeline] echo Splunk: running before the git library's git step [JTE][Step - git/git.push()] [Pipeline] echo git push [JTE][Stage - continuous_integration] [JTE][@BeforeStep - splunk/splunk_step_watcher.before] [Pipeline] echo Splunk: running before the null library's unit_test step [JTE][Step - null/unit_test.call()] [Pipeline] stage [Pipeline] { (Unit Test) [Pipeline] node Running on Jenkins in /var/jenkins_home/workspace/single-job [Pipeline] { [Pipeline] isUnix [Pipeline] withEnv [Pipeline] { [Pipeline] sh + docker inspect -f . maven . [Pipeline] } [Pipeline] // withEnv [Pipeline] withDockerContainer Jenkins seems to be running inside container f8a61ccd04d1fd2e436dc0ccbc3f5ad59cd95b6a736420fb3ef808b9da5b7dec $ docker run -t -d -u 0:0 -w /var/jenkins_home/workspace/single-job --volumes-from f8a61ccd04d1fd2e436dc0ccbc3f5ad59cd95b6a736420fb3ef808b9da5b7dec -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** -e ******** maven cat $ docker top 8a28e4d78d0bb343f652e5420c00767b50a4e87f12f075f420ecfd5ce73a32d3 -eo pid,comm [Pipeline] { [Pipeline] unstash [Pipeline] sh + mvn -v Apache Maven 3.8.7 (b89d5959fcde851dcb1c8946a785a163f14e1e29) Maven home: /usr/share/maven Java version: 17.0.5, vendor: Eclipse Adoptium, runtime: /opt/java/openjdk Default locale: en_US, platform encoding: UTF-8 OS name: \"linux\", version: \"5.10.104-linuxkit\", arch: \"amd64\", family: \"unix\" [Pipeline] } $ docker stop --time=1 8a28e4d78d0bb343f652e5420c00767b50a4e87f12f075f420ecfd5ce73a32d3 $ docker rm -f --volumes 8a28e4d78d0bb343f652e5420c00767b50a4e87f12f075f420ecfd5ce73a32d3 [Pipeline] // withDockerContainer [Pipeline] } [Pipeline] // node [Pipeline] } [Pipeline] // stage [JTE][@AfterStep - splunk/splunk_step_watcher.after] [Pipeline] echo Splunk: running after the null library's unit_test step [JTE][@BeforeStep - splunk/splunk_step_watcher.before] [Pipeline] echo Splunk: running before the maven library's build step [JTE][Step - maven/build.call()] [Pipeline] stage [Pipeline] { (Maven: Build) [Pipeline] echo build from the maven library [Pipeline] } [Pipeline] // stage [JTE][@BeforeStep - splunk/splunk_step_watcher.before] [Pipeline] echo Splunk: running before the sonarqube library's static_code_analysis step [JTE][Step - sonarqube/static_code_analysis.call()] [Pipeline] stage [Pipeline] { (SonarQube: Static Code Analysis) [Pipeline] echo static code analysis from the sonarqube library [Pipeline] } [Pipeline] // stage [JTE][@AfterStep - splunk/splunk_step_watcher.after] [Pipeline] echo Splunk: running after the sonarqube library's static_code_analysis step [JTE][@BeforeStep - splunk/splunk_step_watcher.before] [Pipeline] echo Splunk: running before the ansible library's deploy_to step [JTE][Step - ansible/deploy_to.call(ApplicationEnvironment)] [Pipeline] stage [Pipeline] { (Deploy to: dev) [Pipeline] echo Performing a deployment through Ansible.. [Pipeline] echo Deploying to 0.0.0.1 [Pipeline] echo Deploying to 0.0.0.2 [Pipeline] } [Pipeline] // stage [JTE][@BeforeStep - splunk/splunk_step_watcher.before] [Pipeline] echo Splunk: running before the ansible library's deploy_to step [JTE][Step - ansible/deploy_to.call(ApplicationEnvironment)] [Pipeline] stage [Pipeline] { (Deploy to: Production) [Pipeline] echo Performing a deployment through Ansible.. [Pipeline] echo Deploying to 0.0.1.1 [Pipeline] echo Deploying to 0.0.1.2 [Pipeline] echo Deploying to 0.0.1.3 [Pipeline] echo Deploying to 0.0.1.4 [Pipeline] } [Pipeline] // stage [JTE][@CleanUp - splunk/splunk_pipeline_end.call] [Pipeline] echo Splunk: end of the pipeline! [Pipeline] End of Pipeline Finished: SUCCESS You learned in this lesson that we can call steps in a very programmatic way from our template, this opens the door to new and creative ways to create a governed pipeline that allows flexibility for different step implementations.","title":"Run the Pipeline"},{"location":"tutorials/jte-advanced-features/5-summary/","text":"Summary \u00b6 Default Step Implementation \u00b6 The Default Step Implementation is used for creating steps on the fly from the Pipeline Configuration. It works by leveraging container images as pipeline runtime dependencies, executing a command or script inside the container image, and then allowing you to stash artifacts generated by the step. Care should be taken when exposing this functionality to users. Pipeline Lifecycle Hooks \u00b6 The Jenkins Templating Engine exposes a feature called Pipeline Lifecycle Hooks that allows methods defined within Library Steps to register themselves to be automatically executed in response to events from the pipeline. This includes steps that take place before and after certain triggers, and a \"cleanup\" step that executes after the main pipeline is finished. These annotations accept Closure parameters for conditional hook execution to dynamically determine if a hook should be invoked. Multi-Method Library Steps \u00b6 Steps contributed by libraries can define more than one method within the step. This is most commonly used when creating steps representing utilities. These steps can be called programmatically for more complex implementations! Where Now? \u00b6 You've completed all of the Jenkins Templating Engine Learning Labs! Consult the JTE Docs for more information and be sure to reach out on GitHub if there is something wrong with the contents of this course.","title":"Summary"},{"location":"tutorials/jte-advanced-features/5-summary/#summary","text":"","title":"Summary"},{"location":"tutorials/jte-advanced-features/5-summary/#default-step-implementation","text":"The Default Step Implementation is used for creating steps on the fly from the Pipeline Configuration. It works by leveraging container images as pipeline runtime dependencies, executing a command or script inside the container image, and then allowing you to stash artifacts generated by the step. Care should be taken when exposing this functionality to users.","title":"Default Step Implementation"},{"location":"tutorials/jte-advanced-features/5-summary/#pipeline-lifecycle-hooks","text":"The Jenkins Templating Engine exposes a feature called Pipeline Lifecycle Hooks that allows methods defined within Library Steps to register themselves to be automatically executed in response to events from the pipeline. This includes steps that take place before and after certain triggers, and a \"cleanup\" step that executes after the main pipeline is finished. These annotations accept Closure parameters for conditional hook execution to dynamically determine if a hook should be invoked.","title":"Pipeline Lifecycle Hooks"},{"location":"tutorials/jte-advanced-features/5-summary/#multi-method-library-steps","text":"Steps contributed by libraries can define more than one method within the step. This is most commonly used when creating steps representing utilities. These steps can be called programmatically for more complex implementations!","title":"Multi-Method Library Steps"},{"location":"tutorials/jte-advanced-features/5-summary/#where-now","text":"You've completed all of the Jenkins Templating Engine Learning Labs! Consult the JTE Docs for more information and be sure to reach out on GitHub if there is something wrong with the contents of this course.","title":"Where Now?"},{"location":"tutorials/jte-primitives/","text":"Jenkins Templating Engine: Pipeline Primitives \u00b6 This lab covers each of the JTE Pipeline Primitives and describes their usage. What Are Primitives \u00b6 When writing reusable Pipeline Templates through the Jenkins Templating Engine, one of the primary goals is to keep the template as easy-to-read as humanly possible. JTE Pipeline Primitives are defined in your Pipeline Configuration file and primarily serve to aid in this endeavor by providing \"syntactic sugar\" during the runtime execution of the template. \"Sugar\" in that it has no functional benefit but makes the code easier to read, and therefore maintain. Throughout this lab, we will cover each of the JTE Pipeline Primitives and describe when to use them and how they make writing templates even easier. Important Pipeline Primitives are defined in the aggregated Pipeline Configuration and make writing and reading Pipeline Templates easier. What You'll Learn \u00b6 The Application Environment Primitive The Stage Primitive The Keyword Primitive","title":"Jenkins Templating Engine: Pipeline Primitives"},{"location":"tutorials/jte-primitives/#jenkins-templating-engine-pipeline-primitives","text":"This lab covers each of the JTE Pipeline Primitives and describes their usage.","title":"Jenkins Templating Engine: Pipeline Primitives"},{"location":"tutorials/jte-primitives/#what-are-primitives","text":"When writing reusable Pipeline Templates through the Jenkins Templating Engine, one of the primary goals is to keep the template as easy-to-read as humanly possible. JTE Pipeline Primitives are defined in your Pipeline Configuration file and primarily serve to aid in this endeavor by providing \"syntactic sugar\" during the runtime execution of the template. \"Sugar\" in that it has no functional benefit but makes the code easier to read, and therefore maintain. Throughout this lab, we will cover each of the JTE Pipeline Primitives and describe when to use them and how they make writing templates even easier. Important Pipeline Primitives are defined in the aggregated Pipeline Configuration and make writing and reading Pipeline Templates easier.","title":"What Are Primitives"},{"location":"tutorials/jte-primitives/#what-youll-learn","text":"The Application Environment Primitive The Stage Primitive The Keyword Primitive","title":"What You'll Learn"},{"location":"tutorials/jte-primitives/1-prerequisites/","text":"Prerequisites \u00b6 Jenkins Instance \u00b6 A Jenkins instance will be required for this lab. If you don't have one available to you, we recommend going through the Local Development Learning Lab to deploy a local Jenkins instance through Docker. JTE: The Basics \u00b6 This lab continues to build upon our knowledge of the Jenkins Templating Engine so first completing the Basics Learning Lab would be very helpful. In this lab we will assume you're using the same Pipeline Configuration repository used during The Basics lab and that it is already configured as a Library Source in the Global Governance Tier. Remove the Global Governance Tier's Pipeline Configuration \u00b6 For the purposes of this lab, we will only be using the Pipeline Job type. In JTE: The Basics, we created a Pipeline Configuration to the Global Governance Tier that applies to every job on the Jenkins instance. If this is still configured, remove it. From the Jenkins homepage, Click Manage Jenkins in the left-hand navigation menu. Under System Configuration click Configure System . Scroll down to the Jenkins Templating Engine configuration section. Remove any text in the Configuration Base Directory text box. Under Pipeline Configuration drop-down menu, select None . Click Save . Note There should still be a global Library Source configured. Leave it as-is.","title":"Prerequisites"},{"location":"tutorials/jte-primitives/1-prerequisites/#prerequisites","text":"","title":"Prerequisites"},{"location":"tutorials/jte-primitives/1-prerequisites/#jenkins-instance","text":"A Jenkins instance will be required for this lab. If you don't have one available to you, we recommend going through the Local Development Learning Lab to deploy a local Jenkins instance through Docker.","title":"Jenkins Instance"},{"location":"tutorials/jte-primitives/1-prerequisites/#jte-the-basics","text":"This lab continues to build upon our knowledge of the Jenkins Templating Engine so first completing the Basics Learning Lab would be very helpful. In this lab we will assume you're using the same Pipeline Configuration repository used during The Basics lab and that it is already configured as a Library Source in the Global Governance Tier.","title":"JTE: The Basics"},{"location":"tutorials/jte-primitives/1-prerequisites/#remove-the-global-governance-tiers-pipeline-configuration","text":"For the purposes of this lab, we will only be using the Pipeline Job type. In JTE: The Basics, we created a Pipeline Configuration to the Global Governance Tier that applies to every job on the Jenkins instance. If this is still configured, remove it. From the Jenkins homepage, Click Manage Jenkins in the left-hand navigation menu. Under System Configuration click Configure System . Scroll down to the Jenkins Templating Engine configuration section. Remove any text in the Configuration Base Directory text box. Under Pipeline Configuration drop-down menu, select None . Click Save . Note There should still be a global Library Source configured. Leave it as-is.","title":"Remove the Global Governance Tier's Pipeline Configuration"},{"location":"tutorials/jte-primitives/2-pipeline-job/","text":"Create a Pipeline Job \u00b6 Before we get started, we'll need to create a Pipeline Job in Jenkins that we can play around with. Feel free to reuse the Pipeline Job created during JTE: The Basics or follow the same instructions to create a new job for this lab. When you're finished, you should have: 1. A pipeline template (Jenkinsfile text box in your job) that reads: build () static_code_analysis () 2. A Pipeline Configuration in your job of: libraries { maven sonarqube } Note If you're reusing the same Pipeline Job, your Pipeline Configuration may specify the gradle instead of the maven library. Either will do for the purposes of this lab, but switch it back to maven if it's different. This Pipeline Job is going to be the playground where we learn about the different Pipeline Primitives for the rest of this lab.","title":"Create a Pipeline Job"},{"location":"tutorials/jte-primitives/2-pipeline-job/#create-a-pipeline-job","text":"Before we get started, we'll need to create a Pipeline Job in Jenkins that we can play around with. Feel free to reuse the Pipeline Job created during JTE: The Basics or follow the same instructions to create a new job for this lab. When you're finished, you should have: 1. A pipeline template (Jenkinsfile text box in your job) that reads: build () static_code_analysis () 2. A Pipeline Configuration in your job of: libraries { maven sonarqube } Note If you're reusing the same Pipeline Job, your Pipeline Configuration may specify the gradle instead of the maven library. Either will do for the purposes of this lab, but switch it back to maven if it's different. This Pipeline Job is going to be the playground where we learn about the different Pipeline Primitives for the rest of this lab.","title":"Create a Pipeline Job"},{"location":"tutorials/jte-primitives/3-stages/","text":"Stages \u00b6 What is a Stage? \u00b6 Stages are a mechanism for chaining multiple steps together to be invoked through an aliased method name. As Pipeline Templates mature in complexity and grow to represent branching strategies for application development teams, it's likely that you would want to call the same series of steps multiple times in the template. To minimize repeating ourselves in Pipeline Templates, the Stage primitive was created to address this use case. Note View the Stage documentation here. Define and Use a Stage \u00b6 A very common Stage to create is a Continuous Integration stage. Note In general, Continuous Integration represents a series of fast-feedback verification steps that help developers quickly determine if the changes made to the code have broken anything obvious. It's common to run steps like building an artifact, running unit tests, and performing static code analysis on every commit in a source code repository and then once again in the Pull Request job to main/master to verify the merged result is still functional. Our current Pipeline Template specifies to run the build() and static_code_analysis() steps. Let's group these together into a continuous_integration() stage. Define the Stage in the Pipeline Configuration \u00b6 In your single-job , update the Pipeline Configuration to: Pipeline Configuration libraries { maven sonarqube } stages { continuous_integration { build static_code_analysis } } Important All Stages will be defined in the stages block of the Pipeline Configuration. Root level keys within this block, in this case continuous_integration , will become invoked methods within the Pipeline Template. The lines within the continuous_integration block outline which steps will be chained together when the stage is invoked. Update the Pipeline Template \u00b6 With the continuous_integration stage defined, we can update the Pipeline Template (Jenkinsfile, in your single-job ) to make use of it. Update the Pipeline Template to: Pipeline Template continuous_integration () Then click Save . Run the Pipeline \u00b6 From the Pipeline Job's main page, click Build Now in the left-hand navigation menu. When viewing the build logs (click Build Number and then Console Output ), you should see output similar to: Started by user admin [JTE] Pipeline Configuration Modifications (show) [JTE] Obtained Pipeline Template from job configuration [JTE] Loading Library maven (show) [JTE] Loading Library sonarqube (show) [JTE] Template Primitives are overwriting Jenkins steps with the following names: (show) [Pipeline] Start of Pipeline [JTE][Stage - continuous_integration] [JTE][Step - maven/build.call()] [Pipeline] stage [Pipeline] { (Maven: Build) [Pipeline] echo build from the maven library [Pipeline] } [Pipeline] // stage [JTE][Step - sonarqube/static_code_analysis.call()] [Pipeline] stage [Pipeline] { (SonarQube: Static Code Analysis) [Pipeline] echo static code analysis from the sonarqube library [Pipeline] } [Pipeline] // stage [Pipeline] End of Pipeline Finished: SUCCESS Important When reading the build logs of a JTE job, you can identify the start of stages by looking for [JTE] [Stage - *] in the output. In this case, the log output was [JTE] [Stage - continuous_integration] indicating a Stage called continuous_integration` is about to be executed.","title":"Stages"},{"location":"tutorials/jte-primitives/3-stages/#stages","text":"","title":"Stages"},{"location":"tutorials/jte-primitives/3-stages/#what-is-a-stage","text":"Stages are a mechanism for chaining multiple steps together to be invoked through an aliased method name. As Pipeline Templates mature in complexity and grow to represent branching strategies for application development teams, it's likely that you would want to call the same series of steps multiple times in the template. To minimize repeating ourselves in Pipeline Templates, the Stage primitive was created to address this use case. Note View the Stage documentation here.","title":"What is a Stage?"},{"location":"tutorials/jte-primitives/3-stages/#define-and-use-a-stage","text":"A very common Stage to create is a Continuous Integration stage. Note In general, Continuous Integration represents a series of fast-feedback verification steps that help developers quickly determine if the changes made to the code have broken anything obvious. It's common to run steps like building an artifact, running unit tests, and performing static code analysis on every commit in a source code repository and then once again in the Pull Request job to main/master to verify the merged result is still functional. Our current Pipeline Template specifies to run the build() and static_code_analysis() steps. Let's group these together into a continuous_integration() stage.","title":"Define and Use a Stage"},{"location":"tutorials/jte-primitives/3-stages/#define-the-stage-in-the-pipeline-configuration","text":"In your single-job , update the Pipeline Configuration to: Pipeline Configuration libraries { maven sonarqube } stages { continuous_integration { build static_code_analysis } } Important All Stages will be defined in the stages block of the Pipeline Configuration. Root level keys within this block, in this case continuous_integration , will become invoked methods within the Pipeline Template. The lines within the continuous_integration block outline which steps will be chained together when the stage is invoked.","title":"Define the Stage in the Pipeline Configuration"},{"location":"tutorials/jte-primitives/3-stages/#update-the-pipeline-template","text":"With the continuous_integration stage defined, we can update the Pipeline Template (Jenkinsfile, in your single-job ) to make use of it. Update the Pipeline Template to: Pipeline Template continuous_integration () Then click Save .","title":"Update the Pipeline Template"},{"location":"tutorials/jte-primitives/3-stages/#run-the-pipeline","text":"From the Pipeline Job's main page, click Build Now in the left-hand navigation menu. When viewing the build logs (click Build Number and then Console Output ), you should see output similar to: Started by user admin [JTE] Pipeline Configuration Modifications (show) [JTE] Obtained Pipeline Template from job configuration [JTE] Loading Library maven (show) [JTE] Loading Library sonarqube (show) [JTE] Template Primitives are overwriting Jenkins steps with the following names: (show) [Pipeline] Start of Pipeline [JTE][Stage - continuous_integration] [JTE][Step - maven/build.call()] [Pipeline] stage [Pipeline] { (Maven: Build) [Pipeline] echo build from the maven library [Pipeline] } [Pipeline] // stage [JTE][Step - sonarqube/static_code_analysis.call()] [Pipeline] stage [Pipeline] { (SonarQube: Static Code Analysis) [Pipeline] echo static code analysis from the sonarqube library [Pipeline] } [Pipeline] // stage [Pipeline] End of Pipeline Finished: SUCCESS Important When reading the build logs of a JTE job, you can identify the start of stages by looking for [JTE] [Stage - *] in the output. In this case, the log output was [JTE] [Stage - continuous_integration] indicating a Stage called continuous_integration` is about to be executed.","title":"Run the Pipeline"},{"location":"tutorials/jte-primitives/4-application-environments/","text":"Application Environments \u00b6 What is an Application Environment? \u00b6 Performing an automated deployment is a ubiquitous step in continuous delivery pipelines. Libraries can implement steps to perform deployments and, in doing so, need a mechanism to tell the deployment step which Application Environment is being deployed to. The Application Environment acts to encapsulate the contextual information that identifies and differentiates an application's environments, such as dev, test, and production. In general, Library Steps shouldn't accept input parameters. Deployment steps are one of the few exceptions to this rule. Note View the Application Environments documentation here . A Word on Input Parameters to Library Steps \u00b6 Understanding why , in general, Library Steps shouldn't accept input parameters is fundamental to understanding the goals of the Jenkins Templating Engine. Let's say we had some teams in an organization leveraging SonarQube for static code analysis and others using Fortify. If the static_code_analysis step implemented by the sonarqube library took input parameters - it would then require that every library that implements static_code_analysis take the same input parameters, lest you break the interchangeability of libraries to use the same template. This would mean that the fortify library's implementation would have to take the same input parameters as the sonarqube 's implementation - otherwise switching between the two would break the code. Being able to swap implementations of steps in and out through different libraries is the primary mechanism through which JTE supports creating reusable, tool-agnostic Pipeline Templates. It's understandable that Library Steps require some externalized configuration to avoid hard-coding dependencies like server locations, thresholds for failure, etc. This is why library configuration is done through the Pipeline Configuration file and passed directly to the steps through the JTE plugin as opposed to directly via input parameters. Deployment steps are different. It is safe to assume that every step that performs a deployment needs to know some information about the environment. Define and Use an Application Environment \u00b6 Create a Deployment Library \u00b6 Let's actually create a mock deployment library to demonstrate the utility of Application Environments. In the same library repository used during JTE: The Basics, add a new library called ansible with a step in a directory called steps . Call the file deploy_to.groovy . Note Like Gradle, Maven, and SonarQube, you don't need to know much about Ansible to complete this lab, but there is a wealth of information about it available online if you are interested and it is a tool commonly used by DevSecOps teams. Remember that libraries are just subdirectories within a source code repository and that steps are just Groovy files in those subdirectories that typically implement a call method. For the sake of our pretend ansible library, let's assume that it needs to know a list of IP addresses relevant to the environment it's deploying to. ./libraries/ansible/steps/deploy_to.groovy void call ( app_env ) { stage ( \"Deploy to: ${app_env.long_name}\" ) { println 'Performing a deployment through Ansible..' app_env . ip_addresses . each { ip -> println \"Deploying to ${ip}\" } } } This step will announce it's performing an Ansible deployment and then iterate over the IP addresses provided for the Application Environment and print out the target server. Note The file structure within your libraries directory should now be: . \u2514\u2500\u2500 libraries \u251c\u2500\u2500 ansible \u2502 \u2514\u2500\u2500 steps \u2502 \u2514\u2500\u2500 deploy_to.groovy \u251c\u2500\u2500 gradle \u2502 \u2514\u2500\u2500 steps \u2502 \u2514\u2500\u2500 build.groovy \u251c\u2500\u2500 maven \u2502 \u2514\u2500\u2500 steps \u2502 \u2514\u2500\u2500 build.groovy \u2514\u2500\u2500 sonarqube \u2514\u2500\u2500 steps \u2514\u2500\u2500 static_code_analysis.groovy Important Make sure you've pushed these changes to your global library repo's main/master branch before proceeding. Define the Application Environments in the Pipeline Configuration \u00b6 We now need to load the ansible library and define the Application Environments. In your single-job , go to Configure and change the Pipeline Configuration to: Pipeline Configuration libraries { maven sonarqube ansible } stages { continuous_integration { build static_code_analysis } } application_environments { dev { ip_addresses = [ \"0.0.0.1\" , \"0.0.0.2\" ] } prod { long_name = \"Production\" ip_addresses = [ \"0.0.1.1\" , \"0.0.1.2\" , \"0.0.1.3\" , \"0.0.1.4\" ] } } Important Application Environments are defined in the application_environments block within the Pipeline Configuration. Each key defined in this block will represent an Application Environment and a variable will be made available in the pipeline template based upon this name. The only two keys that Application Environments explicitly define are short_name and long_name . These values both default to the key defining the Application Environment (i.e. long_name would have been prod and not Production if we had not declared it) in the Pipeline Configuration, but can be overridden. Update the Pipeline Template \u00b6 Now that we have a library that performs a deployment step and Application Environments defined in the Pipeline Configuration, let's update the Pipeline Template to pull it all together. Update the Jenkinsfile (your default pipeline template in your single-job ) to: Jenkinsfile (in your single-job) continuous_integration () deploy_to dev deploy_to prod Note These variables dev and prod come directly from the Applications Environments we just defined in the Pipeline Configuration. Run the Pipeline \u00b6 Save your configuration. From the Pipeline job's main page, click Build Now in the left-hand navigation menu. When viewing the build logs (click Build number, then Console Output ), you should see output similar to: Started by user admin [JTE] Pipeline Configuration Modifications (show) [JTE] Obtained Pipeline Template from job configuration [JTE] Loading Library maven (show) [JTE] Loading Library sonarqube (show) [JTE] Loading Library ansible (show) [JTE] Template Primitives are overwriting Jenkins steps with the following names: (show) [Pipeline] Start of Pipeline [JTE][Stage - continuous_integration] [JTE][Step - maven/build.call()] [Pipeline] stage [Pipeline] { (Maven: Build) [Pipeline] echo build from the maven library [Pipeline] } [Pipeline] // stage [JTE][Step - sonarqube/static_code_analysis.call()] [Pipeline] stage [Pipeline] { (SonarQube: Static Code Analysis) [Pipeline] echo static code analysis from the sonarqube library [Pipeline] } [Pipeline] // stage [JTE][Step - ansible/deploy_to.call(ApplicationEnvironment)] [Pipeline] stage [Pipeline] { (Deploy to: dev) [Pipeline] echo Performing a deployment through Ansible.. [Pipeline] echo Deploying to 0.0.0.1 [Pipeline] echo Deploying to 0.0.0.2 [Pipeline] } [Pipeline] // stage [JTE][Step - ansible/deploy_to.call(ApplicationEnvironment)] [Pipeline] stage [Pipeline] { (Deploy to: Production) [Pipeline] echo Performing a deployment through Ansible.. [Pipeline] echo Deploying to 0.0.1.1 [Pipeline] echo Deploying to 0.0.1.2 [Pipeline] echo Deploying to 0.0.1.3 [Pipeline] echo Deploying to 0.0.1.4 [Pipeline] } [Pipeline] // stage [Pipeline] End of Pipeline Finished: SUCCESS Notice the output was different for the deployment to the dev environment vs the deployment to prod . This is because different values were stored in each Application Environment and the library was able to use this contextual information and respond accordingly.","title":"Application Environments"},{"location":"tutorials/jte-primitives/4-application-environments/#application-environments","text":"","title":"Application Environments"},{"location":"tutorials/jte-primitives/4-application-environments/#what-is-an-application-environment","text":"Performing an automated deployment is a ubiquitous step in continuous delivery pipelines. Libraries can implement steps to perform deployments and, in doing so, need a mechanism to tell the deployment step which Application Environment is being deployed to. The Application Environment acts to encapsulate the contextual information that identifies and differentiates an application's environments, such as dev, test, and production. In general, Library Steps shouldn't accept input parameters. Deployment steps are one of the few exceptions to this rule. Note View the Application Environments documentation here .","title":"What is an Application Environment?"},{"location":"tutorials/jte-primitives/4-application-environments/#a-word-on-input-parameters-to-library-steps","text":"Understanding why , in general, Library Steps shouldn't accept input parameters is fundamental to understanding the goals of the Jenkins Templating Engine. Let's say we had some teams in an organization leveraging SonarQube for static code analysis and others using Fortify. If the static_code_analysis step implemented by the sonarqube library took input parameters - it would then require that every library that implements static_code_analysis take the same input parameters, lest you break the interchangeability of libraries to use the same template. This would mean that the fortify library's implementation would have to take the same input parameters as the sonarqube 's implementation - otherwise switching between the two would break the code. Being able to swap implementations of steps in and out through different libraries is the primary mechanism through which JTE supports creating reusable, tool-agnostic Pipeline Templates. It's understandable that Library Steps require some externalized configuration to avoid hard-coding dependencies like server locations, thresholds for failure, etc. This is why library configuration is done through the Pipeline Configuration file and passed directly to the steps through the JTE plugin as opposed to directly via input parameters. Deployment steps are different. It is safe to assume that every step that performs a deployment needs to know some information about the environment.","title":"A Word on Input Parameters to Library Steps"},{"location":"tutorials/jte-primitives/4-application-environments/#define-and-use-an-application-environment","text":"","title":"Define and Use an Application Environment"},{"location":"tutorials/jte-primitives/4-application-environments/#create-a-deployment-library","text":"Let's actually create a mock deployment library to demonstrate the utility of Application Environments. In the same library repository used during JTE: The Basics, add a new library called ansible with a step in a directory called steps . Call the file deploy_to.groovy . Note Like Gradle, Maven, and SonarQube, you don't need to know much about Ansible to complete this lab, but there is a wealth of information about it available online if you are interested and it is a tool commonly used by DevSecOps teams. Remember that libraries are just subdirectories within a source code repository and that steps are just Groovy files in those subdirectories that typically implement a call method. For the sake of our pretend ansible library, let's assume that it needs to know a list of IP addresses relevant to the environment it's deploying to. ./libraries/ansible/steps/deploy_to.groovy void call ( app_env ) { stage ( \"Deploy to: ${app_env.long_name}\" ) { println 'Performing a deployment through Ansible..' app_env . ip_addresses . each { ip -> println \"Deploying to ${ip}\" } } } This step will announce it's performing an Ansible deployment and then iterate over the IP addresses provided for the Application Environment and print out the target server. Note The file structure within your libraries directory should now be: . \u2514\u2500\u2500 libraries \u251c\u2500\u2500 ansible \u2502 \u2514\u2500\u2500 steps \u2502 \u2514\u2500\u2500 deploy_to.groovy \u251c\u2500\u2500 gradle \u2502 \u2514\u2500\u2500 steps \u2502 \u2514\u2500\u2500 build.groovy \u251c\u2500\u2500 maven \u2502 \u2514\u2500\u2500 steps \u2502 \u2514\u2500\u2500 build.groovy \u2514\u2500\u2500 sonarqube \u2514\u2500\u2500 steps \u2514\u2500\u2500 static_code_analysis.groovy Important Make sure you've pushed these changes to your global library repo's main/master branch before proceeding.","title":"Create a Deployment Library"},{"location":"tutorials/jte-primitives/4-application-environments/#define-the-application-environments-in-the-pipeline-configuration","text":"We now need to load the ansible library and define the Application Environments. In your single-job , go to Configure and change the Pipeline Configuration to: Pipeline Configuration libraries { maven sonarqube ansible } stages { continuous_integration { build static_code_analysis } } application_environments { dev { ip_addresses = [ \"0.0.0.1\" , \"0.0.0.2\" ] } prod { long_name = \"Production\" ip_addresses = [ \"0.0.1.1\" , \"0.0.1.2\" , \"0.0.1.3\" , \"0.0.1.4\" ] } } Important Application Environments are defined in the application_environments block within the Pipeline Configuration. Each key defined in this block will represent an Application Environment and a variable will be made available in the pipeline template based upon this name. The only two keys that Application Environments explicitly define are short_name and long_name . These values both default to the key defining the Application Environment (i.e. long_name would have been prod and not Production if we had not declared it) in the Pipeline Configuration, but can be overridden.","title":"Define the Application Environments in the Pipeline Configuration"},{"location":"tutorials/jte-primitives/4-application-environments/#update-the-pipeline-template","text":"Now that we have a library that performs a deployment step and Application Environments defined in the Pipeline Configuration, let's update the Pipeline Template to pull it all together. Update the Jenkinsfile (your default pipeline template in your single-job ) to: Jenkinsfile (in your single-job) continuous_integration () deploy_to dev deploy_to prod Note These variables dev and prod come directly from the Applications Environments we just defined in the Pipeline Configuration.","title":"Update the Pipeline Template"},{"location":"tutorials/jte-primitives/4-application-environments/#run-the-pipeline","text":"Save your configuration. From the Pipeline job's main page, click Build Now in the left-hand navigation menu. When viewing the build logs (click Build number, then Console Output ), you should see output similar to: Started by user admin [JTE] Pipeline Configuration Modifications (show) [JTE] Obtained Pipeline Template from job configuration [JTE] Loading Library maven (show) [JTE] Loading Library sonarqube (show) [JTE] Loading Library ansible (show) [JTE] Template Primitives are overwriting Jenkins steps with the following names: (show) [Pipeline] Start of Pipeline [JTE][Stage - continuous_integration] [JTE][Step - maven/build.call()] [Pipeline] stage [Pipeline] { (Maven: Build) [Pipeline] echo build from the maven library [Pipeline] } [Pipeline] // stage [JTE][Step - sonarqube/static_code_analysis.call()] [Pipeline] stage [Pipeline] { (SonarQube: Static Code Analysis) [Pipeline] echo static code analysis from the sonarqube library [Pipeline] } [Pipeline] // stage [JTE][Step - ansible/deploy_to.call(ApplicationEnvironment)] [Pipeline] stage [Pipeline] { (Deploy to: dev) [Pipeline] echo Performing a deployment through Ansible.. [Pipeline] echo Deploying to 0.0.0.1 [Pipeline] echo Deploying to 0.0.0.2 [Pipeline] } [Pipeline] // stage [JTE][Step - ansible/deploy_to.call(ApplicationEnvironment)] [Pipeline] stage [Pipeline] { (Deploy to: Production) [Pipeline] echo Performing a deployment through Ansible.. [Pipeline] echo Deploying to 0.0.1.1 [Pipeline] echo Deploying to 0.0.1.2 [Pipeline] echo Deploying to 0.0.1.3 [Pipeline] echo Deploying to 0.0.1.4 [Pipeline] } [Pipeline] // stage [Pipeline] End of Pipeline Finished: SUCCESS Notice the output was different for the deployment to the dev environment vs the deployment to prod . This is because different values were stored in each Application Environment and the library was able to use this contextual information and respond accordingly.","title":"Run the Pipeline"},{"location":"tutorials/jte-primitives/5-keywords/","text":"Keywords \u00b6 What is a Keyword? \u00b6 Keywords allow you to define variables in the Pipeline Configuration that can be referenced in your Pipeline Template. This allows you to keep templates as readable as possible by externalizing the definition of complex variables out of the template. The most common use case so far for Keywords is storing regular expressions that map to common branch names in the GitFlow Branching Strategy to be used in evaluating whether or not aspects of the pipeline should execute based on a matching branch. In this example, we'll use a Keyword as a feature flag externalized from the Pipeline Template to conditionally determine if a manual gate is required before the deployment to Production. Note View the Keyword documentation here . Define and Use a Keyword \u00b6 Define the Keyword in the Pipeline Configuration \u00b6 For your single-job configuration, update the Pipeline Configuration to: Pipeline Configuration libraries { maven sonarqube ansible } stages { continuous_integration { build static_code_analysis } } application_environments { dev { ip_addresses = [ \"0.0.0.1\" , \"0.0.0.2\" ] } prod { long_name = \"Production\" ip_addresses = [ \"0.0.1.1\" , \"0.0.1.2\" , \"0.0.1.3\" , \"0.0.1.4\" ] } } keywords { requiresApproval = true } Important All Keywords will be defined in the keywords block of the Pipeline Configuration. Traditional variable setting syntax of a = b is used to define Keywords. Update the Pipeline Template \u00b6 With the continuous_integration stage defined, we can update the pipeline template to make use of it. Update the Pipeline Template ( Jenkinsfile in your single-job) to: Jenkinsfile continuous_integration () deploy_to dev if ( requiresApproval ) { timeout ( time: 5 , unit: 'MINUTES' ) { input 'Approve the deployment?' } } deploy_to prod Important This is an example to demonstrate the use of a Keyword in a Pipeline Template and not how we would recommend you enable this sort of gate in a production pipeline. We would recommend that, in practice, the deployment library inherits this manual gate approval so that requiresApproval could be set on each Application Environment individually. Run the Pipeline \u00b6 From the Pipeline Job's main page, click Build Now in the left-hand navigation menu. When viewing the build logs (click Build number, then Console Output ), you should see output similar to: Started by user admin [JTE] Pipeline Configuration Modifications (show) [JTE] Obtained Pipeline Template from job configuration [JTE] Loading Library maven (show) [JTE] Loading Library sonarqube (show) [JTE] Loading Library ansible (show) [JTE] Template Primitives are overwriting Jenkins steps with the following names: (show) [Pipeline] Start of Pipeline [JTE][Stage - continuous_integration] [JTE][Step - maven/build.call()] [Pipeline] stage [Pipeline] { (Maven: Build) [Pipeline] echo build from the maven library [Pipeline] } [Pipeline] // stage [JTE][Step - sonarqube/static_code_analysis.call()] [Pipeline] stage [Pipeline] { (SonarQube: Static Code Analysis) [Pipeline] echo static code analysis from the sonarqube library [Pipeline] } [Pipeline] // stage [JTE][Step - ansible/deploy_to.call(ApplicationEnvironment)] [Pipeline] stage [Pipeline] { (Deploy to: dev) [Pipeline] echo Performing a deployment through Ansible.. [Pipeline] echo Deploying to 0.0.0.1 [Pipeline] echo Deploying to 0.0.0.2 [Pipeline] } [Pipeline] // stage [Pipeline] timeout Timeout set to expire in 5 min 0 sec [Pipeline] { [Pipeline] input Approve the deployment? Proceed or Abort Click the Proceed link and the job should continue, showing Approved by admin : Approved by admin [Pipeline] } [Pipeline] // timeout [JTE][Step - ansible/deploy_to.call(ApplicationEnvironment)] [Pipeline] stage [Pipeline] { (Deploy to: Production) [Pipeline] echo Performing a deployment through Ansible.. [Pipeline] echo Deploying to 0.0.1.1 [Pipeline] echo Deploying to 0.0.1.2 [Pipeline] echo Deploying to 0.0.1.3 [Pipeline] echo Deploying to 0.0.1.4 [Pipeline] } [Pipeline] // stage [Pipeline] End of Pipeline Finished: SUCCESS Important When reading the build logs of a JTE job, you can identify the start of stages by looking for [JTE] [Stage - *] in the output. In this case, the log output was [JTE] [Stage - continuous_integration] indicating a Stage called continuous_integration is about to be executed. Note The exercise of setting requiresApproval = false and seeing the difference is left to the reader.","title":"Keywords"},{"location":"tutorials/jte-primitives/5-keywords/#keywords","text":"","title":"Keywords"},{"location":"tutorials/jte-primitives/5-keywords/#what-is-a-keyword","text":"Keywords allow you to define variables in the Pipeline Configuration that can be referenced in your Pipeline Template. This allows you to keep templates as readable as possible by externalizing the definition of complex variables out of the template. The most common use case so far for Keywords is storing regular expressions that map to common branch names in the GitFlow Branching Strategy to be used in evaluating whether or not aspects of the pipeline should execute based on a matching branch. In this example, we'll use a Keyword as a feature flag externalized from the Pipeline Template to conditionally determine if a manual gate is required before the deployment to Production. Note View the Keyword documentation here .","title":"What is a Keyword?"},{"location":"tutorials/jte-primitives/5-keywords/#define-and-use-a-keyword","text":"","title":"Define and Use a Keyword"},{"location":"tutorials/jte-primitives/5-keywords/#define-the-keyword-in-the-pipeline-configuration","text":"For your single-job configuration, update the Pipeline Configuration to: Pipeline Configuration libraries { maven sonarqube ansible } stages { continuous_integration { build static_code_analysis } } application_environments { dev { ip_addresses = [ \"0.0.0.1\" , \"0.0.0.2\" ] } prod { long_name = \"Production\" ip_addresses = [ \"0.0.1.1\" , \"0.0.1.2\" , \"0.0.1.3\" , \"0.0.1.4\" ] } } keywords { requiresApproval = true } Important All Keywords will be defined in the keywords block of the Pipeline Configuration. Traditional variable setting syntax of a = b is used to define Keywords.","title":"Define the Keyword in the Pipeline Configuration"},{"location":"tutorials/jte-primitives/5-keywords/#update-the-pipeline-template","text":"With the continuous_integration stage defined, we can update the pipeline template to make use of it. Update the Pipeline Template ( Jenkinsfile in your single-job) to: Jenkinsfile continuous_integration () deploy_to dev if ( requiresApproval ) { timeout ( time: 5 , unit: 'MINUTES' ) { input 'Approve the deployment?' } } deploy_to prod Important This is an example to demonstrate the use of a Keyword in a Pipeline Template and not how we would recommend you enable this sort of gate in a production pipeline. We would recommend that, in practice, the deployment library inherits this manual gate approval so that requiresApproval could be set on each Application Environment individually.","title":"Update the Pipeline Template"},{"location":"tutorials/jte-primitives/5-keywords/#run-the-pipeline","text":"From the Pipeline Job's main page, click Build Now in the left-hand navigation menu. When viewing the build logs (click Build number, then Console Output ), you should see output similar to: Started by user admin [JTE] Pipeline Configuration Modifications (show) [JTE] Obtained Pipeline Template from job configuration [JTE] Loading Library maven (show) [JTE] Loading Library sonarqube (show) [JTE] Loading Library ansible (show) [JTE] Template Primitives are overwriting Jenkins steps with the following names: (show) [Pipeline] Start of Pipeline [JTE][Stage - continuous_integration] [JTE][Step - maven/build.call()] [Pipeline] stage [Pipeline] { (Maven: Build) [Pipeline] echo build from the maven library [Pipeline] } [Pipeline] // stage [JTE][Step - sonarqube/static_code_analysis.call()] [Pipeline] stage [Pipeline] { (SonarQube: Static Code Analysis) [Pipeline] echo static code analysis from the sonarqube library [Pipeline] } [Pipeline] // stage [JTE][Step - ansible/deploy_to.call(ApplicationEnvironment)] [Pipeline] stage [Pipeline] { (Deploy to: dev) [Pipeline] echo Performing a deployment through Ansible.. [Pipeline] echo Deploying to 0.0.0.1 [Pipeline] echo Deploying to 0.0.0.2 [Pipeline] } [Pipeline] // stage [Pipeline] timeout Timeout set to expire in 5 min 0 sec [Pipeline] { [Pipeline] input Approve the deployment? Proceed or Abort Click the Proceed link and the job should continue, showing Approved by admin : Approved by admin [Pipeline] } [Pipeline] // timeout [JTE][Step - ansible/deploy_to.call(ApplicationEnvironment)] [Pipeline] stage [Pipeline] { (Deploy to: Production) [Pipeline] echo Performing a deployment through Ansible.. [Pipeline] echo Deploying to 0.0.1.1 [Pipeline] echo Deploying to 0.0.1.2 [Pipeline] echo Deploying to 0.0.1.3 [Pipeline] echo Deploying to 0.0.1.4 [Pipeline] } [Pipeline] // stage [Pipeline] End of Pipeline Finished: SUCCESS Important When reading the build logs of a JTE job, you can identify the start of stages by looking for [JTE] [Stage - *] in the output. In this case, the log output was [JTE] [Stage - continuous_integration] indicating a Stage called continuous_integration is about to be executed. Note The exercise of setting requiresApproval = false and seeing the difference is left to the reader.","title":"Run the Pipeline"},{"location":"tutorials/jte-primitives/6-summary/","text":"Summary \u00b6 We learned a lot in this lab! Let's recap some of what we learned: Pipeline Primitives \u00b6 Primitive Type Description Block Identifier Stage Groups steps together into a single invoked method as to avoid duplication in the Pipeline Template. stages{} Application Environment Encapsulates environment specific context, primarily for use in deployment steps. application_environments{} Keywords Externalize the act of setting variables out of Pipeline Templates and into the Pipeline Configuration. keywords{} Recall: Why shouldn't Library Steps take input parameters? \u00b6 It fundamentally breaks the interchangeability of different implementations of the same step by different libraries by introducing a requirement that all implementations of that step accept the same parameters.","title":"Summary"},{"location":"tutorials/jte-primitives/6-summary/#summary","text":"We learned a lot in this lab! Let's recap some of what we learned:","title":"Summary"},{"location":"tutorials/jte-primitives/6-summary/#pipeline-primitives","text":"Primitive Type Description Block Identifier Stage Groups steps together into a single invoked method as to avoid duplication in the Pipeline Template. stages{} Application Environment Encapsulates environment specific context, primarily for use in deployment steps. application_environments{} Keywords Externalize the act of setting variables out of Pipeline Templates and into the Pipeline Configuration. keywords{}","title":"Pipeline Primitives"},{"location":"tutorials/jte-primitives/6-summary/#recall-why-shouldnt-library-steps-take-input-parameters","text":"It fundamentally breaks the interchangeability of different implementations of the same step by different libraries by introducing a requirement that all implementations of that step accept the same parameters.","title":"Recall: Why shouldn't Library Steps take input parameters?"},{"location":"tutorials/jte-the-basics/","text":"JTE: The Basics \u00b6 The purpose of this lab is to introduce you to the Jenkins Templating Engine as a framework for building Jenkins pipelines in a way that allows you to share pipeline templates between teams. By being able to build reusable, tool-agnostic pipeline templates and apply them to multiple applications simultaneously, you can then remove individual Jenkinsfiles from each source code repository. Important In JTE, we talk a lot about the concept of governance. When we say governance, we mean that JTE allows you to enforce a common software delivery process by applying the same pipeline template to multiple repositories at the same time. The modularity that JTE promotes allows you to do this across teams regardless of the specific tools that each application may be using. What You'll Learn \u00b6 Configure your first tool-agnostic pipeline template Create your first set of Pipeline Libraries to provide tool-specific implementations of steps Create your first Pipeline Configuration file that implements the template Learn the different types of jobs available in Jenkins and when to use them Learn how to take the Jenkinsfile (pipeline template) out of the repository Learn how to consolidate Pipeline Configurations and apply the same template across multiple repositories","title":"JTE: The Basics"},{"location":"tutorials/jte-the-basics/#jte-the-basics","text":"The purpose of this lab is to introduce you to the Jenkins Templating Engine as a framework for building Jenkins pipelines in a way that allows you to share pipeline templates between teams. By being able to build reusable, tool-agnostic pipeline templates and apply them to multiple applications simultaneously, you can then remove individual Jenkinsfiles from each source code repository. Important In JTE, we talk a lot about the concept of governance. When we say governance, we mean that JTE allows you to enforce a common software delivery process by applying the same pipeline template to multiple repositories at the same time. The modularity that JTE promotes allows you to do this across teams regardless of the specific tools that each application may be using.","title":"JTE: The Basics"},{"location":"tutorials/jte-the-basics/#what-youll-learn","text":"Configure your first tool-agnostic pipeline template Create your first set of Pipeline Libraries to provide tool-specific implementations of steps Create your first Pipeline Configuration file that implements the template Learn the different types of jobs available in Jenkins and when to use them Learn how to take the Jenkinsfile (pipeline template) out of the repository Learn how to consolidate Pipeline Configurations and apply the same template across multiple repositories","title":"What You'll Learn"},{"location":"tutorials/jte-the-basics/1-prerequisites/","text":"Prerequisites \u00b6 Jenkins Instance \u00b6 A Jenkins instance will be required for this lab. If you don't have one available to you, we would recommend going through the Local Development Learning Lab to deploy a local Jenkins instance through Docker. Ability to Create GitHub Repositories \u00b6 When creating your first set of Pipeline Libraries and externalizing the Pipeline Configuration from Jenkins you will need to be able to create GitHub repositories on github.com . Note Theoretically, any git-based SCM provider (Bitbucket, GitHub, GitLab, etc.) should integrate and work as expected with JTE. For the purposes of simplifying this lab, we will be using GitHub. GitHub PAT in the Jenkins Credential Store \u00b6 Note If you intend to create public repositories then your PAT is merely acting to authenticate to GitHub in order to avoid rate limiting; you don't need to grant any scopes to the PAT. If you will be creating private repositories, you'll need to grant the repo scope to the PAT. Create a GitHub Personal Access Token (PAT) . See the link for more specific directions. Click on your profile picture at the top-right of GitHub, then Settings , Developer settings , and Personal access tokens . Create a Classic token, don't use the Fine-grained token beta. Note/Name: jte-the-basics Select scope: repo (Full control of private repositories) Select scope: admin:org (Full control of GitHub organizations and teams, read and write organization projects) Leave all other scopes blank, click the Generate token button. Copy this token and store it in the Jenkins credential store. From your Jenkins home page, click Manage Jenkins in the left-hand navigation menu. From your the Manage Jenkins page, click Manage Credentials . Select the (global) link under Domains . Click the Add Credentials button at the top right. Kind: Username with password Enter your GitHub username in the Username field. Paste the Personal Access Token into the Password field. Enter github into the ID field. Enter github into the Description field. Click Create . You should see your new credential under the list of Global credentials (unrestricted) :","title":"Prerequisites"},{"location":"tutorials/jte-the-basics/1-prerequisites/#prerequisites","text":"","title":"Prerequisites"},{"location":"tutorials/jte-the-basics/1-prerequisites/#jenkins-instance","text":"A Jenkins instance will be required for this lab. If you don't have one available to you, we would recommend going through the Local Development Learning Lab to deploy a local Jenkins instance through Docker.","title":"Jenkins Instance"},{"location":"tutorials/jte-the-basics/1-prerequisites/#ability-to-create-github-repositories","text":"When creating your first set of Pipeline Libraries and externalizing the Pipeline Configuration from Jenkins you will need to be able to create GitHub repositories on github.com . Note Theoretically, any git-based SCM provider (Bitbucket, GitHub, GitLab, etc.) should integrate and work as expected with JTE. For the purposes of simplifying this lab, we will be using GitHub.","title":"Ability to Create GitHub Repositories"},{"location":"tutorials/jte-the-basics/1-prerequisites/#github-pat-in-the-jenkins-credential-store","text":"Note If you intend to create public repositories then your PAT is merely acting to authenticate to GitHub in order to avoid rate limiting; you don't need to grant any scopes to the PAT. If you will be creating private repositories, you'll need to grant the repo scope to the PAT. Create a GitHub Personal Access Token (PAT) . See the link for more specific directions. Click on your profile picture at the top-right of GitHub, then Settings , Developer settings , and Personal access tokens . Create a Classic token, don't use the Fine-grained token beta. Note/Name: jte-the-basics Select scope: repo (Full control of private repositories) Select scope: admin:org (Full control of GitHub organizations and teams, read and write organization projects) Leave all other scopes blank, click the Generate token button. Copy this token and store it in the Jenkins credential store. From your Jenkins home page, click Manage Jenkins in the left-hand navigation menu. From your the Manage Jenkins page, click Manage Credentials . Select the (global) link under Domains . Click the Add Credentials button at the top right. Kind: Username with password Enter your GitHub username in the Username field. Paste the Personal Access Token into the Password field. Enter github into the ID field. Enter github into the Description field. Click Create . You should see your new credential under the list of Global credentials (unrestricted) :","title":"GitHub PAT in the Jenkins Credential Store"},{"location":"tutorials/jte-the-basics/2-pipeline-job/","text":"Writing a Template \u00b6 In JTE, instead of creating application-specific Jenkinsfiles , we create pipeline templates that represent the business logic of a pipeline in a tool-agnostic way. Create a Pipeline Job \u00b6 To demonstrate this, first create a Pipeline job. From the Jenkins home page, navigate to New Item in the left-hand navigation menu. Enter an item name for the job to be created. You can use single-job . Select Pipeline from the list of available job types. Click OK . Write the Template \u00b6 Overview \u00b6 For this lab, let's create a pipeline that can build an artifact with Maven and then perform static code analysis with SonarQube. Scroll down on the job's configuration page to the Pipeline configuration section. Make sure that Jenkins Templating Engine is selected in the Definition drop down configuration option. Check the box for Provide default pipeline template (Jenkinsfile) . In the Jenkinsfile text box, enter: build() static_code_analysis() Note A word on vocabulary: The entire script above is called a Pipeline Template . Pipeline Templates invoke steps , in this case build and static_code_analysis , that are implemented by libraries . You can click Save to save this configuration. In the next section we'll be creating the Pipeline Libraries that implement the build and static_code_analysis steps.","title":"Writing a Template"},{"location":"tutorials/jte-the-basics/2-pipeline-job/#writing-a-template","text":"In JTE, instead of creating application-specific Jenkinsfiles , we create pipeline templates that represent the business logic of a pipeline in a tool-agnostic way.","title":"Writing a Template"},{"location":"tutorials/jte-the-basics/2-pipeline-job/#create-a-pipeline-job","text":"To demonstrate this, first create a Pipeline job. From the Jenkins home page, navigate to New Item in the left-hand navigation menu. Enter an item name for the job to be created. You can use single-job . Select Pipeline from the list of available job types. Click OK .","title":"Create a Pipeline Job"},{"location":"tutorials/jte-the-basics/2-pipeline-job/#write-the-template","text":"","title":"Write the Template"},{"location":"tutorials/jte-the-basics/2-pipeline-job/#overview","text":"For this lab, let's create a pipeline that can build an artifact with Maven and then perform static code analysis with SonarQube. Scroll down on the job's configuration page to the Pipeline configuration section. Make sure that Jenkins Templating Engine is selected in the Definition drop down configuration option. Check the box for Provide default pipeline template (Jenkinsfile) . In the Jenkinsfile text box, enter: build() static_code_analysis() Note A word on vocabulary: The entire script above is called a Pipeline Template . Pipeline Templates invoke steps , in this case build and static_code_analysis , that are implemented by libraries . You can click Save to save this configuration. In the next section we'll be creating the Pipeline Libraries that implement the build and static_code_analysis steps.","title":"Overview"},{"location":"tutorials/jte-the-basics/3-first-libraries/","text":"Creating a Library \u00b6 In the previous section we created a Pipeline Template that invoked build() and static_code_analysis() steps. In this part of the lab, we're going to create libraries for these steps to implement them in our template. Create a GitHub Repository \u00b6 Libraries can either be packaged into a separate plugin for distribution or fetched directly from a source code management repository. Retrieving libraries from a repository is the most common way of storing Pipeline Libraries for integration with the Jenkins Templating Engine (JTE). Go ahead and create a new GitHub repository in your account: Click your profile picture in GitHub, and go to Your repositories . Click the New button at top right and use the following configuration: It can be named whatever you like, though jte-the-basics would make sense. Create the Libraries \u00b6 Clone your new repository, then create and push the following directory structure, with empty files: . \u2514\u2500\u2500 libraries \u251c\u2500\u2500 maven \u2502 \u2514\u2500\u2500 steps \u2502 \u2514\u2500\u2500 build.groovy \u2514\u2500\u2500 sonarqube \u2514\u2500\u2500 steps \u2514\u2500\u2500 static_code_analysis.groovy Important If you need a primer on Git (that is, how to commit and push code changes) try reading over the Making changes section here . When configuring this repository as a Library Source for JTE in Jenkins, you will be able to configure the base directory. Since there might be other sources in this repository in the future, all of the libraries we create will be stored in the libraries directory. It is important to understand that a library in JTE is just a directory , likely in a source code repository, that contains a steps directory with Groovy script files. When a library is loaded, each Groovy file in the library's steps directory will become a step named after the base filename. Push the code to the main branch. Your repo in the GitHub web UI should look like this: Implement the Steps \u00b6 In this lab, we're just getting accustomed to the Jenkins Templating Engine and how it works. So the implementation of the steps for this lab will just be print statements that show where the step is coming from. Generally, the most idiomatic way to define a step is to create a call method that takes no input parameters. Note In future labs, we'll learn how to pass information to our steps through the Pipeline Configuration file. Push the following code to the empty files you created in your library repo: ./libraries/maven/steps/build.groovy void call () { stage ( \"Maven: Build\" ) { println \"build from the maven library\" } } ./libraries/sonarqube/steps/static_code_analysis.groovy void call () { stage ( \"SonarQube: Static Code Analysis\" ) { println \"static code analysis from the sonarqube library\" } } Configure the Library Source \u00b6 Now that we have a GitHub repository containing Pipeline Libraries, we have to tell JTE where to find them. This is done by configuring a Library Source in Jenkins. To make our libraries accessible to every job configured to use JTE on the Jenkins instance: In the left-hand navigation menu, click Manage Jenkins . User System Configuration , click Configure System . Scroll down to the Jenkins Templating Engine configuration section. Click Add under Library Sources -- don't edit the Pipeline Configuration section. Ensure the Library Provider is set to From SCM . Select Git as the SCM type. Enter the https repository URL to your library repository you pushed the Groovy scripts to. It should end in .git . Under Branch Specifier , specify whatever branch you have been pushing changes to, be it */main , */master , or something else. In the Credentials drop down menu, select the github credential created during the prerequisites. Enter libraries in the Base Directory text box. Click Save . Note As an aside - you can define as many Library Sources as you need. They can be defined globally for the entire Jenkins instance in Manage Jenkins > Configure System > Jenkins Templating Engine or under the Jenkins Templating Engine configuration section on Folders, or per-job, for more complex inheritance of libraries.","title":"Creating a Library"},{"location":"tutorials/jte-the-basics/3-first-libraries/#creating-a-library","text":"In the previous section we created a Pipeline Template that invoked build() and static_code_analysis() steps. In this part of the lab, we're going to create libraries for these steps to implement them in our template.","title":"Creating a Library"},{"location":"tutorials/jte-the-basics/3-first-libraries/#create-a-github-repository","text":"Libraries can either be packaged into a separate plugin for distribution or fetched directly from a source code management repository. Retrieving libraries from a repository is the most common way of storing Pipeline Libraries for integration with the Jenkins Templating Engine (JTE). Go ahead and create a new GitHub repository in your account: Click your profile picture in GitHub, and go to Your repositories . Click the New button at top right and use the following configuration: It can be named whatever you like, though jte-the-basics would make sense.","title":"Create a GitHub Repository"},{"location":"tutorials/jte-the-basics/3-first-libraries/#create-the-libraries","text":"Clone your new repository, then create and push the following directory structure, with empty files: . \u2514\u2500\u2500 libraries \u251c\u2500\u2500 maven \u2502 \u2514\u2500\u2500 steps \u2502 \u2514\u2500\u2500 build.groovy \u2514\u2500\u2500 sonarqube \u2514\u2500\u2500 steps \u2514\u2500\u2500 static_code_analysis.groovy Important If you need a primer on Git (that is, how to commit and push code changes) try reading over the Making changes section here . When configuring this repository as a Library Source for JTE in Jenkins, you will be able to configure the base directory. Since there might be other sources in this repository in the future, all of the libraries we create will be stored in the libraries directory. It is important to understand that a library in JTE is just a directory , likely in a source code repository, that contains a steps directory with Groovy script files. When a library is loaded, each Groovy file in the library's steps directory will become a step named after the base filename. Push the code to the main branch. Your repo in the GitHub web UI should look like this:","title":"Create the Libraries"},{"location":"tutorials/jte-the-basics/3-first-libraries/#implement-the-steps","text":"In this lab, we're just getting accustomed to the Jenkins Templating Engine and how it works. So the implementation of the steps for this lab will just be print statements that show where the step is coming from. Generally, the most idiomatic way to define a step is to create a call method that takes no input parameters. Note In future labs, we'll learn how to pass information to our steps through the Pipeline Configuration file. Push the following code to the empty files you created in your library repo: ./libraries/maven/steps/build.groovy void call () { stage ( \"Maven: Build\" ) { println \"build from the maven library\" } } ./libraries/sonarqube/steps/static_code_analysis.groovy void call () { stage ( \"SonarQube: Static Code Analysis\" ) { println \"static code analysis from the sonarqube library\" } }","title":"Implement the Steps"},{"location":"tutorials/jte-the-basics/3-first-libraries/#configure-the-library-source","text":"Now that we have a GitHub repository containing Pipeline Libraries, we have to tell JTE where to find them. This is done by configuring a Library Source in Jenkins. To make our libraries accessible to every job configured to use JTE on the Jenkins instance: In the left-hand navigation menu, click Manage Jenkins . User System Configuration , click Configure System . Scroll down to the Jenkins Templating Engine configuration section. Click Add under Library Sources -- don't edit the Pipeline Configuration section. Ensure the Library Provider is set to From SCM . Select Git as the SCM type. Enter the https repository URL to your library repository you pushed the Groovy scripts to. It should end in .git . Under Branch Specifier , specify whatever branch you have been pushing changes to, be it */main , */master , or something else. In the Credentials drop down menu, select the github credential created during the prerequisites. Enter libraries in the Base Directory text box. Click Save . Note As an aside - you can define as many Library Sources as you need. They can be defined globally for the entire Jenkins instance in Manage Jenkins > Configure System > Jenkins Templating Engine or under the Jenkins Templating Engine configuration section on Folders, or per-job, for more complex inheritance of libraries.","title":"Configure the Library Source"},{"location":"tutorials/jte-the-basics/4-first-configuration-file/","text":"Configure the Pipeline \u00b6 With the libraries created now discoverable by JTE, we can configure the pipeline and run it! From the main Jenkins page, click the job created earlier in this lab. In the left-hand navigation menu, click Configure . Scrolling down to the Pipeline portion of the job configuration, you should still see the Pipeline Template created earlier in the Jenkinsfile text box: build () static_code_analysis () It's now time to configure the pipeline by providing a Pipeline Configuration. Check the box for Provide pipeline configuration . In the Pipeline Configuration text box enter: libraries { maven sonarqube } Click Save . Important The libraries portion of the Pipeline Configuration file will read much like an application's technical stack. In this case, we're telling JTE during the initialization of the pipeline that it should load the maven and sonarqube libraries. The maven library will provide the build() step and the sonarqube library will provide the static_code_analysis() step. With these steps now implemented, we can run the pipeline. Run the Pipeline \u00b6 After clicking Save you'll be directed back to the main page for the job. To run the pipeline, click Build Now in the left-hand navigation menu. Refresh the page and you should see build number 1 in the Build History on the bottom left-hand side of the screen. Click the link to go to the Build's page. If you see a red X, something is wrong with your configuration. Look at the logs and see if they give any meaningful information; there may be something wrong with your library code or your branch may be wrong in the Library Sources configuration you set up earlier. In the left-hand navigation menu, click Console Output to view the build logs for this run of the pipeline. Started by user admin [JTE] Pipeline Configuration Modifications (show) [JTE] Obtained Pipeline Template from job configuration [JTE] Loading Library maven (show) [JTE] Loading Library sonarqube (show) [JTE] Template Primitives are overwriting Jenkins steps with the following names: [JTE] 1. build [Pipeline] Start of Pipeline [JTE][Step - maven/build.call()] [Pipeline] stage [Pipeline] { (Maven: Build) [Pipeline] echo build from the maven library [Pipeline] } [Pipeline] // stage [JTE][Step - sonarqube/static_code_analysis.call()] [Pipeline] stage [Pipeline] { (SonarQube: Static Code Analysis) [Pipeline] echo static code analysis from the sonarqube library [Pipeline] } [Pipeline] // stage [Pipeline] End of Pipeline Finished: SUCCESS As expected, JTE loaded the maven and sonarqube libraries and then executed the template. Note A couple points to note about the build log: Any line that starts with [JTE] is a log coming from the Jenkins Templating Engine. Pieces of JTE log output are hidden by default. Clicking show will expand these sections. The first part of the pipeline output shows the initialization process: Pipeline Configuration files being aggregated. Libraries being loaded. Before steps are executed, JTE will tell you which step is being run and what library contributed the step.","title":"Configure the Pipeline"},{"location":"tutorials/jte-the-basics/4-first-configuration-file/#configure-the-pipeline","text":"With the libraries created now discoverable by JTE, we can configure the pipeline and run it! From the main Jenkins page, click the job created earlier in this lab. In the left-hand navigation menu, click Configure . Scrolling down to the Pipeline portion of the job configuration, you should still see the Pipeline Template created earlier in the Jenkinsfile text box: build () static_code_analysis () It's now time to configure the pipeline by providing a Pipeline Configuration. Check the box for Provide pipeline configuration . In the Pipeline Configuration text box enter: libraries { maven sonarqube } Click Save . Important The libraries portion of the Pipeline Configuration file will read much like an application's technical stack. In this case, we're telling JTE during the initialization of the pipeline that it should load the maven and sonarqube libraries. The maven library will provide the build() step and the sonarqube library will provide the static_code_analysis() step. With these steps now implemented, we can run the pipeline.","title":"Configure the Pipeline"},{"location":"tutorials/jte-the-basics/4-first-configuration-file/#run-the-pipeline","text":"After clicking Save you'll be directed back to the main page for the job. To run the pipeline, click Build Now in the left-hand navigation menu. Refresh the page and you should see build number 1 in the Build History on the bottom left-hand side of the screen. Click the link to go to the Build's page. If you see a red X, something is wrong with your configuration. Look at the logs and see if they give any meaningful information; there may be something wrong with your library code or your branch may be wrong in the Library Sources configuration you set up earlier. In the left-hand navigation menu, click Console Output to view the build logs for this run of the pipeline. Started by user admin [JTE] Pipeline Configuration Modifications (show) [JTE] Obtained Pipeline Template from job configuration [JTE] Loading Library maven (show) [JTE] Loading Library sonarqube (show) [JTE] Template Primitives are overwriting Jenkins steps with the following names: [JTE] 1. build [Pipeline] Start of Pipeline [JTE][Step - maven/build.call()] [Pipeline] stage [Pipeline] { (Maven: Build) [Pipeline] echo build from the maven library [Pipeline] } [Pipeline] // stage [JTE][Step - sonarqube/static_code_analysis.call()] [Pipeline] stage [Pipeline] { (SonarQube: Static Code Analysis) [Pipeline] echo static code analysis from the sonarqube library [Pipeline] } [Pipeline] // stage [Pipeline] End of Pipeline Finished: SUCCESS As expected, JTE loaded the maven and sonarqube libraries and then executed the template. Note A couple points to note about the build log: Any line that starts with [JTE] is a log coming from the Jenkins Templating Engine. Pieces of JTE log output are hidden by default. Clicking show will expand these sections. The first part of the pipeline output shows the initialization process: Pipeline Configuration files being aggregated. Libraries being loaded. Before steps are executed, JTE will tell you which step is being run and what library contributed the step.","title":"Run the Pipeline"},{"location":"tutorials/jte-the-basics/5-swap-to-gradle/","text":"Swap Libraries \u00b6 The purpose of the Jenkins Templating Engine is three fold: Optimize pipeline code reuse . Now organizations can coalesce around a portfolio of centralized, reusable Pipeline Libraries representing different tool integrations for their CI/CD pipelines. Simplify pipeline maintainability . Separating a pipeline into templates, configuration files, and Pipeline Libraries can also be thought of as separating the business logic of your pipeline from the technical implementation . In our experience, it is significantly easier to manage a template backed by modularized Pipeline Libraries than it is to manage application-specific Jenkinsfiles. Provide organizational governance . With the traditional Jenkinsfile defined within, and duplicated across, source code repositories it can be very challenging to ensure the same process is being followed by disparate application development teams and confirm that organizational policies around code quality and security are being met. JTE brings a level of governance to your pipelines by centralizing the definition of the pipeline workflow to a common place. To demonstrate the reusability of pipeline templates, what would happen if the development team switched to using Gradle? All we would have to do is create a gradle library that implements the build() step of the pipeline template. Create the Gradle Library \u00b6 Remember that a library is just a subdirectory within the Base Directory configured as part of the Library Source in Jenkins. So to create the gradle library we should create a gradle directory under the libraries directory. Then, to implement the build() step for the pipeline, create a build.groovy file within the newly created gradle subdirectory. When you have created the new gradle directory and build.groovy file, your repository file structure will now be: . \u2514\u2500\u2500 libraries \u251c\u2500\u2500 gradle \u2502 \u2514\u2500\u2500 steps \u2502 \u2514\u2500\u2500 build.groovy \u251c\u2500\u2500 maven \u2502 \u2514\u2500\u2500 steps \u2502 \u2514\u2500\u2500 build.groovy \u2514\u2500\u2500 sonarqube \u2514\u2500\u2500 steps \u2514\u2500\u2500 static_code_analysis.groovy The contents of build.groovy should be: ./libraries/gradle/steps/build.groovy void call () { stage ( \"Gradle: Build\" ) { println \"build from the gradle library\" } } Important Please make sure you've pushed this change to the main/master branch of your library repository. Swap from Maven to Gradle \u00b6 Now that we have a modifiable implementation for the build() step of the pipeline template, switching from Maven to Gradle is as easy as changing the libraries listed in the Pipeline Configuration. Going back to the job configuration (click the single-job from Jenkins home page, then Configure ), in the Pipeline Configuration text box, swap the maven line to gradle and click Save . The Pipeline Configuration should now be: libraries { gradle sonarqube } Run the Pipeline \u00b6 Follow the same steps as before to run the job again and checkout the build logs ( Console Output after clicking the build number): Started by user admin [JTE] Pipeline Configuration Modifications (show) [JTE] Obtained Pipeline Template from job configuration [JTE] Loading Library gradle (show) [JTE] Loading Library sonarqube (show) [JTE] Template Primitives are overwriting Jenkins steps with the following names: (show) [Pipeline] Start of Pipeline [JTE][Step - gradle/build.call()] [Pipeline] stage [Pipeline] { (Gradle: Build) [Pipeline] echo build from the gradle library [Pipeline] } [Pipeline] // stage [JTE][Step - sonarqube/static_code_analysis.call()] [Pipeline] stage [Pipeline] { (SonarQube: Static Code Analysis) [Pipeline] echo static code analysis from the sonarqube library [Pipeline] } [Pipeline] // stage [Pipeline] End of Pipeline Finished: SUCCESS Congrats! You've demonstrated the value of being able to quickly substitute one library for another to fulfill the same step in the Pipeline Template.","title":"Swap Libraries"},{"location":"tutorials/jte-the-basics/5-swap-to-gradle/#swap-libraries","text":"The purpose of the Jenkins Templating Engine is three fold: Optimize pipeline code reuse . Now organizations can coalesce around a portfolio of centralized, reusable Pipeline Libraries representing different tool integrations for their CI/CD pipelines. Simplify pipeline maintainability . Separating a pipeline into templates, configuration files, and Pipeline Libraries can also be thought of as separating the business logic of your pipeline from the technical implementation . In our experience, it is significantly easier to manage a template backed by modularized Pipeline Libraries than it is to manage application-specific Jenkinsfiles. Provide organizational governance . With the traditional Jenkinsfile defined within, and duplicated across, source code repositories it can be very challenging to ensure the same process is being followed by disparate application development teams and confirm that organizational policies around code quality and security are being met. JTE brings a level of governance to your pipelines by centralizing the definition of the pipeline workflow to a common place. To demonstrate the reusability of pipeline templates, what would happen if the development team switched to using Gradle? All we would have to do is create a gradle library that implements the build() step of the pipeline template.","title":"Swap Libraries"},{"location":"tutorials/jte-the-basics/5-swap-to-gradle/#create-the-gradle-library","text":"Remember that a library is just a subdirectory within the Base Directory configured as part of the Library Source in Jenkins. So to create the gradle library we should create a gradle directory under the libraries directory. Then, to implement the build() step for the pipeline, create a build.groovy file within the newly created gradle subdirectory. When you have created the new gradle directory and build.groovy file, your repository file structure will now be: . \u2514\u2500\u2500 libraries \u251c\u2500\u2500 gradle \u2502 \u2514\u2500\u2500 steps \u2502 \u2514\u2500\u2500 build.groovy \u251c\u2500\u2500 maven \u2502 \u2514\u2500\u2500 steps \u2502 \u2514\u2500\u2500 build.groovy \u2514\u2500\u2500 sonarqube \u2514\u2500\u2500 steps \u2514\u2500\u2500 static_code_analysis.groovy The contents of build.groovy should be: ./libraries/gradle/steps/build.groovy void call () { stage ( \"Gradle: Build\" ) { println \"build from the gradle library\" } } Important Please make sure you've pushed this change to the main/master branch of your library repository.","title":"Create the Gradle Library"},{"location":"tutorials/jte-the-basics/5-swap-to-gradle/#swap-from-maven-to-gradle","text":"Now that we have a modifiable implementation for the build() step of the pipeline template, switching from Maven to Gradle is as easy as changing the libraries listed in the Pipeline Configuration. Going back to the job configuration (click the single-job from Jenkins home page, then Configure ), in the Pipeline Configuration text box, swap the maven line to gradle and click Save . The Pipeline Configuration should now be: libraries { gradle sonarqube }","title":"Swap from Maven to Gradle"},{"location":"tutorials/jte-the-basics/5-swap-to-gradle/#run-the-pipeline","text":"Follow the same steps as before to run the job again and checkout the build logs ( Console Output after clicking the build number): Started by user admin [JTE] Pipeline Configuration Modifications (show) [JTE] Obtained Pipeline Template from job configuration [JTE] Loading Library gradle (show) [JTE] Loading Library sonarqube (show) [JTE] Template Primitives are overwriting Jenkins steps with the following names: (show) [Pipeline] Start of Pipeline [JTE][Step - gradle/build.call()] [Pipeline] stage [Pipeline] { (Gradle: Build) [Pipeline] echo build from the gradle library [Pipeline] } [Pipeline] // stage [JTE][Step - sonarqube/static_code_analysis.call()] [Pipeline] stage [Pipeline] { (SonarQube: Static Code Analysis) [Pipeline] echo static code analysis from the sonarqube library [Pipeline] } [Pipeline] // stage [Pipeline] End of Pipeline Finished: SUCCESS Congrats! You've demonstrated the value of being able to quickly substitute one library for another to fulfill the same step in the Pipeline Template.","title":"Run the Pipeline"},{"location":"tutorials/jte-the-basics/6-multibranch/","text":"Apply to a GitHub Repository \u00b6 So far we've learned: What a Pipeline Template is (the business logic of your pipeline). How to create some mock Pipeline Libraries (just Groovy files implementing a call() method inside a directory in a repository). What the Pipeline Configuration does (defines libraries to implement the template so it actually does things). How to use the same pipeline template with two different tech stacks by modifying the Pipeline Configuration. Next, we're going to learn how to apply a Pipeline Template to an entire GitHub repository. This is a more realistic scenario and it has the added benefit of taking the Pipeline Template and Pipeline Configuration file out of the Jenkins UI and storing them in a Pipeline Configuration repository. Move the Pipeline Template to a Repository \u00b6 When creating libraries, we created a GitHub JTE library repository and stored the libraries in a subdirectory called libraries . In this example, we can create a new subdirectory at the root of the repository called pipeline-configuration . Note The actual names of the libraries and pipeline-configuration subdirectories don't matter and are configurable, the convention is to keep them named like this, however. Within this pipeline-configuration directory create a file called Jenkinsfile and populate it with the same contents as the Pipeline Template text box in the Jenkins UI. ./pipeline-configuration/Jenkinsfile build () static_code_analysis () Important The Jenkinsfile is the default Pipeline Template that will be used. It is possible to define more than one Pipeline Template and let application teams select which template applies to them. More on that later, or read the doc on template selection . Move the Pipeline Configuration to a Repository \u00b6 In the same pipeline-configuration directory, create a file called pipeline_config.groovy . Important When the Pipeline Configuration is stored in a file in a source code repository, it will always be called pipeline_config.groovy . Populate this file with the same contents as the Pipeline Configuration text box in the Jenkins UI for the single-job you created and updated earlier: ./pipeline-configuration/pipeline_config.groovy libraries { gradle sonarqube } The file structure in your GitHub repository should now look like this: . \u251c\u2500\u2500 libraries \u2502 \u251c\u2500\u2500 gradle \u2502 \u2502 \u2514\u2500\u2500 steps \u2502 \u2502 \u2514\u2500\u2500 build.groovy \u2502 \u251c\u2500\u2500 maven \u2502 \u2502 \u2514\u2500\u2500 steps \u2502 \u2502 \u2514\u2500\u2500 build.groovy \u2502 \u2514\u2500\u2500 sonarqube \u2502 \u2514\u2500\u2500 steps \u2502 \u2514\u2500\u2500 static_code_analysis.groovy \u2514\u2500\u2500 pipeline-configuration \u251c\u2500\u2500 Jenkinsfile \u2514\u2500\u2500 pipeline_config.groovy Create the Global Governance Tier \u00b6 Now that we have our template and Pipeline Configuration externalized into a source code repository, we have to tell Jenkins where to find it. From the Jenkins home page: In the left-hand navigation menu click Manage Jenkins . Click Configure System . Scroll down to the Jenkins Templating Engine configuration section. Under Pipeline Configuration select From SCM . Select Git for the Source Location drop down menu. Under Repository URL type the https URL of the GitHub repository containing the libraries, template, and configuration file. In the Credentials drop down menu, select the github credential created during the prerequisites. Under Branches to build you may have to specify */main , */master or something else. Type pipeline-configuration in the Configuration Base Directory text box. Click Save . Note You just configured your first Governance Tier ! Governance Tiers are the combination of: A Pipeline Configuration repository specifying where the Pipeline Configuration file and Pipeline Templates can be found. A set of library sources. When done in Manage Jenkins > Configure System it's called the Global Governance Tier and applies to every job on the Jenkins instance. Governance Tiers can also be configured for every Folder in Jenkins. When configured, they apply to every Job within that Folder. For more on Jenkins Folders: https://plugins.jenkins.io/cloudbees-folder/. Through Governance Tiers, you can create a governance hierarchy that matches your organizational hierarchy just by how you organize jobs within Jenkins. Create an Application Repository \u00b6 We're going to apply the Pipeline Template and configuration file to every branch in a GitHub repository. Create a GitHub Repository that will serve as our mock application repository named jte-the-basics-app-gradle . Initialize the Repository with a README file. Modify the README in order to create a branch called test . Push the new branch. Consult Git documentation on how to create and push a branch if you don't know how, or follow the older guide GIF below: Create a Multibranch Project \u00b6 Now that we have a GitHub repository representing our application, we can create a Multibranch Project in Jenkins. Important Multibranch Projects are Folders in Jenkins that automatically create pipeline jobs for every branch and Pull Request in the source code repository they represent. Through JTE, we can configure each branch and Pull Request to use the same Pipeline Template. This removes the need for a per-repository Jenkinsfile. From the Jenkins home page, select New Item in the left-hand navigation menu. In the Enter an item name text box, type gradle-app . Select Multibranch Pipeline as the job type. Click OK to create the job. Under Branch Sources > Add Source select GitHub . Select your github credential under the Credentials drop down menu. Enter the https repository URL of jte-the-basics-app-gradle under Repository HTTPS URL . Under Build Configuration select Jenkins Templating Engine from the Mode drop-down menu. Click Save . When the job is created, you will be redirected to a page showing the logs for scanning the repository. In the breadcrumbs at the top of the page, you can select gradle-app to see the branch overview. In this overview, you'll see two jobs in progress once the repository scan has repeated: a job for the main branch and a job for the test branch. When these jobs complete, clicking them will show that each branch executed the Pipeline Template with the same configuration.","title":"Apply to a GitHub Repository"},{"location":"tutorials/jte-the-basics/6-multibranch/#apply-to-a-github-repository","text":"So far we've learned: What a Pipeline Template is (the business logic of your pipeline). How to create some mock Pipeline Libraries (just Groovy files implementing a call() method inside a directory in a repository). What the Pipeline Configuration does (defines libraries to implement the template so it actually does things). How to use the same pipeline template with two different tech stacks by modifying the Pipeline Configuration. Next, we're going to learn how to apply a Pipeline Template to an entire GitHub repository. This is a more realistic scenario and it has the added benefit of taking the Pipeline Template and Pipeline Configuration file out of the Jenkins UI and storing them in a Pipeline Configuration repository.","title":"Apply to a GitHub Repository"},{"location":"tutorials/jte-the-basics/6-multibranch/#move-the-pipeline-template-to-a-repository","text":"When creating libraries, we created a GitHub JTE library repository and stored the libraries in a subdirectory called libraries . In this example, we can create a new subdirectory at the root of the repository called pipeline-configuration . Note The actual names of the libraries and pipeline-configuration subdirectories don't matter and are configurable, the convention is to keep them named like this, however. Within this pipeline-configuration directory create a file called Jenkinsfile and populate it with the same contents as the Pipeline Template text box in the Jenkins UI. ./pipeline-configuration/Jenkinsfile build () static_code_analysis () Important The Jenkinsfile is the default Pipeline Template that will be used. It is possible to define more than one Pipeline Template and let application teams select which template applies to them. More on that later, or read the doc on template selection .","title":"Move the Pipeline Template to a Repository"},{"location":"tutorials/jte-the-basics/6-multibranch/#move-the-pipeline-configuration-to-a-repository","text":"In the same pipeline-configuration directory, create a file called pipeline_config.groovy . Important When the Pipeline Configuration is stored in a file in a source code repository, it will always be called pipeline_config.groovy . Populate this file with the same contents as the Pipeline Configuration text box in the Jenkins UI for the single-job you created and updated earlier: ./pipeline-configuration/pipeline_config.groovy libraries { gradle sonarqube } The file structure in your GitHub repository should now look like this: . \u251c\u2500\u2500 libraries \u2502 \u251c\u2500\u2500 gradle \u2502 \u2502 \u2514\u2500\u2500 steps \u2502 \u2502 \u2514\u2500\u2500 build.groovy \u2502 \u251c\u2500\u2500 maven \u2502 \u2502 \u2514\u2500\u2500 steps \u2502 \u2502 \u2514\u2500\u2500 build.groovy \u2502 \u2514\u2500\u2500 sonarqube \u2502 \u2514\u2500\u2500 steps \u2502 \u2514\u2500\u2500 static_code_analysis.groovy \u2514\u2500\u2500 pipeline-configuration \u251c\u2500\u2500 Jenkinsfile \u2514\u2500\u2500 pipeline_config.groovy","title":"Move the Pipeline Configuration to a Repository"},{"location":"tutorials/jte-the-basics/6-multibranch/#create-the-global-governance-tier","text":"Now that we have our template and Pipeline Configuration externalized into a source code repository, we have to tell Jenkins where to find it. From the Jenkins home page: In the left-hand navigation menu click Manage Jenkins . Click Configure System . Scroll down to the Jenkins Templating Engine configuration section. Under Pipeline Configuration select From SCM . Select Git for the Source Location drop down menu. Under Repository URL type the https URL of the GitHub repository containing the libraries, template, and configuration file. In the Credentials drop down menu, select the github credential created during the prerequisites. Under Branches to build you may have to specify */main , */master or something else. Type pipeline-configuration in the Configuration Base Directory text box. Click Save . Note You just configured your first Governance Tier ! Governance Tiers are the combination of: A Pipeline Configuration repository specifying where the Pipeline Configuration file and Pipeline Templates can be found. A set of library sources. When done in Manage Jenkins > Configure System it's called the Global Governance Tier and applies to every job on the Jenkins instance. Governance Tiers can also be configured for every Folder in Jenkins. When configured, they apply to every Job within that Folder. For more on Jenkins Folders: https://plugins.jenkins.io/cloudbees-folder/. Through Governance Tiers, you can create a governance hierarchy that matches your organizational hierarchy just by how you organize jobs within Jenkins.","title":"Create the Global Governance Tier"},{"location":"tutorials/jte-the-basics/6-multibranch/#create-an-application-repository","text":"We're going to apply the Pipeline Template and configuration file to every branch in a GitHub repository. Create a GitHub Repository that will serve as our mock application repository named jte-the-basics-app-gradle . Initialize the Repository with a README file. Modify the README in order to create a branch called test . Push the new branch. Consult Git documentation on how to create and push a branch if you don't know how, or follow the older guide GIF below:","title":"Create an Application Repository"},{"location":"tutorials/jte-the-basics/6-multibranch/#create-a-multibranch-project","text":"Now that we have a GitHub repository representing our application, we can create a Multibranch Project in Jenkins. Important Multibranch Projects are Folders in Jenkins that automatically create pipeline jobs for every branch and Pull Request in the source code repository they represent. Through JTE, we can configure each branch and Pull Request to use the same Pipeline Template. This removes the need for a per-repository Jenkinsfile. From the Jenkins home page, select New Item in the left-hand navigation menu. In the Enter an item name text box, type gradle-app . Select Multibranch Pipeline as the job type. Click OK to create the job. Under Branch Sources > Add Source select GitHub . Select your github credential under the Credentials drop down menu. Enter the https repository URL of jte-the-basics-app-gradle under Repository HTTPS URL . Under Build Configuration select Jenkins Templating Engine from the Mode drop-down menu. Click Save . When the job is created, you will be redirected to a page showing the logs for scanning the repository. In the breadcrumbs at the top of the page, you can select gradle-app to see the branch overview. In this overview, you'll see two jobs in progress once the repository scan has repeated: a job for the main branch and a job for the test branch. When these jobs complete, clicking them will show that each branch executed the Pipeline Template with the same configuration.","title":"Create a Multibranch Project"},{"location":"tutorials/jte-the-basics/7-github-org/","text":"Apply to Multiple GitHub Repositories \u00b6 Next up, we'll see how to apply the same template to two different applications that are using different tools. Let's walk through how you would setup JTE to this situation where one application is using Maven, the other application is using Gradle, and both applications are using SonarQube. Create a New GitHub Repository \u00b6 We already have a Pipeline Configuration repository that has our Pipeline Template, Pipeline Configuration file, and Pipeline Libraries as well as a repository that represents an application using Gradle. Follow the same procedure as before when creating the gradle application's repository to create a Maven application repository named jte-the-basics-app-maven . Modify the Pipeline Configurations \u00b6 Now that there are multiple applications with some configurations that are common and some configurations that are unique, we need to introduce the concept of Pipeline Configuration aggregation. Modify the Governance Tier Configuration File \u00b6 We need to edit the Pipeline Configuration file ( pipeline_config.groovy ) we created earlier to represent the common configurations that will be applied to both apps and explicitly allow these applications to add their own configurations. Update the pipeline_config.groovy file in your library repository to this: ./pipeline-configuration/pipeline_config.groovy @merge libraries { sonarqube } Note In this configuration, both applications are using SonarQube. Since this is a common configuration, we'll leave it in the Governance Tier's configuration file. The application repositories will each get their own pipeline_config.groovy to indicate if they are using the maven or the gradle library. Since we want to allow these apps to add additional configurations, we need to be explicit about that by annotating the libraries block with @merge in the Pipeline Configuration. Push this change to the main/master branch of your library repository. Important When aggregating Pipeline Configurations, JTE applies conditional inheritance during the aggregation process. Create a Pipeline Configuration File for the Maven Application \u00b6 In the jte-the-basics-app-maven repository we just created, add a pipeline_config.groovy file at the root that specifies you want to load the maven library: jte-the-basics-app-maven/pipeline_config.groovy libraries { maven } Note Since this application will inherit the global configurations defined in the Governance Tier, we don't have to duplicate the configuration of loading the sonarqube library in the repo-level Pipeline Configuration. Create a Pipeline Configuration File for the Gradle Application \u00b6 In the jte-the-basics-app-gradle repository we created earlier for the Multibranch Project, add a pipeline_config.groovy file at the root that specifies you want to load the gradle library: jte-the-basics-app-gradle/pipeline_config.groovy libraries { gradle } Important Push both changes to both repos in the main/master branch. You can delete the test branch you created in the Gradle repo earlier. Create a GitHub Organization Project \u00b6 We created a Multibranch Project that automatically created jobs for every branch and Pull Request in a repository. Now, we'll create a GitHub Organization Project that can automatically create Multibranch Projects for every repository within a GitHub Organization. From the Jenkins home page, select New Item in the left-hand navigation menu. Select a name for the job: example-org , it will be renamed with your organization (your username). Select Organization Folder then click OK . Under Repository Sources , add a GitHub Organization . Enter https://api.github.com as the API endpoint, or leave it blank if you can't fill it in. Select the github credential under the Credentials drop-down menu. Enter your GitHub username under the Owner field. Under Behaviors click Add then under Repositories (not to be confused with Within Repositories ), select Filter by name (with wildcards) . Enter jte-the-basics-app-* in the Include text box (assuming you've following the naming recommendations of the application repositories). Under Project Recognizers hit the red X to delete the Pipeline Jenkinsfile Recognizer. Under Project Recognizers select Add and click Jenkins Templating Engine . Click Save . After creating the GitHub Organization job in Jenkins, you will be redirected to the logs of the GitHub Organization being scanned to find repositories that match the wildcard format entered during job creation. This will scope the repositories for which jobs are created to just this lab's application repositories. Once scanning has finished, go view the GitHub Organization's job page in Jenkins and you will see two Multibranch Projects have been created for jte-the-basics-app-gradle and jte-the-basics-app-maven . Explore each of these jobs to see that the gradle repository's pipeline loaded the gradle library and the maven repository loaded the maven library and both pipelines loaded the sonarqube library. Important We just created a configuration where multiple applications used the same pipeline template, shared a common configuration, but still have the flexibility to choose the correct build tool for their application!","title":"Apply to Multiple GitHub Repositories"},{"location":"tutorials/jte-the-basics/7-github-org/#apply-to-multiple-github-repositories","text":"Next up, we'll see how to apply the same template to two different applications that are using different tools. Let's walk through how you would setup JTE to this situation where one application is using Maven, the other application is using Gradle, and both applications are using SonarQube.","title":"Apply to Multiple GitHub Repositories"},{"location":"tutorials/jte-the-basics/7-github-org/#create-a-new-github-repository","text":"We already have a Pipeline Configuration repository that has our Pipeline Template, Pipeline Configuration file, and Pipeline Libraries as well as a repository that represents an application using Gradle. Follow the same procedure as before when creating the gradle application's repository to create a Maven application repository named jte-the-basics-app-maven .","title":"Create a New GitHub Repository"},{"location":"tutorials/jte-the-basics/7-github-org/#modify-the-pipeline-configurations","text":"Now that there are multiple applications with some configurations that are common and some configurations that are unique, we need to introduce the concept of Pipeline Configuration aggregation.","title":"Modify the Pipeline Configurations"},{"location":"tutorials/jte-the-basics/7-github-org/#modify-the-governance-tier-configuration-file","text":"We need to edit the Pipeline Configuration file ( pipeline_config.groovy ) we created earlier to represent the common configurations that will be applied to both apps and explicitly allow these applications to add their own configurations. Update the pipeline_config.groovy file in your library repository to this: ./pipeline-configuration/pipeline_config.groovy @merge libraries { sonarqube } Note In this configuration, both applications are using SonarQube. Since this is a common configuration, we'll leave it in the Governance Tier's configuration file. The application repositories will each get their own pipeline_config.groovy to indicate if they are using the maven or the gradle library. Since we want to allow these apps to add additional configurations, we need to be explicit about that by annotating the libraries block with @merge in the Pipeline Configuration. Push this change to the main/master branch of your library repository. Important When aggregating Pipeline Configurations, JTE applies conditional inheritance during the aggregation process.","title":"Modify the Governance Tier Configuration File"},{"location":"tutorials/jte-the-basics/7-github-org/#create-a-pipeline-configuration-file-for-the-maven-application","text":"In the jte-the-basics-app-maven repository we just created, add a pipeline_config.groovy file at the root that specifies you want to load the maven library: jte-the-basics-app-maven/pipeline_config.groovy libraries { maven } Note Since this application will inherit the global configurations defined in the Governance Tier, we don't have to duplicate the configuration of loading the sonarqube library in the repo-level Pipeline Configuration.","title":"Create a Pipeline Configuration File for the Maven Application"},{"location":"tutorials/jte-the-basics/7-github-org/#create-a-pipeline-configuration-file-for-the-gradle-application","text":"In the jte-the-basics-app-gradle repository we created earlier for the Multibranch Project, add a pipeline_config.groovy file at the root that specifies you want to load the gradle library: jte-the-basics-app-gradle/pipeline_config.groovy libraries { gradle } Important Push both changes to both repos in the main/master branch. You can delete the test branch you created in the Gradle repo earlier.","title":"Create a Pipeline Configuration File for the Gradle Application"},{"location":"tutorials/jte-the-basics/7-github-org/#create-a-github-organization-project","text":"We created a Multibranch Project that automatically created jobs for every branch and Pull Request in a repository. Now, we'll create a GitHub Organization Project that can automatically create Multibranch Projects for every repository within a GitHub Organization. From the Jenkins home page, select New Item in the left-hand navigation menu. Select a name for the job: example-org , it will be renamed with your organization (your username). Select Organization Folder then click OK . Under Repository Sources , add a GitHub Organization . Enter https://api.github.com as the API endpoint, or leave it blank if you can't fill it in. Select the github credential under the Credentials drop-down menu. Enter your GitHub username under the Owner field. Under Behaviors click Add then under Repositories (not to be confused with Within Repositories ), select Filter by name (with wildcards) . Enter jte-the-basics-app-* in the Include text box (assuming you've following the naming recommendations of the application repositories). Under Project Recognizers hit the red X to delete the Pipeline Jenkinsfile Recognizer. Under Project Recognizers select Add and click Jenkins Templating Engine . Click Save . After creating the GitHub Organization job in Jenkins, you will be redirected to the logs of the GitHub Organization being scanned to find repositories that match the wildcard format entered during job creation. This will scope the repositories for which jobs are created to just this lab's application repositories. Once scanning has finished, go view the GitHub Organization's job page in Jenkins and you will see two Multibranch Projects have been created for jte-the-basics-app-gradle and jte-the-basics-app-maven . Explore each of these jobs to see that the gradle repository's pipeline loaded the gradle library and the maven repository loaded the maven library and both pipelines loaded the sonarqube library. Important We just created a configuration where multiple applications used the same pipeline template, shared a common configuration, but still have the flexibility to choose the correct build tool for their application!","title":"Create a GitHub Organization Project"},{"location":"tutorials/jte-the-basics/8-summary/","text":"Summary \u00b6 We learned a lot in this lab! Let's recap some of what we learned: GitHub Credentials in Jenkins \u00b6 We learned how to create a Personal Access Token and store it in the Jenkins credential store. While credentials aren't strictly needed for public repositories, GitHub will rate limit your Jenkins instance's API requests, which can dramatically slow down the pipeline and cause it to fail. Different Types of Jenkins Jobs \u00b6 We learned about the three kinds of Jenkins Jobs most commonly used when working with the Jenkins Templating Engine: Job Type Description Pipeline Job Best suited for one-off tasks or debugging pipelines developed with JTE. Multibranch Projects Represent an entire GitHub repository and create a job for every branch and Pull Request. GitHub Organization Folder Represent an entire GitHub Organization. Can be filtered to restrict which repositories are automatically represented in Jenkins. What makes up a pipeline in JTE? \u00b6 We learned that: Pipeline Templates can call steps contributed by libraries . Pipeline Templates are responsible for the business logic of your pipeline. Libraries are responsible for the technical implementation of your pipeline. Pipeline Configuration Files implement a template by specifying (among other things) what libraries to load. When stored in a repo, Pipeline Configuration files are named pipeline_config.groovy and are located at the root of the repository. What is a Governance Tier? \u00b6 We learned that: Governance Tiers are a way to externalize configuration into source code repositories. A Governance Tier is made up of a Pipeline Configuration repository and a set of Library Sources. Pipeline Configuration repositories optionally contain a pipeline template and a Pipeline Configuration file. The Jenkinsfile is the default pipeline template in a Governance Tier. How can we reuse Pipeline Templates? \u00b6 We learned that: Pipeline Templates can be applied to multiple repositories simultaneously through the GitHub Organization Job Type. Pipeline Configuration files can be aggregated between Governance Tiers and an application repository itself. There are rules around conditional inheritance when it comes to Pipeline Configuration aggregation.","title":"Summary"},{"location":"tutorials/jte-the-basics/8-summary/#summary","text":"We learned a lot in this lab! Let's recap some of what we learned:","title":"Summary"},{"location":"tutorials/jte-the-basics/8-summary/#github-credentials-in-jenkins","text":"We learned how to create a Personal Access Token and store it in the Jenkins credential store. While credentials aren't strictly needed for public repositories, GitHub will rate limit your Jenkins instance's API requests, which can dramatically slow down the pipeline and cause it to fail.","title":"GitHub Credentials in Jenkins"},{"location":"tutorials/jte-the-basics/8-summary/#different-types-of-jenkins-jobs","text":"We learned about the three kinds of Jenkins Jobs most commonly used when working with the Jenkins Templating Engine: Job Type Description Pipeline Job Best suited for one-off tasks or debugging pipelines developed with JTE. Multibranch Projects Represent an entire GitHub repository and create a job for every branch and Pull Request. GitHub Organization Folder Represent an entire GitHub Organization. Can be filtered to restrict which repositories are automatically represented in Jenkins.","title":"Different Types of Jenkins Jobs"},{"location":"tutorials/jte-the-basics/8-summary/#what-makes-up-a-pipeline-in-jte","text":"We learned that: Pipeline Templates can call steps contributed by libraries . Pipeline Templates are responsible for the business logic of your pipeline. Libraries are responsible for the technical implementation of your pipeline. Pipeline Configuration Files implement a template by specifying (among other things) what libraries to load. When stored in a repo, Pipeline Configuration files are named pipeline_config.groovy and are located at the root of the repository.","title":"What makes up a pipeline in JTE?"},{"location":"tutorials/jte-the-basics/8-summary/#what-is-a-governance-tier","text":"We learned that: Governance Tiers are a way to externalize configuration into source code repositories. A Governance Tier is made up of a Pipeline Configuration repository and a set of Library Sources. Pipeline Configuration repositories optionally contain a pipeline template and a Pipeline Configuration file. The Jenkinsfile is the default pipeline template in a Governance Tier.","title":"What is a Governance Tier?"},{"location":"tutorials/jte-the-basics/8-summary/#how-can-we-reuse-pipeline-templates","text":"We learned that: Pipeline Templates can be applied to multiple repositories simultaneously through the GitHub Organization Job Type. Pipeline Configuration files can be aggregated between Governance Tiers and an application repository itself. There are rules around conditional inheritance when it comes to Pipeline Configuration aggregation.","title":"How can we reuse Pipeline Templates?"},{"location":"tutorials/local-development/","text":"Local Development \u00b6 What You'll Learn \u00b6 How to deploy a Jenkins instance on your local machine using Docker that can: Run containers as part of a CI/CD pipeline. Leverage the Jenkins Templating Engine.","title":"Local Development"},{"location":"tutorials/local-development/#local-development","text":"","title":"Local Development"},{"location":"tutorials/local-development/#what-youll-learn","text":"How to deploy a Jenkins instance on your local machine using Docker that can: Run containers as part of a CI/CD pipeline. Leverage the Jenkins Templating Engine.","title":"What You'll Learn"},{"location":"tutorials/local-development/1-prerequisites/","text":"Prerequisites \u00b6 Docker \u00b6 Ensure Docker is installed and running on your local machine. You can validate the Docker process is running by running docker ps in your terminal. You should see output similar to: Internet Connectivity \u00b6 The Jenkins LTS container image from Docker Hub will be used in this course. You can validate your ability to download this container image by running docker pull jenkins/jenkins:lts . You should see output similar to:","title":"Prerequisites"},{"location":"tutorials/local-development/1-prerequisites/#prerequisites","text":"","title":"Prerequisites"},{"location":"tutorials/local-development/1-prerequisites/#docker","text":"Ensure Docker is installed and running on your local machine. You can validate the Docker process is running by running docker ps in your terminal. You should see output similar to:","title":"Docker"},{"location":"tutorials/local-development/1-prerequisites/#internet-connectivity","text":"The Jenkins LTS container image from Docker Hub will be used in this course. You can validate your ability to download this container image by running docker pull jenkins/jenkins:lts . You should see output similar to:","title":"Internet Connectivity"},{"location":"tutorials/local-development/2-run-jenkins/","text":"Running the Jenkins Container \u00b6 Docker simplifies packaging applications with all their dependencies. Through Docker, we can have a running Jenkins instance in a matter of seconds. In your terminal, first build a Jenkins image with docker installed. Create a Dockerfile in an empty directory with the following: FROM jenkins/jenkins:lts USER root RUN apt-get update && apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common RUN curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add RUN add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/debian \\ $(lsb_release -cs) \\ stable\" RUN apt-get update && apt-get install -y docker-ce docker-ce-cli containerd.io EXPOSE 8080 From the same directory as your Dockerfile, build the image: docker build -t jenkins:lts-docker . Note Running Jenkins as a docker container on a BAH-managed machine requires specific certificates and tooling to be installed. A Dockerfile meeting these requirements can be built from the solutions-delivery-platform/bah-jenkins repository. Then, to start Jenkins, run: On Linux or Mac: docker run --name jenkins \\ -v /var/run/docker.sock:/var/run/docker.sock \\ --privileged \\ --user root \\ -p 50000:50000 \\ -p 8080:8080 \\ -d \\ jenkins:lts-docker On Windows: docker run --name jenkins \\ -v //var/run/docker.sock:/var/run/docker.sock \\ --privileged \\ --user root \\ -p 50000:50000 \\ -p 8080:8080 \\ -d \\ jenkins:lts-docker Command Line Breakdown \u00b6 Command Section Description docker run Tells Docker to run a command in a new container. --name jenkins Names the container being launched jenkins . This is done for ease of referencing it later. -v /var/run/docker.sock:/var/run/docker.sock Mounts the local Docker daemon socket to the Jenkins container. \u2013-privileged Escalates the container permissions so it can launch containers on the host docker daemon. \u2013-user root Runs the container as the root user so it can launch containers on the host docker daemon. -p 50000:50000 Port forwarding of the default JNLP agent port to our localhost. -p 8080:8080 Port forwarding of the Jenkins port to our localhost. -d Runs the container process in the background. jenkins:lts-docker The container image from which to run this container. Note If port 8080 is already in use by another process then this command will fail. To run Jenkins on a different port, swap out the first 8080 to your desired port: <desired port number>:8080 . You can run docker logs -f jenkins to see the Jenkins logs. It will say \"Jenkins is fully up and running\" when Jenkins is ready. You can validate the container launched as expected by going to http://localhost:8080 . You should see the Jenkins Startup Wizard: In the next section, we'll learn how to get past this Startup Wizard and configure the newly deployed Jenkins instance.","title":"Running the Jenkins Container"},{"location":"tutorials/local-development/2-run-jenkins/#running-the-jenkins-container","text":"Docker simplifies packaging applications with all their dependencies. Through Docker, we can have a running Jenkins instance in a matter of seconds. In your terminal, first build a Jenkins image with docker installed. Create a Dockerfile in an empty directory with the following: FROM jenkins/jenkins:lts USER root RUN apt-get update && apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common RUN curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add RUN add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/debian \\ $(lsb_release -cs) \\ stable\" RUN apt-get update && apt-get install -y docker-ce docker-ce-cli containerd.io EXPOSE 8080 From the same directory as your Dockerfile, build the image: docker build -t jenkins:lts-docker . Note Running Jenkins as a docker container on a BAH-managed machine requires specific certificates and tooling to be installed. A Dockerfile meeting these requirements can be built from the solutions-delivery-platform/bah-jenkins repository. Then, to start Jenkins, run: On Linux or Mac: docker run --name jenkins \\ -v /var/run/docker.sock:/var/run/docker.sock \\ --privileged \\ --user root \\ -p 50000:50000 \\ -p 8080:8080 \\ -d \\ jenkins:lts-docker On Windows: docker run --name jenkins \\ -v //var/run/docker.sock:/var/run/docker.sock \\ --privileged \\ --user root \\ -p 50000:50000 \\ -p 8080:8080 \\ -d \\ jenkins:lts-docker","title":"Running the Jenkins Container"},{"location":"tutorials/local-development/2-run-jenkins/#command-line-breakdown","text":"Command Section Description docker run Tells Docker to run a command in a new container. --name jenkins Names the container being launched jenkins . This is done for ease of referencing it later. -v /var/run/docker.sock:/var/run/docker.sock Mounts the local Docker daemon socket to the Jenkins container. \u2013-privileged Escalates the container permissions so it can launch containers on the host docker daemon. \u2013-user root Runs the container as the root user so it can launch containers on the host docker daemon. -p 50000:50000 Port forwarding of the default JNLP agent port to our localhost. -p 8080:8080 Port forwarding of the Jenkins port to our localhost. -d Runs the container process in the background. jenkins:lts-docker The container image from which to run this container. Note If port 8080 is already in use by another process then this command will fail. To run Jenkins on a different port, swap out the first 8080 to your desired port: <desired port number>:8080 . You can run docker logs -f jenkins to see the Jenkins logs. It will say \"Jenkins is fully up and running\" when Jenkins is ready. You can validate the container launched as expected by going to http://localhost:8080 . You should see the Jenkins Startup Wizard: In the next section, we'll learn how to get past this Startup Wizard and configure the newly deployed Jenkins instance.","title":"Command Line Breakdown"},{"location":"tutorials/local-development/3-configure-jenkins/","text":"Configuring Jenkins \u00b6 In the last section, we ran a local Jenkins instance via Docker and validated that it's running on http://localhost:8080 . Now, we're going to configure that Jenkins instance by: Entering the initial admin password Installing the default suggested plugins Installing the Jenkins Templating Engine Installing the Docker Pipeline plugin Initial Admin Password \u00b6 There are two ways to get the initial admin password for Jenkins. 1. From the initialAdminPassword file \u00b6 The initial admin password is stored in /var/jenkins_home/secrets/initialAdminPassword within the container. You can print this password in your terminal by running: docker exec jenkins cat /var/jenkins_home/secrets/initialAdminPassword . You should see something similar to this: Copy and paste this password into the Administrator password text box in Jenkins. 2. From the container log output \u00b6 The initial admin password is also printed to standard out while Jenkins starts up. To view the Jenkins logs, run docker logs jenkins . In the output, you should see something similar to this: Copy and paste the password into the text box in Jenkins. Installing the Suggested Plugins \u00b6 After entering the initial admin password, Jenkins will bring you to a Customize Jenkins page. Click the Install suggested plugins button. This will bring you to a loading screen displaying the progress as Jenkins installs the most popular community plugins. The process takes less than five minutes. Setup Initial Admin User \u00b6 After the plugins are done installing, Jenkins will send you to a screen to configure the default admin user: Feel free to create a custom username and password or continue as admin; no one will use this test Jenkins installation except you. Important If you click \"continue as admin\" then the username will be admin and the password will be the initial admin password we found earlier. If you change your admin user/password to something else you will need to remember it. Instance Configuration \u00b6 After creating the initial admin user, Jenkins will send you to a screen where you can configure the instance's URL. The text box will be pre-populated with what's currently in your browser, so click Save and Finish in the bottom right-hand side of the screen. Then click \" Start Using Jenkins \" and you will be directed to the Jenkins home page: Installing the Jenkins Templating Engine \u00b6 At this point, you've completed the Jenkins Startup Wizard process. Now, we're going to install the Jenkins Templating Engine, which can be found as the Templating Engine Plugin in the Plugin Manager. In the left-hand navigation menu, select Manage Jenkins . In the middle of the screen, select Manage Plugins . In the left-hand navigation menu, select Available plugins . In the Search available plugins text box, type: Templating Engine At this point you should see: Make sure to select the Templating Engine checkbox and click the \" Download now and install after restart \" button. This will direct you to a screen showing the download progress of JTE. Scroll to the bottom of the Download progress screen and select \" Restart Jenkins when installation is complete and no jobs are running .\" At this point, Jenkins will restart automatically. Log in again with either the custom admin user you created earlier or the initial admin password. Important You can run docker logs -f jenkins to see the Jenkins logs. It will say \"Jenkins is fully up and running\" somewhere in the logs (with a timestamp) when Jenkins has completed the restart and is ready to be interacted with. Installing the Docker Pipeline plugin \u00b6 Now, we need to install the Docker Pipeline plugin, which can be found as the Docker Pipeline in the Plugin Manager. In the left-hand navigation menu, select Manage Jenkins . In the middle of the screen, select Manage Plugins . In the left-hand navigation menu, select Available plugins . In the Search available plugins text box, type: Docker Pipeline Follow the same steps used for installing the Jenkins Templating Engine and restart the Jenkins instance.","title":"Configuring Jenkins"},{"location":"tutorials/local-development/3-configure-jenkins/#configuring-jenkins","text":"In the last section, we ran a local Jenkins instance via Docker and validated that it's running on http://localhost:8080 . Now, we're going to configure that Jenkins instance by: Entering the initial admin password Installing the default suggested plugins Installing the Jenkins Templating Engine Installing the Docker Pipeline plugin","title":"Configuring Jenkins"},{"location":"tutorials/local-development/3-configure-jenkins/#initial-admin-password","text":"There are two ways to get the initial admin password for Jenkins.","title":"Initial Admin Password"},{"location":"tutorials/local-development/3-configure-jenkins/#1-from-the-initialadminpassword-file","text":"The initial admin password is stored in /var/jenkins_home/secrets/initialAdminPassword within the container. You can print this password in your terminal by running: docker exec jenkins cat /var/jenkins_home/secrets/initialAdminPassword . You should see something similar to this: Copy and paste this password into the Administrator password text box in Jenkins.","title":"1. From the initialAdminPassword file"},{"location":"tutorials/local-development/3-configure-jenkins/#2-from-the-container-log-output","text":"The initial admin password is also printed to standard out while Jenkins starts up. To view the Jenkins logs, run docker logs jenkins . In the output, you should see something similar to this: Copy and paste the password into the text box in Jenkins.","title":"2. From the container log output"},{"location":"tutorials/local-development/3-configure-jenkins/#installing-the-suggested-plugins","text":"After entering the initial admin password, Jenkins will bring you to a Customize Jenkins page. Click the Install suggested plugins button. This will bring you to a loading screen displaying the progress as Jenkins installs the most popular community plugins. The process takes less than five minutes.","title":"Installing the Suggested Plugins"},{"location":"tutorials/local-development/3-configure-jenkins/#setup-initial-admin-user","text":"After the plugins are done installing, Jenkins will send you to a screen to configure the default admin user: Feel free to create a custom username and password or continue as admin; no one will use this test Jenkins installation except you. Important If you click \"continue as admin\" then the username will be admin and the password will be the initial admin password we found earlier. If you change your admin user/password to something else you will need to remember it.","title":"Setup Initial Admin User"},{"location":"tutorials/local-development/3-configure-jenkins/#instance-configuration","text":"After creating the initial admin user, Jenkins will send you to a screen where you can configure the instance's URL. The text box will be pre-populated with what's currently in your browser, so click Save and Finish in the bottom right-hand side of the screen. Then click \" Start Using Jenkins \" and you will be directed to the Jenkins home page:","title":"Instance Configuration"},{"location":"tutorials/local-development/3-configure-jenkins/#installing-the-jenkins-templating-engine","text":"At this point, you've completed the Jenkins Startup Wizard process. Now, we're going to install the Jenkins Templating Engine, which can be found as the Templating Engine Plugin in the Plugin Manager. In the left-hand navigation menu, select Manage Jenkins . In the middle of the screen, select Manage Plugins . In the left-hand navigation menu, select Available plugins . In the Search available plugins text box, type: Templating Engine At this point you should see: Make sure to select the Templating Engine checkbox and click the \" Download now and install after restart \" button. This will direct you to a screen showing the download progress of JTE. Scroll to the bottom of the Download progress screen and select \" Restart Jenkins when installation is complete and no jobs are running .\" At this point, Jenkins will restart automatically. Log in again with either the custom admin user you created earlier or the initial admin password. Important You can run docker logs -f jenkins to see the Jenkins logs. It will say \"Jenkins is fully up and running\" somewhere in the logs (with a timestamp) when Jenkins has completed the restart and is ready to be interacted with.","title":"Installing the Jenkins Templating Engine"},{"location":"tutorials/local-development/3-configure-jenkins/#installing-the-docker-pipeline-plugin","text":"Now, we need to install the Docker Pipeline plugin, which can be found as the Docker Pipeline in the Plugin Manager. In the left-hand navigation menu, select Manage Jenkins . In the middle of the screen, select Manage Plugins . In the left-hand navigation menu, select Available plugins . In the Search available plugins text box, type: Docker Pipeline Follow the same steps used for installing the Jenkins Templating Engine and restart the Jenkins instance.","title":"Installing the Docker Pipeline plugin"},{"location":"tutorials/local-development/4-validate-jenkins/","text":"Validate Jenkins \u00b6 That's it! You've now deployed and configured a local Jenkins instance. We're going to run through a couple quick steps to ensure that the deployed Jenkins can launch containers as part of the pipeline. Create a Pipeline Job \u00b6 From the Jenkins home page, select New Item in the left-hand navigation menu. Enter a name for the job. \"validate\" will do. Select the Pipeline job type. Click the OK button at the bottom of the screen. Configure a Pipeline \u00b6 Scroll down to the Pipeline Configuration section. The Definition drop down should already be set to Jenkins Templating Engine . Important This confirms that JTE has been installed successfully! Check the box \"Provide a pipeline template (Jenkinsfile)\". In the Jenkinsfile text box, enter: docker.image(\"maven\").inside{ sh \"mvn -v\" } Note This Jenkinsfile pulls the latest maven image from Docker Hub and executes the command mvn -v within that container image. This will validate that your local Jenkins can pull container images, run them, and then execute pipeline commands inside the launched container. Click the Save button at the bottom of the screen. This will redirect you back to the job's main page. Click Build Now in the left-hand navigation menu. Under Build History select #1 to navigate to the Build page. In the left-hand navigation menu, select Console Output to read the build logs. Confirm that the pipeline successfully pulled the maven container image. Confirm that the command mvn -v executed successfully and shows the Maven version. Validate that the build finished successfully. If all went well, the console output should show something like:","title":"Validate Jenkins"},{"location":"tutorials/local-development/4-validate-jenkins/#validate-jenkins","text":"That's it! You've now deployed and configured a local Jenkins instance. We're going to run through a couple quick steps to ensure that the deployed Jenkins can launch containers as part of the pipeline.","title":"Validate Jenkins"},{"location":"tutorials/local-development/4-validate-jenkins/#create-a-pipeline-job","text":"From the Jenkins home page, select New Item in the left-hand navigation menu. Enter a name for the job. \"validate\" will do. Select the Pipeline job type. Click the OK button at the bottom of the screen.","title":"Create a Pipeline Job"},{"location":"tutorials/local-development/4-validate-jenkins/#configure-a-pipeline","text":"Scroll down to the Pipeline Configuration section. The Definition drop down should already be set to Jenkins Templating Engine . Important This confirms that JTE has been installed successfully! Check the box \"Provide a pipeline template (Jenkinsfile)\". In the Jenkinsfile text box, enter: docker.image(\"maven\").inside{ sh \"mvn -v\" } Note This Jenkinsfile pulls the latest maven image from Docker Hub and executes the command mvn -v within that container image. This will validate that your local Jenkins can pull container images, run them, and then execute pipeline commands inside the launched container. Click the Save button at the bottom of the screen. This will redirect you back to the job's main page. Click Build Now in the left-hand navigation menu. Under Build History select #1 to navigate to the Build page. In the left-hand navigation menu, select Console Output to read the build logs. Confirm that the pipeline successfully pulled the maven container image. Confirm that the command mvn -v executed successfully and shows the Maven version. Validate that the build finished successfully. If all went well, the console output should show something like:","title":"Configure a Pipeline"}]}